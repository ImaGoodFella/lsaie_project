START TIME: Wed Dec 17 17:45:29 CET 2025
Running DeepSpeed Stage: superoffload_compile
Job ID: 1254473
Output will be in: logs/deepspeed/1254473.out
df: /users/bzuidema/.triton/autotune: No such file or directory
[DeepCompile PATCH] Loading patched patch_compiled_func.py (PyTorch 2.6.0a0+ecf3bae40a.nv25.01)
[DeepCompile PATCH] Using PyTorch 2.6 fallback code path (handles NGC alpha versions)
[2025-12-17 17:45:53,573] [WARNING] [runner.py:232:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected VISIBLE_DEVICES=0,1,2,3 but ignoring it because one or several of --include/--exclude/--num_gpus/--num_nodes cl args were used. If you want to use CUDA_VISIBLE_DEVICES don't pass any of these arguments to deepspeed.
[2025-12-17 17:45:53,573] [INFO] [runner.py:630:main] cmd = /usr/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgM119 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None --bind_cores_to_rank --log_level=info /users/bzuidema/scratch/project/src/train.py --deepspeed_config /users/bzuidema/scratch/project/configs/deepspeed/stage_superoffload_compile.json --batch-size 1 --learning-rate 5e-5 --lr-warmup-steps 100 --training-steps 1000 --sequence-length 2048 --deepspeed
[DeepCompile PATCH] Loading patched patch_compiled_func.py (PyTorch 2.6.0a0+ecf3bae40a.nv25.01)
[DeepCompile PATCH] Using PyTorch 2.6 fallback code path (handles NGC alpha versions)
[2025-12-17 17:46:00,821] [INFO] [launch.py:155:main] 0 NCCL_NET_PLUGIN=ofi
[2025-12-17 17:46:00,821] [INFO] [launch.py:155:main] 0 NCCL_VERSION=2.25.1
[2025-12-17 17:46:00,821] [INFO] [launch.py:155:main] 0 NCCL_SOCKET_IFNAME=hsn
[2025-12-17 17:46:00,821] [INFO] [launch.py:155:main] 0 NCCL_NVLS_ENABLE=0
[2025-12-17 17:46:00,821] [INFO] [launch.py:155:main] 0 NCCL_NET_GDR_LEVEL=PHB
[2025-12-17 17:46:00,821] [INFO] [launch.py:155:main] 0 TORCH_NCCL_USE_COMM_NONBLOCKING=0
[2025-12-17 17:46:00,821] [INFO] [launch.py:155:main] 0 NCCL_NET=AWS Libfabric
[2025-12-17 17:46:00,822] [INFO] [launch.py:155:main] 0 AWS_OFI_NCCL_VERSION=1.12.1
[2025-12-17 17:46:00,822] [INFO] [launch.py:155:main] 0 NCCL_CROSS_NIC=1
[2025-12-17 17:46:00,822] [INFO] [launch.py:162:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3]}
[2025-12-17 17:46:00,822] [INFO] [launch.py:168:main] nnodes=1, num_local_procs=4, node_rank=0
[2025-12-17 17:46:00,822] [INFO] [launch.py:179:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3]})
[2025-12-17 17:46:00,822] [INFO] [launch.py:180:main] dist_world_size=4
[2025-12-17 17:46:00,822] [INFO] [launch.py:184:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
[2025-12-17 17:46:00,848] [INFO] [launch.py:272:main] process 82860 spawned with command: ['numactl', '-m', '0', '-C', '0-71', '/usr/bin/python', '-u', '/users/bzuidema/scratch/project/src/train.py', '--local_rank=0', '--deepspeed_config', '/users/bzuidema/scratch/project/configs/deepspeed/stage_superoffload_compile.json', '--batch-size', '1', '--learning-rate', '5e-5', '--lr-warmup-steps', '100', '--training-steps', '1000', '--sequence-length', '2048', '--deepspeed']
[2025-12-17 17:46:00,863] [INFO] [launch.py:272:main] process 82863 spawned with command: ['numactl', '-m', '1', '-C', '72-143', '/usr/bin/python', '-u', '/users/bzuidema/scratch/project/src/train.py', '--local_rank=1', '--deepspeed_config', '/users/bzuidema/scratch/project/configs/deepspeed/stage_superoffload_compile.json', '--batch-size', '1', '--learning-rate', '5e-5', '--lr-warmup-steps', '100', '--training-steps', '1000', '--sequence-length', '2048', '--deepspeed']
[2025-12-17 17:46:00,879] [INFO] [launch.py:272:main] process 82866 spawned with command: ['numactl', '-m', '2', '-C', '144-215', '/usr/bin/python', '-u', '/users/bzuidema/scratch/project/src/train.py', '--local_rank=2', '--deepspeed_config', '/users/bzuidema/scratch/project/configs/deepspeed/stage_superoffload_compile.json', '--batch-size', '1', '--learning-rate', '5e-5', '--lr-warmup-steps', '100', '--training-steps', '1000', '--sequence-length', '2048', '--deepspeed']
[2025-12-17 17:46:00,895] [INFO] [launch.py:272:main] process 82870 spawned with command: ['numactl', '-m', '3', '-C', '216-287', '/usr/bin/python', '-u', '/users/bzuidema/scratch/project/src/train.py', '--local_rank=3', '--deepspeed_config', '/users/bzuidema/scratch/project/configs/deepspeed/stage_superoffload_compile.json', '--batch-size', '1', '--learning-rate', '5e-5', '--lr-warmup-steps', '100', '--training-steps', '1000', '--sequence-length', '2048', '--deepspeed']
[DeepCompile PATCH] Loading patched patch_compiled_func.py (PyTorch 2.6.0a0+ecf3bae40a.nv25.01)
[DeepCompile PATCH] Using PyTorch 2.6 fallback code path (handles NGC alpha versions)
[DeepCompile PATCH] Loading patched patch_compiled_func.py (PyTorch 2.6.0a0+ecf3bae40a.nv25.01)
[DeepCompile PATCH] Using PyTorch 2.6 fallback code path (handles NGC alpha versions)
2025-12-17 17:46:09,793 - root - INFO - Experiment args: Namespace(dataset='/capstor/store/cscs/ethz/large-sc-2/datasets/train_data.parquet', tokenizer_name_or_path='unsloth/Mistral-Nemo-Base-2407-bnb-4bit', sequence_length=2048, batch_size=1, fused_optimizer=False, learning_rate=5e-05, lr_warmup_steps=100, training_steps=1000, logging_frequency=5, profile=False, profile_step_start=10, profile_step_end=12, grad_max_norm=1, model_dtype='bf16', compile=False, deepspeed=True, deepspeed_config='/users/bzuidema/scratch/project/configs/deepspeed/stage_superoffload_compile.json', local_rank=0)
[DeepCompile PATCH] Loading patched patch_compiled_func.py (PyTorch 2.6.0a0+ecf3bae40a.nv25.01)
[DeepCompile PATCH] Using PyTorch 2.6 fallback code path (handles NGC alpha versions)
[DeepCompile PATCH] Loading patched patch_compiled_func.py (PyTorch 2.6.0a0+ecf3bae40a.nv25.01)
[DeepCompile PATCH] Using PyTorch 2.6 fallback code path (handles NGC alpha versions)
2025-12-17 17:46:10,420 - root - INFO - Experiment args: Namespace(dataset='/capstor/store/cscs/ethz/large-sc-2/datasets/train_data.parquet', tokenizer_name_or_path='unsloth/Mistral-Nemo-Base-2407-bnb-4bit', sequence_length=2048, batch_size=1, fused_optimizer=False, learning_rate=5e-05, lr_warmup_steps=100, training_steps=1000, logging_frequency=5, profile=False, profile_step_start=10, profile_step_end=12, grad_max_norm=1, model_dtype='bf16', compile=False, deepspeed=True, deepspeed_config='/users/bzuidema/scratch/project/configs/deepspeed/stage_superoffload_compile.json', local_rank=2)
2025-12-17 17:46:10,798 - root - INFO - Experiment args: Namespace(dataset='/capstor/store/cscs/ethz/large-sc-2/datasets/train_data.parquet', tokenizer_name_or_path='unsloth/Mistral-Nemo-Base-2407-bnb-4bit', sequence_length=2048, batch_size=1, fused_optimizer=False, learning_rate=5e-05, lr_warmup_steps=100, training_steps=1000, logging_frequency=5, profile=False, profile_step_start=10, profile_step_end=12, grad_max_norm=1, model_dtype='bf16', compile=False, deepspeed=True, deepspeed_config='/users/bzuidema/scratch/project/configs/deepspeed/stage_superoffload_compile.json', local_rank=1)
2025-12-17 17:46:10,819 - root - INFO - Experiment args: Namespace(dataset='/capstor/store/cscs/ethz/large-sc-2/datasets/train_data.parquet', tokenizer_name_or_path='unsloth/Mistral-Nemo-Base-2407-bnb-4bit', sequence_length=2048, batch_size=1, fused_optimizer=False, learning_rate=5e-05, lr_warmup_steps=100, training_steps=1000, logging_frequency=5, profile=False, profile_step_start=10, profile_step_end=12, grad_max_norm=1, model_dtype='bf16', compile=False, deepspeed=True, deepspeed_config='/users/bzuidema/scratch/project/configs/deepspeed/stage_superoffload_compile.json', local_rank=3)
2025-12-17 17:46:16,247 - root - INFO - Setting up DataLoaders...
2025-12-17 17:46:16,247 - root - INFO - Setting up DataLoaders...
2025-12-17 17:46:16,247 - root - INFO - Setting up DataLoaders...
2025-12-17 17:46:16,249 - root - INFO - Setting up DataLoaders...
2025-12-17 17:46:19,665 - root - INFO - Setting up Model...
2025-12-17 17:46:19,665 - root - INFO - Setting up Model...
2025-12-17 17:46:19,666 - root - INFO - Setting up Model...
2025-12-17 17:46:19,666 - root - INFO - Setting up Model...
2025-12-17 17:46:19,668 - root - INFO - Using ZeRO Stage 3 partition-at-init
2025-12-17 17:46:19,668 - root - INFO - Using ZeRO Stage 3 partition-at-init
2025-12-17 17:46:19,668 - root - INFO - Using ZeRO Stage 3 partition-at-init
2025-12-17 17:46:19,669 - root - INFO - Using ZeRO Stage 3 partition-at-init
2025-12-17 17:46:25,016 - root - INFO - Model parameters (excluding embedding): 8,053,329,920
2025-12-17 17:46:25,016 - root - INFO - FLOPs per token: 51,541,204,992
2025-12-17 17:46:25,016 - root - INFO - Using DeepSpeed
2025-12-17 17:46:25,017 - root - INFO - Using DeepSpeedCPUAdam (optimizer offload to CPU enabled)
2025-12-17 17:46:25,029 - root - INFO - Model parameters (excluding embedding): 8,053,329,920
2025-12-17 17:46:25,029 - root - INFO - FLOPs per token: 51,541,204,992
2025-12-17 17:46:25,029 - root - INFO - Using DeepSpeed
2025-12-17 17:46:25,030 - root - INFO - Using DeepSpeedCPUAdam (optimizer offload to CPU enabled)
2025-12-17 17:46:25,040 - root - INFO - Model parameters (excluding embedding): 8,053,329,920
2025-12-17 17:46:25,040 - root - INFO - FLOPs per token: 51,541,204,992
2025-12-17 17:46:25,040 - root - INFO - Model parameters (excluding embedding): 8,053,329,920
2025-12-17 17:46:25,040 - root - INFO - Using DeepSpeed
2025-12-17 17:46:25,040 - root - INFO - FLOPs per token: 51,541,204,992
2025-12-17 17:46:25,040 - root - INFO - Using DeepSpeed
2025-12-17 17:46:25,040 - root - INFO - Using DeepSpeedCPUAdam (optimizer offload to CPU enabled)
2025-12-17 17:46:25,040 - root - INFO - Using DeepSpeedCPUAdam (optimizer offload to CPU enabled)
/usr/local/lib/python3.12/dist-packages/torch/utils/cpp_extension.py:2011: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].
  warnings.warn(
Stage 3 initialize beginning
MA 3.75 GB         Max_MA 4.75 GB         CA 4.88 GB         Max_CA 5 GB 
CPU Virtual Memory:  used = 177.3 GB, percent = 20.7%
DeepSpeedZeRoOffload initialize [begin]
MA 3.75 GB         Max_MA 3.75 GB         CA 4.88 GB         Max_CA 5 GB 
CPU Virtual Memory:  used = 177.3 GB, percent = 20.7%
Parameter Offload - Persistent parameters statistics: param_count = 65, numel = 266240
DeepSpeedZeRoOffload initialize [end]
MA 3.75 GB         Max_MA 3.75 GB         CA 4.88 GB         Max_CA 5 GB 
CPU Virtual Memory:  used = 177.3 GB, percent = 20.8%
Before creating fp16 partitions
MA 3.75 GB         Max_MA 3.75 GB         CA 4.88 GB         Max_CA 5 GB 
CPU Virtual Memory:  used = 177.3 GB, percent = 20.8%
After creating fp16 partitions: 5
MA 3.75 GB         Max_MA 3.75 GB         CA 3.75 GB         Max_CA 5 GB 
CPU Virtual Memory:  used = 187.13 GB, percent = 21.9%
Before creating fp32 partitions
MA 3.75 GB         Max_MA 3.75 GB         CA 3.75 GB         Max_CA 4 GB 
CPU Virtual Memory:  used = 211.16 GB, percent = 24.7%
After creating fp32 partitions
MA 3.75 GB         Max_MA 3.75 GB         CA 3.75 GB         Max_CA 4 GB 
CPU Virtual Memory:  used = 249.8 GB, percent = 29.2%
Before initializing optimizer states
MA 3.75 GB         Max_MA 3.75 GB         CA 3.75 GB         Max_CA 4 GB 
CPU Virtual Memory:  used = 325.22 GB, percent = 38.1%
After initializing optimizer states
MA 3.75 GB         Max_MA 3.75 GB         CA 3.75 GB         Max_CA 4 GB 
CPU Virtual Memory:  used = 272.42 GB, percent = 31.9%
[2025-12-17 17:46:54,785] [WARNING] [lr_schedules.py:690:get_lr] Attempting to get learning rate from scheduler before it has started
2025-12-17 17:46:54,786 - root - INFO - Enabling Compile in DeepSpeed
[2025-12-17 17:46:54,788] [WARNING] [lr_schedules.py:690:get_lr] Attempting to get learning rate from scheduler before it has started
2025-12-17 17:46:54,789 - root - INFO - Enabling Compile in DeepSpeed
[2025-12-17 17:46:54,791] [WARNING] [lr_schedules.py:690:get_lr] Attempting to get learning rate from scheduler before it has started
2025-12-17 17:46:54,792 - root - INFO - Enabling Compile in DeepSpeed
2025-12-17 17:46:54,837 - root - INFO - Starting training!
2025-12-17 17:46:54,841 - root - INFO - Starting training!
2025-12-17 17:46:54,842 - root - INFO - Starting training!
After initializing ZeRO optimizer
MA 4.5 GB         Max_MA 6.5 GB         CA 6.5 GB         Max_CA 6 GB 
CPU Virtual Memory:  used = 263.69 GB, percent = 30.9%
[2025-12-17 17:46:54,922] [WARNING] [lr_schedules.py:690:get_lr] Attempting to get learning rate from scheduler before it has started
2025-12-17 17:46:54,923 - root - INFO - Enabling Compile in DeepSpeed
2025-12-17 17:46:55,097 - root - INFO - Starting training!
[rank3]:W1217 17:46:57.537000 82870 torch/_dynamo/convert_frame.py:915] [2/8] torch._dynamo hit config.cache_size_limit (8)
[rank3]:W1217 17:46:57.537000 82870 torch/_dynamo/convert_frame.py:915] [2/8]    function: 'torch_dynamo_resume_in__forward_prologue_at_2187' (/users/bzuidema/lsaie_deepspeed/deepspeed/runtime/engine.py:2187)
[rank3]:W1217 17:46:57.537000 82870 torch/_dynamo/convert_frame.py:915] [2/8]    last reason: 2/0: ___tuple_iterator_len(L['___stack0']) == 388                
[rank3]:W1217 17:46:57.537000 82870 torch/_dynamo/convert_frame.py:915] [2/8] To log all recompilation reasons, use TORCH_LOGS="recompiles".
[rank3]:W1217 17:46:57.537000 82870 torch/_dynamo/convert_frame.py:915] [2/8] To diagnose recompilation issues, see https://pytorch.org/docs/main/torch.compiler_troubleshooting.html.
[rank2]:W1217 17:46:57.553000 82866 torch/_dynamo/convert_frame.py:915] [2/8] torch._dynamo hit config.cache_size_limit (8)
[rank2]:W1217 17:46:57.553000 82866 torch/_dynamo/convert_frame.py:915] [2/8]    function: 'torch_dynamo_resume_in__forward_prologue_at_2187' (/users/bzuidema/lsaie_deepspeed/deepspeed/runtime/engine.py:2187)
[rank2]:W1217 17:46:57.553000 82866 torch/_dynamo/convert_frame.py:915] [2/8]    last reason: 2/0: ___tuple_iterator_len(L['___stack0']) == 388                
[rank2]:W1217 17:46:57.553000 82866 torch/_dynamo/convert_frame.py:915] [2/8] To log all recompilation reasons, use TORCH_LOGS="recompiles".
[rank2]:W1217 17:46:57.553000 82866 torch/_dynamo/convert_frame.py:915] [2/8] To diagnose recompilation issues, see https://pytorch.org/docs/main/torch.compiler_troubleshooting.html.
[rank1]:W1217 17:46:57.584000 82863 torch/_dynamo/convert_frame.py:915] [2/8] torch._dynamo hit config.cache_size_limit (8)
[rank1]:W1217 17:46:57.584000 82863 torch/_dynamo/convert_frame.py:915] [2/8]    function: 'torch_dynamo_resume_in__forward_prologue_at_2187' (/users/bzuidema/lsaie_deepspeed/deepspeed/runtime/engine.py:2187)
[rank1]:W1217 17:46:57.584000 82863 torch/_dynamo/convert_frame.py:915] [2/8]    last reason: 2/0: ___tuple_iterator_len(L['___stack0']) == 388                
[rank1]:W1217 17:46:57.584000 82863 torch/_dynamo/convert_frame.py:915] [2/8] To log all recompilation reasons, use TORCH_LOGS="recompiles".
[rank1]:W1217 17:46:57.584000 82863 torch/_dynamo/convert_frame.py:915] [2/8] To diagnose recompilation issues, see https://pytorch.org/docs/main/torch.compiler_troubleshooting.html.
[rank0]:W1217 17:46:57.709000 82860 torch/_dynamo/convert_frame.py:915] [2/8] torch._dynamo hit config.cache_size_limit (8)
[rank0]:W1217 17:46:57.709000 82860 torch/_dynamo/convert_frame.py:915] [2/8]    function: 'torch_dynamo_resume_in__forward_prologue_at_2187' (/users/bzuidema/lsaie_deepspeed/deepspeed/runtime/engine.py:2187)
[rank0]:W1217 17:46:57.709000 82860 torch/_dynamo/convert_frame.py:915] [2/8]    last reason: 2/0: ___tuple_iterator_len(L['___stack0']) == 388                
[rank0]:W1217 17:46:57.709000 82860 torch/_dynamo/convert_frame.py:915] [2/8] To log all recompilation reasons, use TORCH_LOGS="recompiles".
[rank0]:W1217 17:46:57.709000 82860 torch/_dynamo/convert_frame.py:915] [2/8] To diagnose recompilation issues, see https://pytorch.org/docs/main/torch.compiler_troubleshooting.html.
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/misc.py:651: UserWarning: The config.capture_autograd_function flag is deprecated, it's now always true.
  warnings.warn(
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/misc.py:651: UserWarning: The config.capture_autograd_function flag is deprecated, it's now always true.
  warnings.warn(
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/misc.py:651: UserWarning: The config.capture_autograd_function flag is deprecated, it's now always true.
  warnings.warn(
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/misc.py:651: UserWarning: The config.capture_autograd_function flag is deprecated, it's now always true.
  warnings.warn(
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/misc.py:651: UserWarning: The config.capture_autograd_function flag is deprecated, it's now always true.
  warnings.warn(
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/misc.py:651: UserWarning: The config.capture_autograd_function flag is deprecated, it's now always true.
  warnings.warn(
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/misc.py:651: UserWarning: The config.capture_autograd_function flag is deprecated, it's now always true.
  warnings.warn(
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/misc.py:651: UserWarning: The config.capture_autograd_function flag is deprecated, it's now always true.
  warnings.warn(
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/misc.py:651: UserWarning: The config.capture_autograd_function flag is deprecated, it's now always true.
  warnings.warn(
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/misc.py:651: UserWarning: The config.capture_autograd_function flag is deprecated, it's now always true.
  warnings.warn(
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/misc.py:651: UserWarning: The config.capture_autograd_function flag is deprecated, it's now always true.
  warnings.warn(
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/misc.py:651: UserWarning: The config.capture_autograd_function flag is deprecated, it's now always true.
  warnings.warn(
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/misc.py:651: UserWarning: The config.capture_autograd_function flag is deprecated, it's now always true.
  warnings.warn(
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/misc.py:651: UserWarning: The config.capture_autograd_function flag is deprecated, it's now always true.
  warnings.warn(
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/misc.py:651: UserWarning: The config.capture_autograd_function flag is deprecated, it's now always true.
  warnings.warn(
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/misc.py:651: UserWarning: The config.capture_autograd_function flag is deprecated, it's now always true.
  warnings.warn(
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/misc.py:651: UserWarning: The config.capture_autograd_function flag is deprecated, it's now always true.
  warnings.warn(
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/misc.py:651: UserWarning: The config.capture_autograd_function flag is deprecated, it's now always true.
  warnings.warn(
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/misc.py:651: UserWarning: The config.capture_autograd_function flag is deprecated, it's now always true.
  warnings.warn(
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/misc.py:651: UserWarning: The config.capture_autograd_function flag is deprecated, it's now always true.
  warnings.warn(
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/misc.py:651: UserWarning: The config.capture_autograd_function flag is deprecated, it's now always true.
  warnings.warn(
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/misc.py:651: UserWarning: The config.capture_autograd_function flag is deprecated, it's now always true.
  warnings.warn(
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/misc.py:651: UserWarning: The config.capture_autograd_function flag is deprecated, it's now always true.
  warnings.warn(
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/misc.py:651: UserWarning: The config.capture_autograd_function flag is deprecated, it's now always true.
  warnings.warn(
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/misc.py:651: UserWarning: The config.capture_autograd_function flag is deprecated, it's now always true.
  warnings.warn(
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/misc.py:651: UserWarning: The config.capture_autograd_function flag is deprecated, it's now always true.
  warnings.warn(
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/misc.py:651: UserWarning: The config.capture_autograd_function flag is deprecated, it's now always true.
  warnings.warn(
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/misc.py:651: UserWarning: The config.capture_autograd_function flag is deprecated, it's now always true.
  warnings.warn(
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/misc.py:651: UserWarning: The config.capture_autograd_function flag is deprecated, it's now always true.
  warnings.warn(
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/misc.py:651: UserWarning: The config.capture_autograd_function flag is deprecated, it's now always true.
  warnings.warn(
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/misc.py:651: UserWarning: The config.capture_autograd_function flag is deprecated, it's now always true.
  warnings.warn(
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/misc.py:651: UserWarning: The config.capture_autograd_function flag is deprecated, it's now always true.
  warnings.warn(
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/misc.py:651: UserWarning: The config.capture_autograd_function flag is deprecated, it's now always true.
  warnings.warn(
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/misc.py:651: UserWarning: The config.capture_autograd_function flag is deprecated, it's now always true.
  warnings.warn(
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/misc.py:651: UserWarning: The config.capture_autograd_function flag is deprecated, it's now always true.
  warnings.warn(
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/misc.py:651: UserWarning: The config.capture_autograd_function flag is deprecated, it's now always true.
  warnings.warn(
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/misc.py:651: UserWarning: The config.capture_autograd_function flag is deprecated, it's now always true.
  warnings.warn(
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/misc.py:651: UserWarning: The config.capture_autograd_function flag is deprecated, it's now always true.
  warnings.warn(
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/misc.py:651: UserWarning: The config.capture_autograd_function flag is deprecated, it's now always true.
  warnings.warn(
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/misc.py:651: UserWarning: The config.capture_autograd_function flag is deprecated, it's now always true.
  warnings.warn(
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/misc.py:651: UserWarning: The config.capture_autograd_function flag is deprecated, it's now always true.
  warnings.warn(
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/misc.py:651: UserWarning: The config.capture_autograd_function flag is deprecated, it's now always true.
  warnings.warn(
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/misc.py:651: UserWarning: The config.capture_autograd_function flag is deprecated, it's now always true.
  warnings.warn(
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/misc.py:651: UserWarning: The config.capture_autograd_function flag is deprecated, it's now always true.
  warnings.warn(
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/misc.py:651: UserWarning: The config.capture_autograd_function flag is deprecated, it's now always true.
  warnings.warn(
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/misc.py:651: UserWarning: The config.capture_autograd_function flag is deprecated, it's now always true.
  warnings.warn(
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/misc.py:651: UserWarning: The config.capture_autograd_function flag is deprecated, it's now always true.
  warnings.warn(
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/misc.py:651: UserWarning: The config.capture_autograd_function flag is deprecated, it's now always true.
  warnings.warn(
/usr/local/lib/python3.12/dist-packages/torch/_inductor/lowering.py:1782: UserWarning: Torchinductor does not support code generation for complex operators. Performance may be worse than eager.
  warnings.warn(
/usr/local/lib/python3.12/dist-packages/torch/_inductor/lowering.py:1782: UserWarning: Torchinductor does not support code generation for complex operators. Performance may be worse than eager.
  warnings.warn(
/usr/local/lib/python3.12/dist-packages/torch/_inductor/lowering.py:1782: UserWarning: Torchinductor does not support code generation for complex operators. Performance may be worse than eager.
  warnings.warn(
/usr/local/lib/python3.12/dist-packages/torch/_inductor/lowering.py:1782: UserWarning: Torchinductor does not support code generation for complex operators. Performance may be worse than eager.
  warnings.warn(
/usr/local/lib/python3.12/dist-packages/torch/utils/cpp_extension.py:2011: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].
  warnings.warn(
[DeepCompile PATCH] Loading patched patch_compiled_func.py (PyTorch 2.6.0a0+ecf3bae40a.nv25.01)
[DeepCompile PATCH] Using PyTorch 2.6 fallback code path (handles NGC alpha versions)
/usr/local/lib/python3.12/dist-packages/torch/utils/cpp_extension.py:2011: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].
  warnings.warn(
[DeepCompile PATCH] Loading patched patch_compiled_func.py (PyTorch 2.6.0a0+ecf3bae40a.nv25.01)
[DeepCompile PATCH] Using PyTorch 2.6 fallback code path (handles NGC alpha versions)
/usr/local/lib/python3.12/dist-packages/torch/utils/cpp_extension.py:2011: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].
  warnings.warn(
[DeepCompile PATCH] Loading patched patch_compiled_func.py (PyTorch 2.6.0a0+ecf3bae40a.nv25.01)
[DeepCompile PATCH] Using PyTorch 2.6 fallback code path (handles NGC alpha versions)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/misc.py:651: UserWarning: The config.capture_autograd_function flag is deprecated, it's now always true.
  warnings.warn(
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/misc.py:651: UserWarning: The config.capture_autograd_function flag is deprecated, it's now always true.
  warnings.warn(
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/misc.py:651: UserWarning: The config.capture_autograd_function flag is deprecated, it's now always true.
  warnings.warn(
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/misc.py:651: UserWarning: The config.capture_autograd_function flag is deprecated, it's now always true.
  warnings.warn(
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/misc.py:651: UserWarning: The config.capture_autograd_function flag is deprecated, it's now always true.
  warnings.warn(
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/misc.py:651: UserWarning: The config.capture_autograd_function flag is deprecated, it's now always true.
  warnings.warn(
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/misc.py:651: UserWarning: The config.capture_autograd_function flag is deprecated, it's now always true.
  warnings.warn(
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/misc.py:651: UserWarning: The config.capture_autograd_function flag is deprecated, it's now always true.
  warnings.warn(
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/misc.py:651: UserWarning: The config.capture_autograd_function flag is deprecated, it's now always true.
  warnings.warn(
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/misc.py:651: UserWarning: The config.capture_autograd_function flag is deprecated, it's now always true.
  warnings.warn(
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/misc.py:651: UserWarning: The config.capture_autograd_function flag is deprecated, it's now always true.
  warnings.warn(
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/misc.py:651: UserWarning: The config.capture_autograd_function flag is deprecated, it's now always true.
  warnings.warn(
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/misc.py:651: UserWarning: The config.capture_autograd_function flag is deprecated, it's now always true.
  warnings.warn(
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/misc.py:651: UserWarning: The config.capture_autograd_function flag is deprecated, it's now always true.
  warnings.warn(
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/misc.py:651: UserWarning: The config.capture_autograd_function flag is deprecated, it's now always true.
  warnings.warn(
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/misc.py:651: UserWarning: The config.capture_autograd_function flag is deprecated, it's now always true.
  warnings.warn(
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/misc.py:651: UserWarning: The config.capture_autograd_function flag is deprecated, it's now always true.
  warnings.warn(
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/misc.py:651: UserWarning: The config.capture_autograd_function flag is deprecated, it's now always true.
  warnings.warn(
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/misc.py:651: UserWarning: The config.capture_autograd_function flag is deprecated, it's now always true.
  warnings.warn(
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/misc.py:651: UserWarning: The config.capture_autograd_function flag is deprecated, it's now always true.
  warnings.warn(
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/misc.py:651: UserWarning: The config.capture_autograd_function flag is deprecated, it's now always true.
  warnings.warn(
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/misc.py:651: UserWarning: The config.capture_autograd_function flag is deprecated, it's now always true.
  warnings.warn(
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/misc.py:651: UserWarning: The config.capture_autograd_function flag is deprecated, it's now always true.
  warnings.warn(
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/misc.py:651: UserWarning: The config.capture_autograd_function flag is deprecated, it's now always true.
  warnings.warn(
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/misc.py:651: UserWarning: The config.capture_autograd_function flag is deprecated, it's now always true.
  warnings.warn(
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/misc.py:651: UserWarning: The config.capture_autograd_function flag is deprecated, it's now always true.
  warnings.warn(
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/misc.py:651: UserWarning: The config.capture_autograd_function flag is deprecated, it's now always true.
  warnings.warn(
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/misc.py:651: UserWarning: The config.capture_autograd_function flag is deprecated, it's now always true.
  warnings.warn(
[rank1]:W1217 17:47:08.620000 82863 torch/_dynamo/convert_frame.py:915] [7/8] torch._dynamo hit config.cache_size_limit (8)
[rank1]:W1217 17:47:08.620000 82863 torch/_dynamo/convert_frame.py:915] [7/8]    function: 'post_sub_module_forward_function' (/users/bzuidema/lsaie_deepspeed/deepspeed/runtime/zero/parameter_offload.py:484)
[rank1]:W1217 17:47:08.620000 82863 torch/_dynamo/convert_frame.py:915] [7/8]    last reason: 7/0: ___check_type_id(L['sub_module'], 966866608)                
[rank1]:W1217 17:47:08.620000 82863 torch/_dynamo/convert_frame.py:915] [7/8] To log all recompilation reasons, use TORCH_LOGS="recompiles".
[rank1]:W1217 17:47:08.620000 82863 torch/_dynamo/convert_frame.py:915] [7/8] To diagnose recompilation issues, see https://pytorch.org/docs/main/torch.compiler_troubleshooting.html.
[rank1]:W1217 17:47:08.623000 82863 torch/_dynamo/convert_frame.py:915] [17/8] torch._dynamo hit config.cache_size_limit (8)
[rank1]:W1217 17:47:08.623000 82863 torch/_dynamo/convert_frame.py:915] [17/8]    function: '_pre_backward_module_hook' (/users/bzuidema/lsaie_deepspeed/deepspeed/runtime/zero/parameter_offload.py:350)
[rank1]:W1217 17:47:08.623000 82863 torch/_dynamo/convert_frame.py:915] [17/8]    last reason: 17/0: ___check_type_id(L['module'], 966866608)                    
[rank1]:W1217 17:47:08.623000 82863 torch/_dynamo/convert_frame.py:915] [17/8] To log all recompilation reasons, use TORCH_LOGS="recompiles".
[rank1]:W1217 17:47:08.623000 82863 torch/_dynamo/convert_frame.py:915] [17/8] To diagnose recompilation issues, see https://pytorch.org/docs/main/torch.compiler_troubleshooting.html.
[rank2]:W1217 17:47:08.630000 82866 torch/_dynamo/convert_frame.py:915] [7/8] torch._dynamo hit config.cache_size_limit (8)
[rank2]:W1217 17:47:08.630000 82866 torch/_dynamo/convert_frame.py:915] [7/8]    function: 'post_sub_module_forward_function' (/users/bzuidema/lsaie_deepspeed/deepspeed/runtime/zero/parameter_offload.py:484)
[rank2]:W1217 17:47:08.630000 82866 torch/_dynamo/convert_frame.py:915] [7/8]    last reason: 7/0: ___check_type_id(L['sub_module'], 1101346480)               
[rank2]:W1217 17:47:08.630000 82866 torch/_dynamo/convert_frame.py:915] [7/8] To log all recompilation reasons, use TORCH_LOGS="recompiles".
[rank2]:W1217 17:47:08.630000 82866 torch/_dynamo/convert_frame.py:915] [7/8] To diagnose recompilation issues, see https://pytorch.org/docs/main/torch.compiler_troubleshooting.html.
[rank3]:W1217 17:47:08.631000 82870 torch/_dynamo/convert_frame.py:915] [7/8] torch._dynamo hit config.cache_size_limit (8)
[rank3]:W1217 17:47:08.631000 82870 torch/_dynamo/convert_frame.py:915] [7/8]    function: 'post_sub_module_forward_function' (/users/bzuidema/lsaie_deepspeed/deepspeed/runtime/zero/parameter_offload.py:484)
[rank3]:W1217 17:47:08.631000 82870 torch/_dynamo/convert_frame.py:915] [7/8]    last reason: 7/0: ___check_type_id(L['sub_module'], 1077294768)               
[rank3]:W1217 17:47:08.631000 82870 torch/_dynamo/convert_frame.py:915] [7/8] To log all recompilation reasons, use TORCH_LOGS="recompiles".
[rank3]:W1217 17:47:08.631000 82870 torch/_dynamo/convert_frame.py:915] [7/8] To diagnose recompilation issues, see https://pytorch.org/docs/main/torch.compiler_troubleshooting.html.
[rank2]:W1217 17:47:08.633000 82866 torch/_dynamo/convert_frame.py:915] [17/8] torch._dynamo hit config.cache_size_limit (8)
[rank2]:W1217 17:47:08.633000 82866 torch/_dynamo/convert_frame.py:915] [17/8]    function: '_pre_backward_module_hook' (/users/bzuidema/lsaie_deepspeed/deepspeed/runtime/zero/parameter_offload.py:350)
[rank2]:W1217 17:47:08.633000 82866 torch/_dynamo/convert_frame.py:915] [17/8]    last reason: 17/0: ___check_type_id(L['module'], 1101346480)                   
[rank2]:W1217 17:47:08.633000 82866 torch/_dynamo/convert_frame.py:915] [17/8] To log all recompilation reasons, use TORCH_LOGS="recompiles".
[rank2]:W1217 17:47:08.633000 82866 torch/_dynamo/convert_frame.py:915] [17/8] To diagnose recompilation issues, see https://pytorch.org/docs/main/torch.compiler_troubleshooting.html.
[rank3]:W1217 17:47:08.634000 82870 torch/_dynamo/convert_frame.py:915] [17/8] torch._dynamo hit config.cache_size_limit (8)
[rank3]:W1217 17:47:08.634000 82870 torch/_dynamo/convert_frame.py:915] [17/8]    function: '_pre_backward_module_hook' (/users/bzuidema/lsaie_deepspeed/deepspeed/runtime/zero/parameter_offload.py:350)
[rank3]:W1217 17:47:08.634000 82870 torch/_dynamo/convert_frame.py:915] [17/8]    last reason: 17/0: ___check_type_id(L['module'], 1077294768)                   
[rank3]:W1217 17:47:08.634000 82870 torch/_dynamo/convert_frame.py:915] [17/8] To log all recompilation reasons, use TORCH_LOGS="recompiles".
[rank3]:W1217 17:47:08.634000 82870 torch/_dynamo/convert_frame.py:915] [17/8] To diagnose recompilation issues, see https://pytorch.org/docs/main/torch.compiler_troubleshooting.html.
[rank0]:W1217 17:47:08.646000 82860 torch/_dynamo/convert_frame.py:915] [7/8] torch._dynamo hit config.cache_size_limit (8)
[rank0]:W1217 17:47:08.646000 82860 torch/_dynamo/convert_frame.py:915] [7/8]    function: 'post_sub_module_forward_function' (/users/bzuidema/lsaie_deepspeed/deepspeed/runtime/zero/parameter_offload.py:484)
[rank0]:W1217 17:47:08.646000 82860 torch/_dynamo/convert_frame.py:915] [7/8]    last reason: 7/0: ___check_type_id(L['sub_module'], 221853360)                
[rank0]:W1217 17:47:08.646000 82860 torch/_dynamo/convert_frame.py:915] [7/8] To log all recompilation reasons, use TORCH_LOGS="recompiles".
[rank0]:W1217 17:47:08.646000 82860 torch/_dynamo/convert_frame.py:915] [7/8] To diagnose recompilation issues, see https://pytorch.org/docs/main/torch.compiler_troubleshooting.html.
[rank0]:W1217 17:47:08.649000 82860 torch/_dynamo/convert_frame.py:915] [17/8] torch._dynamo hit config.cache_size_limit (8)
[rank0]:W1217 17:47:08.649000 82860 torch/_dynamo/convert_frame.py:915] [17/8]    function: '_pre_backward_module_hook' (/users/bzuidema/lsaie_deepspeed/deepspeed/runtime/zero/parameter_offload.py:350)
[rank0]:W1217 17:47:08.649000 82860 torch/_dynamo/convert_frame.py:915] [17/8]    last reason: 17/0: ___check_type_id(L['module'], 221853360)                    
[rank0]:W1217 17:47:08.649000 82860 torch/_dynamo/convert_frame.py:915] [17/8] To log all recompilation reasons, use TORCH_LOGS="recompiles".
[rank0]:W1217 17:47:08.649000 82860 torch/_dynamo/convert_frame.py:915] [17/8] To diagnose recompilation issues, see https://pytorch.org/docs/main/torch.compiler_troubleshooting.html.
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/misc.py:651: UserWarning: The config.capture_autograd_function flag is deprecated, it's now always true.
  warnings.warn(
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/misc.py:651: UserWarning: The config.capture_autograd_function flag is deprecated, it's now always true.
  warnings.warn(
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/misc.py:651: UserWarning: The config.capture_autograd_function flag is deprecated, it's now always true.
  warnings.warn(
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/misc.py:651: UserWarning: The config.capture_autograd_function flag is deprecated, it's now always true.
  warnings.warn(
[rank3]:W1217 17:47:09.771000 82870 torch/_dynamo/convert_frame.py:915] [6/8] torch._dynamo hit config.cache_size_limit (8)
[rank3]:W1217 17:47:09.771000 82870 torch/_dynamo/convert_frame.py:915] [6/8]    function: '_post_forward_module_hook' (/users/bzuidema/lsaie_deepspeed/deepspeed/runtime/zero/parameter_offload.py:302)
[rank3]:W1217 17:47:09.771000 82870 torch/_dynamo/convert_frame.py:915] [6/8]    last reason: 6/0: ___check_type_id(L['module'], 1077294768)                   
[rank3]:W1217 17:47:09.771000 82870 torch/_dynamo/convert_frame.py:915] [6/8] To log all recompilation reasons, use TORCH_LOGS="recompiles".
[rank3]:W1217 17:47:09.771000 82870 torch/_dynamo/convert_frame.py:915] [6/8] To diagnose recompilation issues, see https://pytorch.org/docs/main/torch.compiler_troubleshooting.html.
[rank2]:W1217 17:47:09.771000 82866 torch/_dynamo/convert_frame.py:915] [6/8] torch._dynamo hit config.cache_size_limit (8)
[rank2]:W1217 17:47:09.771000 82866 torch/_dynamo/convert_frame.py:915] [6/8]    function: '_post_forward_module_hook' (/users/bzuidema/lsaie_deepspeed/deepspeed/runtime/zero/parameter_offload.py:302)
[rank2]:W1217 17:47:09.771000 82866 torch/_dynamo/convert_frame.py:915] [6/8]    last reason: 6/0: ___check_type_id(L['module'], 1101346480)                   
[rank2]:W1217 17:47:09.771000 82866 torch/_dynamo/convert_frame.py:915] [6/8] To log all recompilation reasons, use TORCH_LOGS="recompiles".
[rank2]:W1217 17:47:09.771000 82866 torch/_dynamo/convert_frame.py:915] [6/8] To diagnose recompilation issues, see https://pytorch.org/docs/main/torch.compiler_troubleshooting.html.
[rank1]:W1217 17:47:09.772000 82863 torch/_dynamo/convert_frame.py:915] [6/8] torch._dynamo hit config.cache_size_limit (8)
[rank1]:W1217 17:47:09.772000 82863 torch/_dynamo/convert_frame.py:915] [6/8]    function: '_post_forward_module_hook' (/users/bzuidema/lsaie_deepspeed/deepspeed/runtime/zero/parameter_offload.py:302)
[rank1]:W1217 17:47:09.772000 82863 torch/_dynamo/convert_frame.py:915] [6/8]    last reason: 6/0: ___check_type_id(L['module'], 966866608)                    
[rank1]:W1217 17:47:09.772000 82863 torch/_dynamo/convert_frame.py:915] [6/8] To log all recompilation reasons, use TORCH_LOGS="recompiles".
[rank1]:W1217 17:47:09.772000 82863 torch/_dynamo/convert_frame.py:915] [6/8] To diagnose recompilation issues, see https://pytorch.org/docs/main/torch.compiler_troubleshooting.html.
[rank0]:W1217 17:47:09.774000 82860 torch/_dynamo/convert_frame.py:915] [6/8] torch._dynamo hit config.cache_size_limit (8)
[rank0]:W1217 17:47:09.774000 82860 torch/_dynamo/convert_frame.py:915] [6/8]    function: '_post_forward_module_hook' (/users/bzuidema/lsaie_deepspeed/deepspeed/runtime/zero/parameter_offload.py:302)
[rank0]:W1217 17:47:09.774000 82860 torch/_dynamo/convert_frame.py:915] [6/8]    last reason: 6/0: ___check_type_id(L['module'], 221853360)                    
[rank0]:W1217 17:47:09.774000 82860 torch/_dynamo/convert_frame.py:915] [6/8] To log all recompilation reasons, use TORCH_LOGS="recompiles".
[rank0]:W1217 17:47:09.774000 82860 torch/_dynamo/convert_frame.py:915] [6/8] To diagnose recompilation issues, see https://pytorch.org/docs/main/torch.compiler_troubleshooting.html.
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/misc.py:651: UserWarning: The config.capture_autograd_function flag is deprecated, it's now always true.
  warnings.warn(
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/misc.py:651: UserWarning: The config.capture_autograd_function flag is deprecated, it's now always true.
  warnings.warn(
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/misc.py:651: UserWarning: The config.capture_autograd_function flag is deprecated, it's now always true.
  warnings.warn(
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/misc.py:651: UserWarning: The config.capture_autograd_function flag is deprecated, it's now always true.
  warnings.warn(
[rank0]:W1217 17:47:12.716000 82860 torch/_dynamo/convert_frame.py:915] [39/8] torch._dynamo hit config.cache_size_limit (8)
[rank0]:W1217 17:47:12.716000 82860 torch/_dynamo/convert_frame.py:915] [39/8]    function: 'torch_dynamo_resume_in__forward_epilogue_at_2200' (/users/bzuidema/lsaie_deepspeed/deepspeed/runtime/engine.py:2200)
[rank0]:W1217 17:47:12.716000 82860 torch/_dynamo/convert_frame.py:915] [39/8]    last reason: 39/0: ___tuple_iterator_len(L['___stack0']) == 388                
[rank0]:W1217 17:47:12.716000 82860 torch/_dynamo/convert_frame.py:915] [39/8] To log all recompilation reasons, use TORCH_LOGS="recompiles".
[rank0]:W1217 17:47:12.716000 82860 torch/_dynamo/convert_frame.py:915] [39/8] To diagnose recompilation issues, see https://pytorch.org/docs/main/torch.compiler_troubleshooting.html.
[rank2]:W1217 17:47:12.743000 82866 torch/_dynamo/convert_frame.py:915] [39/8] torch._dynamo hit config.cache_size_limit (8)
[rank2]:W1217 17:47:12.743000 82866 torch/_dynamo/convert_frame.py:915] [39/8]    function: 'torch_dynamo_resume_in__forward_epilogue_at_2200' (/users/bzuidema/lsaie_deepspeed/deepspeed/runtime/engine.py:2200)
[rank2]:W1217 17:47:12.743000 82866 torch/_dynamo/convert_frame.py:915] [39/8]    last reason: 39/0: ___tuple_iterator_len(L['___stack0']) == 388                
[rank2]:W1217 17:47:12.743000 82866 torch/_dynamo/convert_frame.py:915] [39/8] To log all recompilation reasons, use TORCH_LOGS="recompiles".
[rank2]:W1217 17:47:12.743000 82866 torch/_dynamo/convert_frame.py:915] [39/8] To diagnose recompilation issues, see https://pytorch.org/docs/main/torch.compiler_troubleshooting.html.
[rank3]:W1217 17:47:12.761000 82870 torch/_dynamo/convert_frame.py:915] [39/8] torch._dynamo hit config.cache_size_limit (8)
[rank3]:W1217 17:47:12.761000 82870 torch/_dynamo/convert_frame.py:915] [39/8]    function: 'torch_dynamo_resume_in__forward_epilogue_at_2200' (/users/bzuidema/lsaie_deepspeed/deepspeed/runtime/engine.py:2200)
[rank3]:W1217 17:47:12.761000 82870 torch/_dynamo/convert_frame.py:915] [39/8]    last reason: 39/0: ___tuple_iterator_len(L['___stack0']) == 388                
[rank3]:W1217 17:47:12.761000 82870 torch/_dynamo/convert_frame.py:915] [39/8] To log all recompilation reasons, use TORCH_LOGS="recompiles".
[rank3]:W1217 17:47:12.761000 82870 torch/_dynamo/convert_frame.py:915] [39/8] To diagnose recompilation issues, see https://pytorch.org/docs/main/torch.compiler_troubleshooting.html.
[rank1]:W1217 17:47:12.784000 82863 torch/_dynamo/convert_frame.py:915] [39/8] torch._dynamo hit config.cache_size_limit (8)
[rank1]:W1217 17:47:12.784000 82863 torch/_dynamo/convert_frame.py:915] [39/8]    function: 'torch_dynamo_resume_in__forward_epilogue_at_2200' (/users/bzuidema/lsaie_deepspeed/deepspeed/runtime/engine.py:2200)
[rank1]:W1217 17:47:12.784000 82863 torch/_dynamo/convert_frame.py:915] [39/8]    last reason: 39/0: ___tuple_iterator_len(L['___stack0']) == 388                
[rank1]:W1217 17:47:12.784000 82863 torch/_dynamo/convert_frame.py:915] [39/8] To log all recompilation reasons, use TORCH_LOGS="recompiles".
[rank1]:W1217 17:47:12.784000 82863 torch/_dynamo/convert_frame.py:915] [39/8] To diagnose recompilation issues, see https://pytorch.org/docs/main/torch.compiler_troubleshooting.html.
2025-12-17 17:47:21,850 - root - INFO - Step: 1 | Loss: 11.92 | Tokens per second: 303.32 | Training tokens per second (%): 28.12 | MFU (%): 0.40 | TFLOPs: 3.91 | Mem Allocated (GB): 8.15 | Mem Reserved (GB): 38.27 | Max Mem Allocated (GB): 24.57
2025-12-17 17:47:21,850 - root - INFO - Step: 1 | Loss: 11.99 | Tokens per second: 306.21 | Training tokens per second (%): 8.84 | MFU (%): 0.40 | TFLOPs: 3.95 | Mem Allocated (GB): 6.52 | Mem Reserved (GB): 35.01 | Max Mem Allocated (GB): 24.57
2025-12-17 17:47:21,850 - root - INFO - Step: 1 | Loss: 12.04 | Tokens per second: 303.27 | Training tokens per second (%): 6.40 | MFU (%): 0.40 | TFLOPs: 3.91 | Mem Allocated (GB): 8.15 | Mem Reserved (GB): 38.27 | Max Mem Allocated (GB): 24.57
2025-12-17 17:47:21,850 - root - INFO - Step: 1 | Loss: 11.94 | Tokens per second: 303.32 | Training tokens per second (%): 35.25 | MFU (%): 0.40 | TFLOPs: 3.91 | Mem Allocated (GB): 8.15 | Mem Reserved (GB): 38.27 | Max Mem Allocated (GB): 24.57
[rank0]:W1217 17:47:23.598000 82860 torch/_dynamo/convert_frame.py:915] [18/8] torch._dynamo hit config.cache_size_limit (8)
[rank0]:W1217 17:47:23.598000 82860 torch/_dynamo/convert_frame.py:915] [18/8]    function: 'apply_to_tensors_only' (/users/bzuidema/lsaie_deepspeed/deepspeed/runtime/zero/utils.py:146)
[rank0]:W1217 17:47:23.598000 82860 torch/_dynamo/convert_frame.py:915] [18/8]    last reason: 18/7: Cache line invalidated because L['function'] got deallocated
[rank0]:W1217 17:47:23.598000 82860 torch/_dynamo/convert_frame.py:915] [18/8] To log all recompilation reasons, use TORCH_LOGS="recompiles".
[rank0]:W1217 17:47:23.598000 82860 torch/_dynamo/convert_frame.py:915] [18/8] To diagnose recompilation issues, see https://pytorch.org/docs/main/torch.compiler_troubleshooting.html.
[rank3]:W1217 17:47:23.616000 82870 torch/_dynamo/convert_frame.py:915] [18/8] torch._dynamo hit config.cache_size_limit (8)
[rank3]:W1217 17:47:23.616000 82870 torch/_dynamo/convert_frame.py:915] [18/8]    function: 'apply_to_tensors_only' (/users/bzuidema/lsaie_deepspeed/deepspeed/runtime/zero/utils.py:146)
[rank3]:W1217 17:47:23.616000 82870 torch/_dynamo/convert_frame.py:915] [18/8]    last reason: 18/7: Cache line invalidated because L['function'] got deallocated
[rank3]:W1217 17:47:23.616000 82870 torch/_dynamo/convert_frame.py:915] [18/8] To log all recompilation reasons, use TORCH_LOGS="recompiles".
[rank3]:W1217 17:47:23.616000 82870 torch/_dynamo/convert_frame.py:915] [18/8] To diagnose recompilation issues, see https://pytorch.org/docs/main/torch.compiler_troubleshooting.html.
[rank2]:W1217 17:47:23.617000 82866 torch/_dynamo/convert_frame.py:915] [18/8] torch._dynamo hit config.cache_size_limit (8)
[rank2]:W1217 17:47:23.617000 82866 torch/_dynamo/convert_frame.py:915] [18/8]    function: 'apply_to_tensors_only' (/users/bzuidema/lsaie_deepspeed/deepspeed/runtime/zero/utils.py:146)
[rank2]:W1217 17:47:23.617000 82866 torch/_dynamo/convert_frame.py:915] [18/8]    last reason: 18/7: Cache line invalidated because L['function'] got deallocated
[rank2]:W1217 17:47:23.617000 82866 torch/_dynamo/convert_frame.py:915] [18/8] To log all recompilation reasons, use TORCH_LOGS="recompiles".
[rank2]:W1217 17:47:23.617000 82866 torch/_dynamo/convert_frame.py:915] [18/8] To diagnose recompilation issues, see https://pytorch.org/docs/main/torch.compiler_troubleshooting.html.
[rank1]:W1217 17:47:23.618000 82863 torch/_dynamo/convert_frame.py:915] [18/8] torch._dynamo hit config.cache_size_limit (8)
[rank1]:W1217 17:47:23.618000 82863 torch/_dynamo/convert_frame.py:915] [18/8]    function: 'apply_to_tensors_only' (/users/bzuidema/lsaie_deepspeed/deepspeed/runtime/zero/utils.py:146)
[rank1]:W1217 17:47:23.618000 82863 torch/_dynamo/convert_frame.py:915] [18/8]    last reason: 18/7: Cache line invalidated because L['function'] got deallocated
[rank1]:W1217 17:47:23.618000 82863 torch/_dynamo/convert_frame.py:915] [18/8] To log all recompilation reasons, use TORCH_LOGS="recompiles".
[rank1]:W1217 17:47:23.618000 82863 torch/_dynamo/convert_frame.py:915] [18/8] To diagnose recompilation issues, see https://pytorch.org/docs/main/torch.compiler_troubleshooting.html.
2025-12-17 17:47:26,847 - root - INFO - Step: 5 | Loss: 11.98 | Tokens per second: 6558.10 | Training tokens per second (%): 39.77 | MFU (%): 8.54 | TFLOPs: 84.50 | Mem Allocated (GB): 6.52 | Mem Reserved (GB): 39.78 | Max Mem Allocated (GB): 27.19
2025-12-17 17:47:26,847 - root - INFO - Step: 5 | Loss: 11.97 | Tokens per second: 6558.20 | Training tokens per second (%): 49.78 | MFU (%): 8.54 | TFLOPs: 84.50 | Mem Allocated (GB): 6.52 | Mem Reserved (GB): 39.78 | Max Mem Allocated (GB): 27.19
2025-12-17 17:47:26,848 - root - INFO - Step: 5 | Loss: 11.99 | Tokens per second: 6557.64 | Training tokens per second (%): 60.00 | MFU (%): 8.54 | TFLOPs: 84.50 | Mem Allocated (GB): 6.52 | Mem Reserved (GB): 37.36 | Max Mem Allocated (GB): 27.19
2025-12-17 17:47:26,848 - root - INFO - Step: 5 | Loss: 11.97 | Tokens per second: 6557.84 | Training tokens per second (%): 27.73 | MFU (%): 8.54 | TFLOPs: 84.50 | Mem Allocated (GB): 6.52 | Mem Reserved (GB): 40.07 | Max Mem Allocated (GB): 27.19
2025-12-17 17:47:30,389 - root - INFO - Step: 10 | Loss: 11.94 | Tokens per second: 11567.47 | Training tokens per second (%): 40.84 | MFU (%): 15.07 | TFLOPs: 149.05 | Mem Allocated (GB): 6.52 | Mem Reserved (GB): 39.78 | Max Mem Allocated (GB): 27.19
2025-12-17 17:47:30,389 - root - INFO - Step: 10 | Loss: 11.94 | Tokens per second: 11567.27 | Training tokens per second (%): 30.87 | MFU (%): 15.07 | TFLOPs: 149.05 | Mem Allocated (GB): 6.52 | Mem Reserved (GB): 39.78 | Max Mem Allocated (GB): 27.19
2025-12-17 17:47:30,389 - root - INFO - Step: 10 | Loss: 11.89 | Tokens per second: 11567.55 | Training tokens per second (%): 27.79 | MFU (%): 15.07 | TFLOPs: 149.05 | Mem Allocated (GB): 6.52 | Mem Reserved (GB): 37.36 | Max Mem Allocated (GB): 27.19
2025-12-17 17:47:30,389 - root - INFO - Step: 10 | Loss: 11.94 | Tokens per second: 11567.28 | Training tokens per second (%): 39.41 | MFU (%): 15.07 | TFLOPs: 149.05 | Mem Allocated (GB): 6.52 | Mem Reserved (GB): 40.07 | Max Mem Allocated (GB): 27.19
2025-12-17 17:47:33,839 - root - INFO - Step: 15 | Loss: 11.95 | Tokens per second: 11874.83 | Training tokens per second (%): 31.92 | MFU (%): 15.47 | TFLOPs: 153.01 | Mem Allocated (GB): 6.52 | Mem Reserved (GB): 39.78 | Max Mem Allocated (GB): 27.19
2025-12-17 17:47:33,839 - root - INFO - Step: 15 | Loss: 11.93 | Tokens per second: 11875.83 | Training tokens per second (%): 25.31 | MFU (%): 15.47 | TFLOPs: 153.02 | Mem Allocated (GB): 6.52 | Mem Reserved (GB): 40.07 | Max Mem Allocated (GB): 27.19
2025-12-17 17:47:33,839 - root - INFO - Step: 15 | Loss: 12.03 | Tokens per second: 11874.81 | Training tokens per second (%): 27.36 | MFU (%): 15.47 | TFLOPs: 153.01 | Mem Allocated (GB): 6.52 | Mem Reserved (GB): 39.78 | Max Mem Allocated (GB): 27.19
2025-12-17 17:47:33,840 - root - INFO - Step: 15 | Loss: 11.95 | Tokens per second: 11874.27 | Training tokens per second (%): 52.16 | MFU (%): 15.47 | TFLOPs: 153.00 | Mem Allocated (GB): 6.52 | Mem Reserved (GB): 37.36 | Max Mem Allocated (GB): 27.19
2025-12-17 17:47:37,288 - root - INFO - Step: 20 | Loss: 11.95 | Tokens per second: 11881.16 | Training tokens per second (%): 40.03 | MFU (%): 15.48 | TFLOPs: 153.09 | Mem Allocated (GB): 6.52 | Mem Reserved (GB): 39.78 | Max Mem Allocated (GB): 27.19
2025-12-17 17:47:37,288 - root - INFO - Step: 20 | Loss: 11.95 | Tokens per second: 11881.30 | Training tokens per second (%): 30.31 | MFU (%): 15.48 | TFLOPs: 153.09 | Mem Allocated (GB): 6.52 | Mem Reserved (GB): 37.36 | Max Mem Allocated (GB): 27.19
2025-12-17 17:47:37,288 - root - INFO - Step: 20 | Loss: 11.97 | Tokens per second: 11879.94 | Training tokens per second (%): 36.23 | MFU (%): 15.48 | TFLOPs: 153.08 | Mem Allocated (GB): 6.52 | Mem Reserved (GB): 40.07 | Max Mem Allocated (GB): 27.19
2025-12-17 17:47:37,288 - root - INFO - Step: 20 | Loss: 11.96 | Tokens per second: 11879.52 | Training tokens per second (%): 20.62 | MFU (%): 15.48 | TFLOPs: 153.07 | Mem Allocated (GB): 6.52 | Mem Reserved (GB): 39.78 | Max Mem Allocated (GB): 27.19
2025-12-17 17:47:40,745 - root - INFO - Step: 25 | Loss: 11.98 | Tokens per second: 11849.60 | Training tokens per second (%): 30.03 | MFU (%): 15.44 | TFLOPs: 152.69 | Mem Allocated (GB): 6.52 | Mem Reserved (GB): 39.78 | Max Mem Allocated (GB): 27.19
2025-12-17 17:47:40,745 - root - INFO - Step: 25 | Loss: 11.92 | Tokens per second: 11849.79 | Training tokens per second (%): 26.37 | MFU (%): 15.44 | TFLOPs: 152.69 | Mem Allocated (GB): 6.52 | Mem Reserved (GB): 39.78 | Max Mem Allocated (GB): 27.19
2025-12-17 17:47:40,746 - root - INFO - Step: 25 | Loss: 11.91 | Tokens per second: 11849.07 | Training tokens per second (%): 25.36 | MFU (%): 15.44 | TFLOPs: 152.68 | Mem Allocated (GB): 6.52 | Mem Reserved (GB): 40.07 | Max Mem Allocated (GB): 27.19
2025-12-17 17:47:40,746 - root - INFO - Step: 25 | Loss: 11.99 | Tokens per second: 11849.10 | Training tokens per second (%): 17.13 | MFU (%): 15.44 | TFLOPs: 152.68 | Mem Allocated (GB): 6.52 | Mem Reserved (GB): 37.36 | Max Mem Allocated (GB): 27.19
2025-12-17 17:47:44,198 - root - INFO - Step: 30 | Loss: 11.98 | Tokens per second: 11865.14 | Training tokens per second (%): 34.40 | MFU (%): 15.46 | TFLOPs: 152.89 | Mem Allocated (GB): 6.52 | Mem Reserved (GB): 40.85 | Max Mem Allocated (GB): 27.19
2025-12-17 17:47:44,198 - root - INFO - Step: 30 | Loss: 11.96 | Tokens per second: 11866.27 | Training tokens per second (%): 38.74 | MFU (%): 15.46 | TFLOPs: 152.90 | Mem Allocated (GB): 6.52 | Mem Reserved (GB): 40.07 | Max Mem Allocated (GB): 27.19
2025-12-17 17:47:44,198 - root - INFO - Step: 30 | Loss: 11.93 | Tokens per second: 11866.03 | Training tokens per second (%): 46.38 | MFU (%): 15.46 | TFLOPs: 152.90 | Mem Allocated (GB): 6.52 | Mem Reserved (GB): 39.78 | Max Mem Allocated (GB): 27.19
2025-12-17 17:47:44,198 - root - INFO - Step: 30 | Loss: 11.89 | Tokens per second: 11865.18 | Training tokens per second (%): 34.33 | MFU (%): 15.46 | TFLOPs: 152.89 | Mem Allocated (GB): 6.52 | Mem Reserved (GB): 37.36 | Max Mem Allocated (GB): 27.19
2025-12-17 17:47:47,650 - root - INFO - Step: 35 | Loss: 11.98 | Tokens per second: 11869.25 | Training tokens per second (%): 39.22 | MFU (%): 15.46 | TFLOPs: 152.94 | Mem Allocated (GB): 6.52 | Mem Reserved (GB): 39.78 | Max Mem Allocated (GB): 27.19
2025-12-17 17:47:47,650 - root - INFO - Step: 35 | Loss: 12.00 | Tokens per second: 11869.33 | Training tokens per second (%): 29.41 | MFU (%): 15.46 | TFLOPs: 152.94 | Mem Allocated (GB): 6.52 | Mem Reserved (GB): 40.85 | Max Mem Allocated (GB): 27.19
2025-12-17 17:47:47,650 - root - INFO - Step: 35 | Loss: 11.94 | Tokens per second: 11870.33 | Training tokens per second (%): 30.36 | MFU (%): 15.47 | TFLOPs: 152.95 | Mem Allocated (GB): 6.52 | Mem Reserved (GB): 37.36 | Max Mem Allocated (GB): 27.19
2025-12-17 17:47:47,650 - root - INFO - Step: 35 | Loss: 12.00 | Tokens per second: 11867.99 | Training tokens per second (%): 42.95 | MFU (%): 15.46 | TFLOPs: 152.92 | Mem Allocated (GB): 6.52 | Mem Reserved (GB): 40.07 | Max Mem Allocated (GB): 27.19
2025-12-17 17:47:51,139 - root - INFO - Step: 40 | Loss: 11.98 | Tokens per second: 11744.58 | Training tokens per second (%): 46.49 | MFU (%): 15.30 | TFLOPs: 151.33 | Mem Allocated (GB): 6.52 | Mem Reserved (GB): 40.07 | Max Mem Allocated (GB): 27.19
2025-12-17 17:47:51,139 - root - INFO - Step: 40 | Loss: 11.93 | Tokens per second: 11743.53 | Training tokens per second (%): 29.43 | MFU (%): 15.30 | TFLOPs: 151.32 | Mem Allocated (GB): 6.52 | Mem Reserved (GB): 39.78 | Max Mem Allocated (GB): 27.19
2025-12-17 17:47:51,139 - root - INFO - Step: 40 | Loss: 11.97 | Tokens per second: 11743.45 | Training tokens per second (%): 22.34 | MFU (%): 15.30 | TFLOPs: 151.32 | Mem Allocated (GB): 6.52 | Mem Reserved (GB): 40.85 | Max Mem Allocated (GB): 27.19
2025-12-17 17:47:51,139 - root - INFO - Step: 40 | Loss: 11.93 | Tokens per second: 11742.22 | Training tokens per second (%): 33.12 | MFU (%): 15.30 | TFLOPs: 151.30 | Mem Allocated (GB): 6.52 | Mem Reserved (GB): 37.36 | Max Mem Allocated (GB): 27.19
2025-12-17 17:47:54,587 - root - INFO - Step: 45 | Loss: 12.05 | Tokens per second: 11882.82 | Training tokens per second (%): 47.55 | MFU (%): 15.48 | TFLOPs: 153.11 | Mem Allocated (GB): 6.52 | Mem Reserved (GB): 39.78 | Max Mem Allocated (GB): 27.19
2025-12-17 17:47:54,587 - root - INFO - Step: 45 | Loss: 11.99 | Tokens per second: 11884.00 | Training tokens per second (%): 18.80 | MFU (%): 15.48 | TFLOPs: 153.13 | Mem Allocated (GB): 6.52 | Mem Reserved (GB): 37.36 | Max Mem Allocated (GB): 27.19
2025-12-17 17:47:54,587 - root - INFO - Step: 45 | Loss: 11.95 | Tokens per second: 11882.91 | Training tokens per second (%): 53.08 | MFU (%): 15.48 | TFLOPs: 153.11 | Mem Allocated (GB): 6.52 | Mem Reserved (GB): 40.85 | Max Mem Allocated (GB): 27.19
2025-12-17 17:47:54,587 - root - INFO - Step: 45 | Loss: 11.94 | Tokens per second: 11881.15 | Training tokens per second (%): 59.87 | MFU (%): 15.48 | TFLOPs: 153.09 | Mem Allocated (GB): 6.52 | Mem Reserved (GB): 40.07 | Max Mem Allocated (GB): 27.19
2025-12-17 17:47:58,265 - root - INFO - Step: 50 | Loss: 11.93 | Tokens per second: 11136.80 | Training tokens per second (%): 50.39 | MFU (%): 14.51 | TFLOPs: 143.50 | Mem Allocated (GB): 6.52 | Mem Reserved (GB): 40.85 | Max Mem Allocated (GB): 27.19
2025-12-17 17:47:58,265 - root - INFO - Step: 50 | Loss: 11.92 | Tokens per second: 11136.47 | Training tokens per second (%): 20.11 | MFU (%): 14.51 | TFLOPs: 143.50 | Mem Allocated (GB): 6.52 | Mem Reserved (GB): 37.36 | Max Mem Allocated (GB): 27.19
2025-12-17 17:47:58,266 - root - INFO - Step: 50 | Loss: 11.98 | Tokens per second: 11137.16 | Training tokens per second (%): 37.09 | MFU (%): 14.51 | TFLOPs: 143.51 | Mem Allocated (GB): 6.52 | Mem Reserved (GB): 41.14 | Max Mem Allocated (GB): 27.19
2025-12-17 17:47:58,266 - root - INFO - Step: 50 | Loss: 11.96 | Tokens per second: 11135.23 | Training tokens per second (%): 56.77 | MFU (%): 14.51 | TFLOPs: 143.48 | Mem Allocated (GB): 6.52 | Mem Reserved (GB): 40.85 | Max Mem Allocated (GB): 27.19
2025-12-17 17:48:02,114 - root - INFO - Step: 55 | Loss: 12.00 | Tokens per second: 10644.00 | Training tokens per second (%): 46.30 | MFU (%): 13.87 | TFLOPs: 137.15 | Mem Allocated (GB): 6.52 | Mem Reserved (GB): 40.85 | Max Mem Allocated (GB): 27.19
2025-12-17 17:48:02,114 - root - INFO - Step: 55 | Loss: 12.03 | Tokens per second: 10644.53 | Training tokens per second (%): 28.41 | MFU (%): 13.87 | TFLOPs: 137.16 | Mem Allocated (GB): 6.52 | Mem Reserved (GB): 41.14 | Max Mem Allocated (GB): 27.19
2025-12-17 17:48:02,115 - root - INFO - Step: 55 | Loss: 11.96 | Tokens per second: 10643.33 | Training tokens per second (%): 42.23 | MFU (%): 13.87 | TFLOPs: 137.14 | Mem Allocated (GB): 6.52 | Mem Reserved (GB): 37.36 | Max Mem Allocated (GB): 27.19
2025-12-17 17:48:02,115 - root - INFO - Step: 55 | Loss: 11.93 | Tokens per second: 10643.81 | Training tokens per second (%): 43.58 | MFU (%): 13.87 | TFLOPs: 137.15 | Mem Allocated (GB): 6.52 | Mem Reserved (GB): 40.85 | Max Mem Allocated (GB): 27.19
2025-12-17 17:48:05,544 - root - INFO - Step: 60 | Loss: 11.94 | Tokens per second: 11945.36 | Training tokens per second (%): 49.20 | MFU (%): 15.56 | TFLOPs: 153.92 | Mem Allocated (GB): 6.52 | Mem Reserved (GB): 40.85 | Max Mem Allocated (GB): 27.19
2025-12-17 17:48:05,545 - root - INFO - Step: 60 | Loss: 11.97 | Tokens per second: 11945.79 | Training tokens per second (%): 60.13 | MFU (%): 15.56 | TFLOPs: 153.93 | Mem Allocated (GB): 6.52 | Mem Reserved (GB): 40.85 | Max Mem Allocated (GB): 27.19
2025-12-17 17:48:05,545 - root - INFO - Step: 60 | Loss: 11.99 | Tokens per second: 11945.34 | Training tokens per second (%): 42.74 | MFU (%): 15.56 | TFLOPs: 153.92 | Mem Allocated (GB): 6.52 | Mem Reserved (GB): 37.36 | Max Mem Allocated (GB): 27.19
2025-12-17 17:48:05,545 - root - INFO - Step: 60 | Loss: 11.99 | Tokens per second: 11944.30 | Training tokens per second (%): 35.04 | MFU (%): 15.56 | TFLOPs: 153.91 | Mem Allocated (GB): 6.52 | Mem Reserved (GB): 41.14 | Max Mem Allocated (GB): 27.19
2025-12-17 17:48:09,008 - root - INFO - Step: 65 | Loss: 11.93 | Tokens per second: 11829.49 | Training tokens per second (%): 34.57 | MFU (%): 15.41 | TFLOPs: 152.43 | Mem Allocated (GB): 6.52 | Mem Reserved (GB): 40.85 | Max Mem Allocated (GB): 27.19
2025-12-17 17:48:09,008 - root - INFO - Step: 65 | Loss: 11.92 | Tokens per second: 11830.40 | Training tokens per second (%): 44.82 | MFU (%): 15.41 | TFLOPs: 152.44 | Mem Allocated (GB): 6.52 | Mem Reserved (GB): 40.85 | Max Mem Allocated (GB): 27.19
2025-12-17 17:48:09,008 - root - INFO - Step: 65 | Loss: 12.01 | Tokens per second: 11830.42 | Training tokens per second (%): 32.40 | MFU (%): 15.41 | TFLOPs: 152.44 | Mem Allocated (GB): 6.52 | Mem Reserved (GB): 37.36 | Max Mem Allocated (GB): 27.19
2025-12-17 17:48:09,008 - root - INFO - Step: 65 | Loss: 11.96 | Tokens per second: 11829.08 | Training tokens per second (%): 49.06 | MFU (%): 15.41 | TFLOPs: 152.42 | Mem Allocated (GB): 6.52 | Mem Reserved (GB): 41.14 | Max Mem Allocated (GB): 27.19
2025-12-17 17:48:12,477 - root - INFO - Step: 70 | Loss: 12.00 | Tokens per second: 11811.60 | Training tokens per second (%): 37.08 | MFU (%): 15.39 | TFLOPs: 152.20 | Mem Allocated (GB): 6.52 | Mem Reserved (GB): 41.14 | Max Mem Allocated (GB): 27.19
2025-12-17 17:48:12,477 - root - INFO - Step: 70 | Loss: 11.92 | Tokens per second: 11810.50 | Training tokens per second (%): 26.19 | MFU (%): 15.39 | TFLOPs: 152.18 | Mem Allocated (GB): 6.52 | Mem Reserved (GB): 40.85 | Max Mem Allocated (GB): 27.19
2025-12-17 17:48:12,477 - root - INFO - Step: 70 | Loss: 11.92 | Tokens per second: 11810.42 | Training tokens per second (%): 25.62 | MFU (%): 15.39 | TFLOPs: 152.18 | Mem Allocated (GB): 6.52 | Mem Reserved (GB): 40.85 | Max Mem Allocated (GB): 27.19
2025-12-17 17:48:12,477 - root - INFO - Step: 70 | Loss: 11.97 | Tokens per second: 11809.03 | Training tokens per second (%): 23.64 | MFU (%): 15.39 | TFLOPs: 152.16 | Mem Allocated (GB): 6.52 | Mem Reserved (GB): 37.36 | Max Mem Allocated (GB): 27.19
2025-12-17 17:48:15,924 - root - INFO - Step: 75 | Loss: 11.90 | Tokens per second: 11884.94 | Training tokens per second (%): 37.45 | MFU (%): 15.48 | TFLOPs: 153.14 | Mem Allocated (GB): 6.52 | Mem Reserved (GB): 40.85 | Max Mem Allocated (GB): 27.19
2025-12-17 17:48:15,924 - root - INFO - Step: 75 | Loss: 11.97 | Tokens per second: 11884.99 | Training tokens per second (%): 40.82 | MFU (%): 15.48 | TFLOPs: 153.14 | Mem Allocated (GB): 6.52 | Mem Reserved (GB): 41.14 | Max Mem Allocated (GB): 27.19
2025-12-17 17:48:15,924 - root - INFO - Step: 75 | Loss: 11.95 | Tokens per second: 11885.03 | Training tokens per second (%): 40.97 | MFU (%): 15.48 | TFLOPs: 153.14 | Mem Allocated (GB): 6.52 | Mem Reserved (GB): 40.85 | Max Mem Allocated (GB): 27.19
2025-12-17 17:48:15,924 - root - INFO - Step: 75 | Loss: 11.93 | Tokens per second: 11884.86 | Training tokens per second (%): 36.48 | MFU (%): 15.48 | TFLOPs: 153.14 | Mem Allocated (GB): 6.52 | Mem Reserved (GB): 37.36 | Max Mem Allocated (GB): 27.19
2025-12-17 17:48:19,374 - root - INFO - Step: 80 | Loss: 11.95 | Tokens per second: 11874.75 | Training tokens per second (%): 45.07 | MFU (%): 15.47 | TFLOPs: 153.01 | Mem Allocated (GB): 6.52 | Mem Reserved (GB): 40.85 | Max Mem Allocated (GB): 27.19
2025-12-17 17:48:19,374 - root - INFO - Step: 80 | Loss: 11.96 | Tokens per second: 11874.88 | Training tokens per second (%): 29.85 | MFU (%): 15.47 | TFLOPs: 153.01 | Mem Allocated (GB): 6.52 | Mem Reserved (GB): 41.14 | Max Mem Allocated (GB): 27.19
2025-12-17 17:48:19,374 - root - INFO - Step: 80 | Loss: 11.91 | Tokens per second: 11874.88 | Training tokens per second (%): 35.16 | MFU (%): 15.47 | TFLOPs: 153.01 | Mem Allocated (GB): 6.52 | Mem Reserved (GB): 40.85 | Max Mem Allocated (GB): 27.19
2025-12-17 17:48:19,374 - root - INFO - Step: 80 | Loss: 11.97 | Tokens per second: 11874.58 | Training tokens per second (%): 45.63 | MFU (%): 15.47 | TFLOPs: 153.01 | Mem Allocated (GB): 6.52 | Mem Reserved (GB): 37.36 | Max Mem Allocated (GB): 27.19
2025-12-17 17:48:22,819 - root - INFO - Step: 85 | Loss: 11.95 | Tokens per second: 11893.91 | Training tokens per second (%): 43.90 | MFU (%): 15.50 | TFLOPs: 153.26 | Mem Allocated (GB): 6.52 | Mem Reserved (GB): 40.85 | Max Mem Allocated (GB): 27.19
2025-12-17 17:48:22,819 - root - INFO - Step: 85 | Loss: 11.96 | Tokens per second: 11893.78 | Training tokens per second (%): 37.54 | MFU (%): 15.50 | TFLOPs: 153.25 | Mem Allocated (GB): 6.52 | Mem Reserved (GB): 40.85 | Max Mem Allocated (GB): 27.19
2025-12-17 17:48:22,819 - root - INFO - Step: 85 | Loss: 12.00 | Tokens per second: 11893.90 | Training tokens per second (%): 35.36 | MFU (%): 15.50 | TFLOPs: 153.26 | Mem Allocated (GB): 6.52 | Mem Reserved (GB): 41.14 | Max Mem Allocated (GB): 27.19
2025-12-17 17:48:22,819 - root - INFO - Step: 85 | Loss: 11.94 | Tokens per second: 11893.64 | Training tokens per second (%): 35.73 | MFU (%): 15.50 | TFLOPs: 153.25 | Mem Allocated (GB): 6.52 | Mem Reserved (GB): 37.36 | Max Mem Allocated (GB): 27.19
2025-12-17 17:48:26,269 - root - INFO - Step: 90 | Loss: 11.93 | Tokens per second: 11872.95 | Training tokens per second (%): 36.42 | MFU (%): 15.47 | TFLOPs: 152.99 | Mem Allocated (GB): 6.52 | Mem Reserved (GB): 40.85 | Max Mem Allocated (GB): 27.19
2025-12-17 17:48:26,269 - root - INFO - Step: 90 | Loss: 11.97 | Tokens per second: 11872.82 | Training tokens per second (%): 27.01 | MFU (%): 15.47 | TFLOPs: 152.98 | Mem Allocated (GB): 6.52 | Mem Reserved (GB): 40.85 | Max Mem Allocated (GB): 27.19
2025-12-17 17:48:26,270 - root - INFO - Step: 90 | Loss: 12.00 | Tokens per second: 11873.70 | Training tokens per second (%): 35.06 | MFU (%): 15.47 | TFLOPs: 153.00 | Mem Allocated (GB): 6.52 | Mem Reserved (GB): 37.36 | Max Mem Allocated (GB): 27.19
2025-12-17 17:48:26,270 - root - INFO - Step: 90 | Loss: 11.95 | Tokens per second: 11871.53 | Training tokens per second (%): 23.58 | MFU (%): 15.47 | TFLOPs: 152.97 | Mem Allocated (GB): 6.52 | Mem Reserved (GB): 41.14 | Max Mem Allocated (GB): 27.19
2025-12-17 17:48:29,721 - root - INFO - Step: 95 | Loss: 11.95 | Tokens per second: 11871.06 | Training tokens per second (%): 38.12 | MFU (%): 15.47 | TFLOPs: 152.96 | Mem Allocated (GB): 6.52 | Mem Reserved (GB): 40.85 | Max Mem Allocated (GB): 27.19
2025-12-17 17:48:29,721 - root - INFO - Step: 95 | Loss: 11.98 | Tokens per second: 11872.25 | Training tokens per second (%): 52.12 | MFU (%): 15.47 | TFLOPs: 152.98 | Mem Allocated (GB): 6.52 | Mem Reserved (GB): 41.14 | Max Mem Allocated (GB): 27.19
2025-12-17 17:48:29,721 - root - INFO - Step: 95 | Loss: 11.92 | Tokens per second: 11871.18 | Training tokens per second (%): 57.74 | MFU (%): 15.47 | TFLOPs: 152.96 | Mem Allocated (GB): 6.52 | Mem Reserved (GB): 40.85 | Max Mem Allocated (GB): 27.19
2025-12-17 17:48:29,721 - root - INFO - Step: 95 | Loss: 11.99 | Tokens per second: 11870.20 | Training tokens per second (%): 19.16 | MFU (%): 15.47 | TFLOPs: 152.95 | Mem Allocated (GB): 6.52 | Mem Reserved (GB): 37.36 | Max Mem Allocated (GB): 27.19
2025-12-17 17:48:33,177 - root - INFO - Step: 100 | Loss: 11.97 | Tokens per second: 11854.40 | Training tokens per second (%): 58.66 | MFU (%): 15.44 | TFLOPs: 152.75 | Mem Allocated (GB): 6.52 | Mem Reserved (GB): 40.85 | Max Mem Allocated (GB): 27.19
2025-12-17 17:48:33,177 - root - INFO - Step: 100 | Loss: 11.84 | Tokens per second: 11854.26 | Training tokens per second (%): 32.33 | MFU (%): 15.44 | TFLOPs: 152.75 | Mem Allocated (GB): 6.52 | Mem Reserved (GB): 40.85 | Max Mem Allocated (GB): 27.19
2025-12-17 17:48:33,177 - root - INFO - Step: 100 | Loss: 11.98 | Tokens per second: 11855.67 | Training tokens per second (%): 64.75 | MFU (%): 15.45 | TFLOPs: 152.76 | Mem Allocated (GB): 6.52 | Mem Reserved (GB): 37.36 | Max Mem Allocated (GB): 27.19
2025-12-17 17:48:33,177 - root - INFO - Step: 100 | Loss: 11.93 | Tokens per second: 11852.87 | Training tokens per second (%): 38.41 | MFU (%): 15.44 | TFLOPs: 152.73 | Mem Allocated (GB): 6.52 | Mem Reserved (GB): 41.14 | Max Mem Allocated (GB): 27.19
2025-12-17 17:48:36,624 - root - INFO - Step: 105 | Loss: 11.90 | Tokens per second: 11885.16 | Training tokens per second (%): 13.50 | MFU (%): 15.48 | TFLOPs: 153.14 | Mem Allocated (GB): 6.52 | Mem Reserved (GB): 40.85 | Max Mem Allocated (GB): 27.19
2025-12-17 17:48:36,624 - root - INFO - Step: 105 | Loss: 11.96 | Tokens per second: 11886.38 | Training tokens per second (%): 55.27 | MFU (%): 15.49 | TFLOPs: 153.16 | Mem Allocated (GB): 6.52 | Mem Reserved (GB): 41.14 | Max Mem Allocated (GB): 27.19
2025-12-17 17:48:36,624 - root - INFO - Step: 105 | Loss: 11.96 | Tokens per second: 11885.23 | Training tokens per second (%): 41.88 | MFU (%): 15.48 | TFLOPs: 153.14 | Mem Allocated (GB): 6.52 | Mem Reserved (GB): 40.85 | Max Mem Allocated (GB): 27.19
2025-12-17 17:48:36,624 - root - INFO - Step: 105 | Loss: 12.00 | Tokens per second: 11883.82 | Training tokens per second (%): 56.19 | MFU (%): 15.48 | TFLOPs: 153.13 | Mem Allocated (GB): 6.52 | Mem Reserved (GB): 37.36 | Max Mem Allocated (GB): 27.19
2025-12-17 17:48:40,088 - root - INFO - Step: 110 | Loss: 11.95 | Tokens per second: 11828.58 | Training tokens per second (%): 38.24 | MFU (%): 15.41 | TFLOPs: 152.41 | Mem Allocated (GB): 6.52 | Mem Reserved (GB): 40.85 | Max Mem Allocated (GB): 27.19
2025-12-17 17:48:40,088 - root - INFO - Step: 110 | Loss: 11.95 | Tokens per second: 11828.67 | Training tokens per second (%): 53.11 | MFU (%): 15.41 | TFLOPs: 152.42 | Mem Allocated (GB): 6.52 | Mem Reserved (GB): 41.14 | Max Mem Allocated (GB): 27.19
2025-12-17 17:48:40,088 - root - INFO - Step: 110 | Loss: 11.98 | Tokens per second: 11829.93 | Training tokens per second (%): 22.74 | MFU (%): 15.41 | TFLOPs: 152.43 | Mem Allocated (GB): 6.52 | Mem Reserved (GB): 37.36 | Max Mem Allocated (GB): 27.19
2025-12-17 17:48:40,088 - root - INFO - Step: 110 | Loss: 11.95 | Tokens per second: 11826.99 | Training tokens per second (%): 30.21 | MFU (%): 15.41 | TFLOPs: 152.39 | Mem Allocated (GB): 6.52 | Mem Reserved (GB): 40.85 | Max Mem Allocated (GB): 27.19
2025-12-17 17:48:43,553 - root - INFO - Step: 115 | Loss: 11.95 | Tokens per second: 11825.69 | Training tokens per second (%): 33.34 | MFU (%): 15.41 | TFLOPs: 152.38 | Mem Allocated (GB): 6.52 | Mem Reserved (GB): 40.85 | Max Mem Allocated (GB): 27.19
2025-12-17 17:48:43,553 - root - INFO - Step: 115 | Loss: 11.98 | Tokens per second: 11822.91 | Training tokens per second (%): 32.16 | MFU (%): 15.40 | TFLOPs: 152.34 | Mem Allocated (GB): 6.52 | Mem Reserved (GB): 40.85 | Max Mem Allocated (GB): 27.19
2025-12-17 17:48:43,553 - root - INFO - Step: 115 | Loss: 11.98 | Tokens per second: 11822.74 | Training tokens per second (%): 60.48 | MFU (%): 15.40 | TFLOPs: 152.34 | Mem Allocated (GB): 6.52 | Mem Reserved (GB): 41.14 | Max Mem Allocated (GB): 27.19
2025-12-17 17:48:43,553 - root - INFO - Step: 115 | Loss: 11.93 | Tokens per second: 11822.80 | Training tokens per second (%): 23.98 | MFU (%): 15.40 | TFLOPs: 152.34 | Mem Allocated (GB): 6.52 | Mem Reserved (GB): 37.36 | Max Mem Allocated (GB): 27.19
2025-12-17 17:48:47,237 - root - INFO - Step: 120 | Loss: 11.93 | Tokens per second: 11119.79 | Training tokens per second (%): 29.38 | MFU (%): 14.49 | TFLOPs: 143.28 | Mem Allocated (GB): 6.52 | Mem Reserved (GB): 40.85 | Max Mem Allocated (GB): 27.19
2025-12-17 17:48:47,237 - root - INFO - Step: 120 | Loss: 12.00 | Tokens per second: 11120.84 | Training tokens per second (%): 55.53 | MFU (%): 14.49 | TFLOPs: 143.30 | Mem Allocated (GB): 6.52 | Mem Reserved (GB): 41.14 | Max Mem Allocated (GB): 27.19
2025-12-17 17:48:47,237 - root - INFO - Step: 120 | Loss: 11.93 | Tokens per second: 11120.73 | Training tokens per second (%): 14.97 | MFU (%): 14.49 | TFLOPs: 143.29 | Mem Allocated (GB): 6.52 | Mem Reserved (GB): 40.85 | Max Mem Allocated (GB): 27.19
2025-12-17 17:48:47,237 - root - INFO - Step: 120 | Loss: 11.93 | Tokens per second: 11119.54 | Training tokens per second (%): 14.89 | MFU (%): 14.49 | TFLOPs: 143.28 | Mem Allocated (GB): 6.52 | Mem Reserved (GB): 37.36 | Max Mem Allocated (GB): 27.19
2025-12-17 17:48:51,100 - root - INFO - Step: 125 | Loss: 12.06 | Tokens per second: 10605.00 | Training tokens per second (%): 24.38 | MFU (%): 13.82 | TFLOPs: 136.65 | Mem Allocated (GB): 6.52 | Mem Reserved (GB): 40.85 | Max Mem Allocated (GB): 27.19
2025-12-17 17:48:51,100 - root - INFO - Step: 125 | Loss: 11.92 | Tokens per second: 10605.10 | Training tokens per second (%): 49.58 | MFU (%): 13.82 | TFLOPs: 136.65 | Mem Allocated (GB): 6.52 | Mem Reserved (GB): 40.85 | Max Mem Allocated (GB): 27.19
2025-12-17 17:48:51,100 - root - INFO - Step: 125 | Loss: 11.96 | Tokens per second: 10605.11 | Training tokens per second (%): 38.47 | MFU (%): 13.82 | TFLOPs: 136.65 | Mem Allocated (GB): 6.52 | Mem Reserved (GB): 41.14 | Max Mem Allocated (GB): 27.19
2025-12-17 17:48:51,100 - root - INFO - Step: 125 | Loss: 11.95 | Tokens per second: 10604.92 | Training tokens per second (%): 28.20 | MFU (%): 13.82 | TFLOPs: 136.65 | Mem Allocated (GB): 6.52 | Mem Reserved (GB): 37.36 | Max Mem Allocated (GB): 27.19
2025-12-17 17:48:54,572 - root - INFO - Step: 130 | Loss: 12.00 | Tokens per second: 11800.46 | Training tokens per second (%): 39.61 | MFU (%): 15.37 | TFLOPs: 152.05 | Mem Allocated (GB): 6.52 | Mem Reserved (GB): 40.85 | Max Mem Allocated (GB): 27.19
2025-12-17 17:48:54,572 - root - INFO - Step: 130 | Loss: 11.90 | Tokens per second: 11799.15 | Training tokens per second (%): 76.61 | MFU (%): 15.37 | TFLOPs: 152.04 | Mem Allocated (GB): 6.52 | Mem Reserved (GB): 40.85 | Max Mem Allocated (GB): 27.19
2025-12-17 17:48:54,572 - root - INFO - Step: 130 | Loss: 11.87 | Tokens per second: 11800.22 | Training tokens per second (%): 29.72 | MFU (%): 15.37 | TFLOPs: 152.05 | Mem Allocated (GB): 6.52 | Mem Reserved (GB): 37.36 | Max Mem Allocated (GB): 27.19
2025-12-17 17:48:54,572 - root - INFO - Step: 130 | Loss: 11.93 | Tokens per second: 11799.13 | Training tokens per second (%): 23.79 | MFU (%): 15.37 | TFLOPs: 152.04 | Mem Allocated (GB): 6.52 | Mem Reserved (GB): 41.14 | Max Mem Allocated (GB): 27.19
2025-12-17 17:48:58,040 - root - INFO - Step: 135 | Loss: 11.98 | Tokens per second: 11814.24 | Training tokens per second (%): 43.57 | MFU (%): 15.39 | TFLOPs: 152.23 | Mem Allocated (GB): 6.52 | Mem Reserved (GB): 40.85 | Max Mem Allocated (GB): 27.19
2025-12-17 17:48:58,040 - root - INFO - Step: 135 | Loss: 11.92 | Tokens per second: 11815.22 | Training tokens per second (%): 47.35 | MFU (%): 15.39 | TFLOPs: 152.24 | Mem Allocated (GB): 6.52 | Mem Reserved (GB): 40.85 | Max Mem Allocated (GB): 27.19
2025-12-17 17:48:58,040 - root - INFO - Step: 135 | Loss: 11.88 | Tokens per second: 11814.03 | Training tokens per second (%): 15.90 | MFU (%): 15.39 | TFLOPs: 152.23 | Mem Allocated (GB): 6.52 | Mem Reserved (GB): 37.36 | Max Mem Allocated (GB): 27.19
2025-12-17 17:48:58,040 - root - INFO - Step: 135 | Loss: 11.90 | Tokens per second: 11813.92 | Training tokens per second (%): 44.98 | MFU (%): 15.39 | TFLOPs: 152.23 | Mem Allocated (GB): 6.52 | Mem Reserved (GB): 41.14 | Max Mem Allocated (GB): 27.19
2025-12-17 17:49:01,501 - root - INFO - Step: 140 | Loss: 11.98 | Tokens per second: 11836.56 | Training tokens per second (%): 29.57 | MFU (%): 15.42 | TFLOPs: 152.52 | Mem Allocated (GB): 6.52 | Mem Reserved (GB): 40.85 | Max Mem Allocated (GB): 27.19
2025-12-17 17:49:01,501 - root - INFO - Step: 140 | Loss: 11.97 | Tokens per second: 11837.30 | Training tokens per second (%): 41.25 | MFU (%): 15.42 | TFLOPs: 152.53 | Mem Allocated (GB): 6.52 | Mem Reserved (GB): 41.14 | Max Mem Allocated (GB): 27.19
2025-12-17 17:49:01,501 - root - INFO - Step: 140 | Loss: 11.96 | Tokens per second: 11837.25 | Training tokens per second (%): 44.90 | MFU (%): 15.42 | TFLOPs: 152.53 | Mem Allocated (GB): 6.52 | Mem Reserved (GB): 37.36 | Max Mem Allocated (GB): 27.19
2025-12-17 17:49:01,502 - root - INFO - Step: 140 | Loss: 11.98 | Tokens per second: 11835.55 | Training tokens per second (%): 51.46 | MFU (%): 15.42 | TFLOPs: 152.50 | Mem Allocated (GB): 6.52 | Mem Reserved (GB): 40.85 | Max Mem Allocated (GB): 27.19
2025-12-17 17:49:04,968 - root - INFO - Step: 145 | Loss: 11.95 | Tokens per second: 11816.66 | Training tokens per second (%): 23.10 | MFU (%): 15.40 | TFLOPs: 152.26 | Mem Allocated (GB): 6.52 | Mem Reserved (GB): 40.85 | Max Mem Allocated (GB): 27.19
2025-12-17 17:49:04,968 - root - INFO - Step: 145 | Loss: 11.90 | Tokens per second: 11817.04 | Training tokens per second (%): 28.04 | MFU (%): 15.40 | TFLOPs: 152.27 | Mem Allocated (GB): 6.52 | Mem Reserved (GB): 37.36 | Max Mem Allocated (GB): 27.19
2025-12-17 17:49:04,969 - root - INFO - Step: 145 | Loss: 11.98 | Tokens per second: 11816.27 | Training tokens per second (%): 40.03 | MFU (%): 15.39 | TFLOPs: 152.26 | Mem Allocated (GB): 6.52 | Mem Reserved (GB): 40.85 | Max Mem Allocated (GB): 27.19
2025-12-17 17:49:04,969 - root - INFO - Step: 145 | Loss: 11.90 | Tokens per second: 11815.57 | Training tokens per second (%): 40.22 | MFU (%): 15.39 | TFLOPs: 152.25 | Mem Allocated (GB): 6.52 | Mem Reserved (GB): 41.14 | Max Mem Allocated (GB): 27.19
2025-12-17 17:49:08,405 - root - INFO - Step: 150 | Loss: 11.97 | Tokens per second: 11922.56 | Training tokens per second (%): 61.47 | MFU (%): 15.53 | TFLOPs: 153.63 | Mem Allocated (GB): 6.52 | Mem Reserved (GB): 40.85 | Max Mem Allocated (GB): 27.19
2025-12-17 17:49:08,405 - root - INFO - Step: 150 | Loss: 11.94 | Tokens per second: 11923.63 | Training tokens per second (%): 41.10 | MFU (%): 15.53 | TFLOPs: 153.64 | Mem Allocated (GB): 6.52 | Mem Reserved (GB): 40.85 | Max Mem Allocated (GB): 27.19
2025-12-17 17:49:08,405 - root - INFO - Step: 150 | Loss: 11.95 | Tokens per second: 11921.06 | Training tokens per second (%): 25.32 | MFU (%): 15.53 | TFLOPs: 153.61 | Mem Allocated (GB): 6.52 | Mem Reserved (GB): 37.36 | Max Mem Allocated (GB): 27.19
2025-12-17 17:49:08,405 - root - INFO - Step: 150 | Loss: 11.95 | Tokens per second: 11922.24 | Training tokens per second (%): 50.62 | MFU (%): 15.53 | TFLOPs: 153.62 | Mem Allocated (GB): 6.52 | Mem Reserved (GB): 41.14 | Max Mem Allocated (GB): 27.19
2025-12-17 17:49:11,855 - root - INFO - Step: 155 | Loss: 11.92 | Tokens per second: 11876.33 | Training tokens per second (%): 36.85 | MFU (%): 15.47 | TFLOPs: 153.03 | Mem Allocated (GB): 6.52 | Mem Reserved (GB): 40.85 | Max Mem Allocated (GB): 27.19
2025-12-17 17:49:11,855 - root - INFO - Step: 155 | Loss: 11.91 | Tokens per second: 11877.64 | Training tokens per second (%): 35.81 | MFU (%): 15.47 | TFLOPs: 153.05 | Mem Allocated (GB): 6.52 | Mem Reserved (GB): 41.14 | Max Mem Allocated (GB): 27.19
2025-12-17 17:49:11,855 - root - INFO - Step: 155 | Loss: 11.96 | Tokens per second: 11876.29 | Training tokens per second (%): 37.36 | MFU (%): 15.47 | TFLOPs: 153.03 | Mem Allocated (GB): 6.52 | Mem Reserved (GB): 40.85 | Max Mem Allocated (GB): 27.19
2025-12-17 17:49:11,855 - root - INFO - Step: 155 | Loss: 11.96 | Tokens per second: 11876.23 | Training tokens per second (%): 61.52 | MFU (%): 15.47 | TFLOPs: 153.03 | Mem Allocated (GB): 6.52 | Mem Reserved (GB): 37.36 | Max Mem Allocated (GB): 27.19
2025-12-17 17:49:15,333 - root - INFO - Step: 160 | Loss: 11.92 | Tokens per second: 11779.34 | Training tokens per second (%): 28.84 | MFU (%): 15.35 | TFLOPs: 151.78 | Mem Allocated (GB): 6.52 | Mem Reserved (GB): 40.85 | Max Mem Allocated (GB): 27.19
2025-12-17 17:49:15,333 - root - INFO - Step: 160 | Loss: 11.94 | Tokens per second: 11780.62 | Training tokens per second (%): 66.84 | MFU (%): 15.35 | TFLOPs: 151.80 | Mem Allocated (GB): 6.52 | Mem Reserved (GB): 37.36 | Max Mem Allocated (GB): 27.19
2025-12-17 17:49:15,333 - root - INFO - Step: 160 | Loss: 11.90 | Tokens per second: 11779.40 | Training tokens per second (%): 38.63 | MFU (%): 15.35 | TFLOPs: 151.78 | Mem Allocated (GB): 6.52 | Mem Reserved (GB): 40.85 | Max Mem Allocated (GB): 27.19
2025-12-17 17:49:15,333 - root - INFO - Step: 160 | Loss: 12.02 | Tokens per second: 11778.06 | Training tokens per second (%): 39.92 | MFU (%): 15.35 | TFLOPs: 151.76 | Mem Allocated (GB): 6.52 | Mem Reserved (GB): 41.14 | Max Mem Allocated (GB): 27.19
2025-12-17 17:49:18,783 - root - INFO - Step: 165 | Loss: 11.98 | Tokens per second: 11874.04 | Training tokens per second (%): 18.36 | MFU (%): 15.47 | TFLOPs: 153.00 | Mem Allocated (GB): 6.52 | Mem Reserved (GB): 40.85 | Max Mem Allocated (GB): 27.19
2025-12-17 17:49:18,783 - root - INFO - Step: 165 | Loss: 12.02 | Tokens per second: 11873.68 | Training tokens per second (%): 35.44 | MFU (%): 15.47 | TFLOPs: 153.00 | Mem Allocated (GB): 6.52 | Mem Reserved (GB): 40.85 | Max Mem Allocated (GB): 27.19
2025-12-17 17:49:18,783 - root - INFO - Step: 165 | Loss: 11.98 | Tokens per second: 11873.30 | Training tokens per second (%): 41.67 | MFU (%): 15.47 | TFLOPs: 152.99 | Mem Allocated (GB): 6.52 | Mem Reserved (GB): 37.36 | Max Mem Allocated (GB): 27.19
2025-12-17 17:49:18,783 - root - INFO - Step: 165 | Loss: 11.91 | Tokens per second: 11873.66 | Training tokens per second (%): 49.42 | MFU (%): 15.47 | TFLOPs: 153.00 | Mem Allocated (GB): 6.52 | Mem Reserved (GB): 41.14 | Max Mem Allocated (GB): 27.19
2025-12-17 17:49:22,231 - root - INFO - Step: 170 | Loss: 11.83 | Tokens per second: 11884.16 | Training tokens per second (%): 50.27 | MFU (%): 15.48 | TFLOPs: 153.13 | Mem Allocated (GB): 6.52 | Mem Reserved (GB): 40.85 | Max Mem Allocated (GB): 27.19
2025-12-17 17:49:22,231 - root - INFO - Step: 170 | Loss: 11.99 | Tokens per second: 11884.37 | Training tokens per second (%): 66.35 | MFU (%): 15.48 | TFLOPs: 153.13 | Mem Allocated (GB): 6.52 | Mem Reserved (GB): 40.85 | Max Mem Allocated (GB): 27.19
2025-12-17 17:49:22,231 - root - INFO - Step: 170 | Loss: 11.96 | Tokens per second: 11884.89 | Training tokens per second (%): 36.91 | MFU (%): 15.48 | TFLOPs: 153.14 | Mem Allocated (GB): 6.52 | Mem Reserved (GB): 37.36 | Max Mem Allocated (GB): 27.19
2025-12-17 17:49:22,231 - root - INFO - Step: 170 | Loss: 11.94 | Tokens per second: 11883.81 | Training tokens per second (%): 32.23 | MFU (%): 15.48 | TFLOPs: 153.13 | Mem Allocated (GB): 6.52 | Mem Reserved (GB): 41.14 | Max Mem Allocated (GB): 27.19
2025-12-17 17:49:25,673 - root - INFO - Step: 175 | Loss: 11.94 | Tokens per second: 11901.81 | Training tokens per second (%): 48.25 | MFU (%): 15.51 | TFLOPs: 153.36 | Mem Allocated (GB): 6.52 | Mem Reserved (GB): 40.85 | Max Mem Allocated (GB): 27.19
2025-12-17 17:49:25,673 - root - INFO - Step: 175 | Loss: 11.96 | Tokens per second: 11900.69 | Training tokens per second (%): 32.18 | MFU (%): 15.50 | TFLOPs: 153.34 | Mem Allocated (GB): 6.52 | Mem Reserved (GB): 40.85 | Max Mem Allocated (GB): 27.19
2025-12-17 17:49:25,673 - root - INFO - Step: 175 | Loss: 11.96 | Tokens per second: 11901.82 | Training tokens per second (%): 40.45 | MFU (%): 15.51 | TFLOPs: 153.36 | Mem Allocated (GB): 6.52 | Mem Reserved (GB): 41.14 | Max Mem Allocated (GB): 27.19
2025-12-17 17:49:25,673 - root - INFO - Step: 175 | Loss: 11.99 | Tokens per second: 11900.45 | Training tokens per second (%): 62.35 | MFU (%): 15.50 | TFLOPs: 153.34 | Mem Allocated (GB): 6.52 | Mem Reserved (GB): 37.36 | Max Mem Allocated (GB): 27.19
2025-12-17 17:49:29,123 - root - INFO - Step: 180 | Loss: 11.92 | Tokens per second: 11874.81 | Training tokens per second (%): 21.20 | MFU (%): 15.47 | TFLOPs: 153.01 | Mem Allocated (GB): 6.52 | Mem Reserved (GB): 40.85 | Max Mem Allocated (GB): 27.19
2025-12-17 17:49:29,123 - root - INFO - Step: 180 | Loss: 11.98 | Tokens per second: 11875.73 | Training tokens per second (%): 23.68 | MFU (%): 15.47 | TFLOPs: 153.02 | Mem Allocated (GB): 6.52 | Mem Reserved (GB): 41.14 | Max Mem Allocated (GB): 27.19
2025-12-17 17:49:29,123 - root - INFO - Step: 180 | Loss: 11.95 | Tokens per second: 11875.79 | Training tokens per second (%): 31.24 | MFU (%): 15.47 | TFLOPs: 153.02 | Mem Allocated (GB): 6.52 | Mem Reserved (GB): 37.36 | Max Mem Allocated (GB): 27.19
2025-12-17 17:49:29,124 - root - INFO - Step: 180 | Loss: 11.98 | Tokens per second: 11874.13 | Training tokens per second (%): 29.83 | MFU (%): 15.47 | TFLOPs: 153.00 | Mem Allocated (GB): 6.52 | Mem Reserved (GB): 40.85 | Max Mem Allocated (GB): 27.19
2025-12-17 17:49:32,763 - root - INFO - Step: 185 | Loss: 12.01 | Tokens per second: 11256.06 | Training tokens per second (%): 36.61 | MFU (%): 14.67 | TFLOPs: 145.04 | Mem Allocated (GB): 6.52 | Mem Reserved (GB): 40.85 | Max Mem Allocated (GB): 27.19
2025-12-17 17:49:32,763 - root - INFO - Step: 185 | Loss: 11.96 | Tokens per second: 11257.16 | Training tokens per second (%): 36.83 | MFU (%): 14.67 | TFLOPs: 145.05 | Mem Allocated (GB): 6.52 | Mem Reserved (GB): 40.85 | Max Mem Allocated (GB): 27.19
2025-12-17 17:49:32,763 - root - INFO - Step: 185 | Loss: 12.00 | Tokens per second: 11255.45 | Training tokens per second (%): 68.74 | MFU (%): 14.66 | TFLOPs: 145.03 | Mem Allocated (GB): 6.52 | Mem Reserved (GB): 41.14 | Max Mem Allocated (GB): 27.19
2025-12-17 17:49:32,763 - root - INFO - Step: 185 | Loss: 11.89 | Tokens per second: 11254.81 | Training tokens per second (%): 26.38 | MFU (%): 14.66 | TFLOPs: 145.02 | Mem Allocated (GB): 6.52 | Mem Reserved (GB): 37.36 | Max Mem Allocated (GB): 27.19
2025-12-17 17:49:36,608 - root - INFO - Step: 190 | Loss: 11.94 | Tokens per second: 10656.28 | Training tokens per second (%): 70.87 | MFU (%): 13.88 | TFLOPs: 137.31 | Mem Allocated (GB): 6.52 | Mem Reserved (GB): 40.85 | Max Mem Allocated (GB): 27.19
2025-12-17 17:49:36,608 - root - INFO - Step: 190 | Loss: 11.95 | Tokens per second: 10657.27 | Training tokens per second (%): 25.28 | MFU (%): 13.88 | TFLOPs: 137.32 | Mem Allocated (GB): 6.52 | Mem Reserved (GB): 37.36 | Max Mem Allocated (GB): 27.19
2025-12-17 17:49:36,608 - root - INFO - Step: 190 | Loss: 11.93 | Tokens per second: 10656.34 | Training tokens per second (%): 46.43 | MFU (%): 13.88 | TFLOPs: 137.31 | Mem Allocated (GB): 6.52 | Mem Reserved (GB): 40.85 | Max Mem Allocated (GB): 27.19
2025-12-17 17:49:36,608 - root - INFO - Step: 190 | Loss: 11.97 | Tokens per second: 10655.56 | Training tokens per second (%): 35.03 | MFU (%): 13.88 | TFLOPs: 137.30 | Mem Allocated (GB): 6.52 | Mem Reserved (GB): 41.14 | Max Mem Allocated (GB): 27.19
2025-12-17 17:49:40,056 - root - INFO - Step: 195 | Loss: 11.95 | Tokens per second: 11880.47 | Training tokens per second (%): 49.31 | MFU (%): 15.48 | TFLOPs: 153.08 | Mem Allocated (GB): 6.52 | Mem Reserved (GB): 41.14 | Max Mem Allocated (GB): 27.19
2025-12-17 17:49:40,056 - root - INFO - Step: 195 | Loss: 12.00 | Tokens per second: 11879.27 | Training tokens per second (%): 26.32 | MFU (%): 15.48 | TFLOPs: 153.07 | Mem Allocated (GB): 6.52 | Mem Reserved (GB): 37.36 | Max Mem Allocated (GB): 27.19
2025-12-17 17:49:40,057 - root - INFO - Step: 195 | Loss: 11.97 | Tokens per second: 11879.14 | Training tokens per second (%): 31.45 | MFU (%): 15.48 | TFLOPs: 153.07 | Mem Allocated (GB): 6.52 | Mem Reserved (GB): 40.85 | Max Mem Allocated (GB): 27.19
2025-12-17 17:49:40,057 - root - INFO - Step: 195 | Loss: 11.98 | Tokens per second: 11877.76 | Training tokens per second (%): 53.39 | MFU (%): 15.48 | TFLOPs: 153.05 | Mem Allocated (GB): 6.52 | Mem Reserved (GB): 40.85 | Max Mem Allocated (GB): 27.19
2025-12-17 17:49:43,507 - root - INFO - Step: 200 | Loss: 11.93 | Tokens per second: 11874.08 | Training tokens per second (%): 59.52 | MFU (%): 15.47 | TFLOPs: 153.00 | Mem Allocated (GB): 6.52 | Mem Reserved (GB): 40.85 | Max Mem Allocated (GB): 27.19
2025-12-17 17:49:43,507 - root - INFO - Step: 200 | Loss: 11.97 | Tokens per second: 11872.83 | Training tokens per second (%): 41.25 | MFU (%): 15.47 | TFLOPs: 152.98 | Mem Allocated (GB): 6.52 | Mem Reserved (GB): 40.85 | Max Mem Allocated (GB): 27.19
2025-12-17 17:49:43,507 - root - INFO - Step: 200 | Loss: 11.96 | Tokens per second: 11872.88 | Training tokens per second (%): 52.17 | MFU (%): 15.47 | TFLOPs: 152.99 | Mem Allocated (GB): 6.52 | Mem Reserved (GB): 41.14 | Max Mem Allocated (GB): 27.19
2025-12-17 17:49:43,508 - root - INFO - Step: 200 | Loss: 11.92 | Tokens per second: 11871.34 | Training tokens per second (%): 35.04 | MFU (%): 15.47 | TFLOPs: 152.97 | Mem Allocated (GB): 6.52 | Mem Reserved (GB): 37.36 | Max Mem Allocated (GB): 27.19
2025-12-17 17:49:46,963 - root - INFO - Step: 205 | Loss: 11.93 | Tokens per second: 11855.78 | Training tokens per second (%): 27.65 | MFU (%): 15.45 | TFLOPs: 152.77 | Mem Allocated (GB): 6.52 | Mem Reserved (GB): 40.85 | Max Mem Allocated (GB): 27.19
2025-12-17 17:49:46,963 - root - INFO - Step: 205 | Loss: 11.87 | Tokens per second: 11855.69 | Training tokens per second (%): 26.38 | MFU (%): 15.45 | TFLOPs: 152.76 | Mem Allocated (GB): 6.52 | Mem Reserved (GB): 40.85 | Max Mem Allocated (GB): 27.19
2025-12-17 17:49:46,963 - root - INFO - Step: 205 | Loss: 11.98 | Tokens per second: 11854.66 | Training tokens per second (%): 25.59 | MFU (%): 15.44 | TFLOPs: 152.75 | Mem Allocated (GB): 6.52 | Mem Reserved (GB): 41.14 | Max Mem Allocated (GB): 27.19
2025-12-17 17:49:46,963 - root - INFO - Step: 205 | Loss: 11.96 | Tokens per second: 11855.59 | Training tokens per second (%): 63.92 | MFU (%): 15.45 | TFLOPs: 152.76 | Mem Allocated (GB): 6.52 | Mem Reserved (GB): 37.36 | Max Mem Allocated (GB): 27.19
2025-12-17 17:49:50,414 - root - INFO - Step: 210 | Loss: 11.97 | Tokens per second: 11873.54 | Training tokens per second (%): 15.79 | MFU (%): 15.47 | TFLOPs: 152.99 | Mem Allocated (GB): 6.52 | Mem Reserved (GB): 40.85 | Max Mem Allocated (GB): 27.19
2025-12-17 17:49:50,414 - root - INFO - Step: 210 | Loss: 11.95 | Tokens per second: 11874.57 | Training tokens per second (%): 45.62 | MFU (%): 15.47 | TFLOPs: 153.01 | Mem Allocated (GB): 6.52 | Mem Reserved (GB): 41.14 | Max Mem Allocated (GB): 27.19
2025-12-17 17:49:50,414 - root - INFO - Step: 210 | Loss: 11.91 | Tokens per second: 11874.56 | Training tokens per second (%): 21.88 | MFU (%): 15.47 | TFLOPs: 153.01 | Mem Allocated (GB): 6.52 | Mem Reserved (GB): 37.36 | Max Mem Allocated (GB): 27.19
2025-12-17 17:49:50,414 - root - INFO - Step: 210 | Loss: 11.92 | Tokens per second: 11871.84 | Training tokens per second (%): 22.44 | MFU (%): 15.47 | TFLOPs: 152.97 | Mem Allocated (GB): 6.52 | Mem Reserved (GB): 40.85 | Max Mem Allocated (GB): 27.19
2025-12-17 17:49:53,861 - root - INFO - Step: 215 | Loss: 11.98 | Tokens per second: 11883.64 | Training tokens per second (%): 38.17 | MFU (%): 15.48 | TFLOPs: 153.12 | Mem Allocated (GB): 6.52 | Mem Reserved (GB): 40.85 | Max Mem Allocated (GB): 27.19
2025-12-17 17:49:53,861 - root - INFO - Step: 215 | Loss: 11.96 | Tokens per second: 11883.65 | Training tokens per second (%): 56.14 | MFU (%): 15.48 | TFLOPs: 153.12 | Mem Allocated (GB): 6.52 | Mem Reserved (GB): 41.14 | Max Mem Allocated (GB): 27.19
2025-12-17 17:49:53,861 - root - INFO - Step: 215 | Loss: 11.89 | Tokens per second: 11885.01 | Training tokens per second (%): 20.32 | MFU (%): 15.48 | TFLOPs: 153.14 | Mem Allocated (GB): 6.52 | Mem Reserved (GB): 40.85 | Max Mem Allocated (GB): 27.19
2025-12-17 17:49:53,862 - root - INFO - Step: 215 | Loss: 11.97 | Tokens per second: 11882.41 | Training tokens per second (%): 28.92 | MFU (%): 15.48 | TFLOPs: 153.11 | Mem Allocated (GB): 6.52 | Mem Reserved (GB): 37.36 | Max Mem Allocated (GB): 27.19
2025-12-17 17:49:57,307 - root - INFO - Step: 220 | Loss: 11.95 | Tokens per second: 11889.67 | Training tokens per second (%): 52.39 | MFU (%): 15.49 | TFLOPs: 153.20 | Mem Allocated (GB): 6.52 | Mem Reserved (GB): 40.85 | Max Mem Allocated (GB): 27.19
2025-12-17 17:49:57,307 - root - INFO - Step: 220 | Loss: 11.98 | Tokens per second: 11889.59 | Training tokens per second (%): 62.31 | MFU (%): 15.49 | TFLOPs: 153.20 | Mem Allocated (GB): 6.52 | Mem Reserved (GB): 40.85 | Max Mem Allocated (GB): 27.19
2025-12-17 17:49:57,307 - root - INFO - Step: 220 | Loss: 11.98 | Tokens per second: 11890.81 | Training tokens per second (%): 36.65 | MFU (%): 15.49 | TFLOPs: 153.22 | Mem Allocated (GB): 6.52 | Mem Reserved (GB): 37.36 | Max Mem Allocated (GB): 27.19
2025-12-17 17:49:57,307 - root - INFO - Step: 220 | Loss: 11.98 | Tokens per second: 11888.29 | Training tokens per second (%): 53.53 | MFU (%): 15.49 | TFLOPs: 153.18 | Mem Allocated (GB): 6.52 | Mem Reserved (GB): 41.14 | Max Mem Allocated (GB): 27.19
2025-12-17 17:50:00,758 - root - INFO - Step: 225 | Loss: 11.96 | Tokens per second: 11872.11 | Training tokens per second (%): 33.09 | MFU (%): 15.47 | TFLOPs: 152.98 | Mem Allocated (GB): 6.52 | Mem Reserved (GB): 40.85 | Max Mem Allocated (GB): 27.19
2025-12-17 17:50:00,758 - root - INFO - Step: 225 | Loss: 11.98 | Tokens per second: 11873.18 | Training tokens per second (%): 60.49 | MFU (%): 15.47 | TFLOPs: 152.99 | Mem Allocated (GB): 6.52 | Mem Reserved (GB): 41.14 | Max Mem Allocated (GB): 27.19
2025-12-17 17:50:00,758 - root - INFO - Step: 225 | Loss: 11.97 | Tokens per second: 11872.03 | Training tokens per second (%): 17.36 | MFU (%): 15.47 | TFLOPs: 152.97 | Mem Allocated (GB): 6.52 | Mem Reserved (GB): 40.85 | Max Mem Allocated (GB): 27.19
2025-12-17 17:50:00,758 - root - INFO - Step: 225 | Loss: 11.98 | Tokens per second: 11870.72 | Training tokens per second (%): 46.07 | MFU (%): 15.47 | TFLOPs: 152.96 | Mem Allocated (GB): 6.52 | Mem Reserved (GB): 37.36 | Max Mem Allocated (GB): 27.19
2025-12-17 17:50:04,198 - root - INFO - Step: 230 | Loss: 11.94 | Tokens per second: 11911.20 | Training tokens per second (%): 33.52 | MFU (%): 15.52 | TFLOPs: 153.48 | Mem Allocated (GB): 6.52 | Mem Reserved (GB): 40.85 | Max Mem Allocated (GB): 27.19
2025-12-17 17:50:04,198 - root - INFO - Step: 230 | Loss: 11.95 | Tokens per second: 11912.23 | Training tokens per second (%): 53.33 | MFU (%): 15.52 | TFLOPs: 153.49 | Mem Allocated (GB): 6.52 | Mem Reserved (GB): 37.36 | Max Mem Allocated (GB): 27.19
2025-12-17 17:50:04,198 - root - INFO - Step: 230 | Loss: 11.96 | Tokens per second: 11911.08 | Training tokens per second (%): 45.48 | MFU (%): 15.52 | TFLOPs: 153.48 | Mem Allocated (GB): 6.52 | Mem Reserved (GB): 40.85 | Max Mem Allocated (GB): 27.19
2025-12-17 17:50:04,198 - root - INFO - Step: 230 | Loss: 11.98 | Tokens per second: 11909.69 | Training tokens per second (%): 54.63 | MFU (%): 15.52 | TFLOPs: 153.46 | Mem Allocated (GB): 6.52 | Mem Reserved (GB): 41.14 | Max Mem Allocated (GB): 27.19
2025-12-17 17:50:07,644 - root - INFO - Step: 235 | Loss: 11.96 | Tokens per second: 11888.16 | Training tokens per second (%): 28.22 | MFU (%): 15.49 | TFLOPs: 153.18 | Mem Allocated (GB): 6.52 | Mem Reserved (GB): 40.85 | Max Mem Allocated (GB): 27.19
2025-12-17 17:50:07,644 - root - INFO - Step: 235 | Loss: 11.97 | Tokens per second: 11886.89 | Training tokens per second (%): 38.65 | MFU (%): 15.49 | TFLOPs: 153.17 | Mem Allocated (GB): 6.52 | Mem Reserved (GB): 40.85 | Max Mem Allocated (GB): 27.19
2025-12-17 17:50:07,644 - root - INFO - Step: 235 | Loss: 12.02 | Tokens per second: 11887.92 | Training tokens per second (%): 33.66 | MFU (%): 15.49 | TFLOPs: 153.18 | Mem Allocated (GB): 6.52 | Mem Reserved (GB): 41.14 | Max Mem Allocated (GB): 27.19
2025-12-17 17:50:07,644 - root - INFO - Step: 235 | Loss: 11.93 | Tokens per second: 11886.73 | Training tokens per second (%): 47.39 | MFU (%): 15.49 | TFLOPs: 153.16 | Mem Allocated (GB): 6.52 | Mem Reserved (GB): 37.36 | Max Mem Allocated (GB): 27.19
2025-12-17 17:50:11,084 - root - INFO - Step: 240 | Loss: 11.98 | Tokens per second: 11911.19 | Training tokens per second (%): 31.04 | MFU (%): 15.52 | TFLOPs: 153.48 | Mem Allocated (GB): 6.52 | Mem Reserved (GB): 40.85 | Max Mem Allocated (GB): 27.19
2025-12-17 17:50:11,084 - root - INFO - Step: 240 | Loss: 12.00 | Tokens per second: 11910.05 | Training tokens per second (%): 44.20 | MFU (%): 15.52 | TFLOPs: 153.46 | Mem Allocated (GB): 6.52 | Mem Reserved (GB): 40.85 | Max Mem Allocated (GB): 27.19
2025-12-17 17:50:11,084 - root - INFO - Step: 240 | Loss: 11.90 | Tokens per second: 11911.32 | Training tokens per second (%): 23.52 | MFU (%): 15.52 | TFLOPs: 153.48 | Mem Allocated (GB): 6.52 | Mem Reserved (GB): 41.14 | Max Mem Allocated (GB): 27.19
2025-12-17 17:50:11,084 - root - INFO - Step: 240 | Loss: 11.99 | Tokens per second: 11909.90 | Training tokens per second (%): 21.17 | MFU (%): 15.52 | TFLOPs: 153.46 | Mem Allocated (GB): 6.52 | Mem Reserved (GB): 37.36 | Max Mem Allocated (GB): 27.19
2025-12-17 17:50:14,535 - root - INFO - Step: 245 | Loss: 11.96 | Tokens per second: 11873.30 | Training tokens per second (%): 56.42 | MFU (%): 15.47 | TFLOPs: 152.99 | Mem Allocated (GB): 6.52 | Mem Reserved (GB): 41.14 | Max Mem Allocated (GB): 27.19
2025-12-17 17:50:14,535 - root - INFO - Step: 245 | Loss: 11.97 | Tokens per second: 11873.45 | Training tokens per second (%): 20.40 | MFU (%): 15.47 | TFLOPs: 152.99 | Mem Allocated (GB): 6.52 | Mem Reserved (GB): 40.85 | Max Mem Allocated (GB): 27.19
2025-12-17 17:50:14,535 - root - INFO - Step: 245 | Loss: 11.93 | Tokens per second: 11872.08 | Training tokens per second (%): 42.85 | MFU (%): 15.47 | TFLOPs: 152.98 | Mem Allocated (GB): 6.52 | Mem Reserved (GB): 40.85 | Max Mem Allocated (GB): 27.19
2025-12-17 17:50:14,535 - root - INFO - Step: 245 | Loss: 11.90 | Tokens per second: 11873.30 | Training tokens per second (%): 52.58 | MFU (%): 15.47 | TFLOPs: 152.99 | Mem Allocated (GB): 6.52 | Mem Reserved (GB): 37.36 | Max Mem Allocated (GB): 27.19
2025-12-17 17:50:18,176 - root - INFO - Step: 250 | Loss: 11.94 | Tokens per second: 11251.35 | Training tokens per second (%): 42.08 | MFU (%): 14.66 | TFLOPs: 144.98 | Mem Allocated (GB): 6.52 | Mem Reserved (GB): 40.85 | Max Mem Allocated (GB): 27.19
2025-12-17 17:50:18,176 - root - INFO - Step: 250 | Loss: 11.94 | Tokens per second: 11251.33 | Training tokens per second (%): 23.88 | MFU (%): 14.66 | TFLOPs: 144.98 | Mem Allocated (GB): 6.52 | Mem Reserved (GB): 41.14 | Max Mem Allocated (GB): 27.19
2025-12-17 17:50:18,176 - root - INFO - Step: 250 | Loss: 12.00 | Tokens per second: 11252.33 | Training tokens per second (%): 41.03 | MFU (%): 14.66 | TFLOPs: 144.99 | Mem Allocated (GB): 6.52 | Mem Reserved (GB): 40.85 | Max Mem Allocated (GB): 27.19
2025-12-17 17:50:18,176 - root - INFO - Training completed
2025-12-17 17:50:18,176 - root - INFO - Training completed
2025-12-17 17:50:18,176 - root - INFO - Training completed
2025-12-17 17:50:18,176 - root - INFO - Step: 250 | Loss: 12.05 | Tokens per second: 11251.01 | Training tokens per second (%): 26.95 | MFU (%): 14.66 | TFLOPs: 144.97 | Mem Allocated (GB): 6.52 | Mem Reserved (GB): 37.36 | Max Mem Allocated (GB): 27.19
2025-12-17 17:50:18,176 - root - INFO - Training completed
[2025-12-17 17:50:20,930] [INFO] [launch.py:367:main] Process 82870 exits successfully.
[2025-12-17 17:50:20,930] [INFO] [launch.py:367:main] Process 82860 exits successfully.
[2025-12-17 17:50:20,930] [INFO] [launch.py:367:main] Process 82866 exits successfully.
[2025-12-17 17:50:20,930] [INFO] [launch.py:367:main] Process 82863 exits successfully.
END TIME: Wed Dec 17 17:50:24 CET 2025
