[2025-11-15 20:20:07,667] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-11-15 20:20:07,961] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-11-15 20:20:08,249] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-11-15 20:20:08,516] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The default cache directory for DeepSpeed Triton autotune, /users/ldionysiou/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
Warning: The default cache directory for DeepSpeed Triton autotune, /users/ldionysiou/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.

[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
Warning: The default cache directory for DeepSpeed Triton autotune, /users/ldionysiou/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
/usr/local/lib/python3.12/dist-packages/deepspeed/runtime/zero/linear.py:47: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  @autocast_custom_fwd
/usr/local/lib/python3.12/dist-packages/deepspeed/runtime/zero/linear.py:47: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  @autocast_custom_fwd
/usr/local/lib/python3.12/dist-packages/deepspeed/runtime/zero/linear.py:66: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  @autocast_custom_bwd
/usr/local/lib/python3.12/dist-packages/deepspeed/runtime/zero/linear.py:66: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  @autocast_custom_bwd
Warning: The default cache directory for DeepSpeed Triton autotune, /users/ldionysiou/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
/usr/local/lib/python3.12/dist-packages/deepspeed/runtime/zero/linear.py:47: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  @autocast_custom_fwd
/usr/local/lib/python3.12/dist-packages/deepspeed/runtime/zero/linear.py:66: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  @autocast_custom_bwd
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
/usr/local/lib/python3.12/dist-packages/deepspeed/runtime/zero/linear.py:47: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  @autocast_custom_fwd
/usr/local/lib/python3.12/dist-packages/deepspeed/runtime/zero/linear.py:66: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  @autocast_custom_bwd
2025-11-15 20:20:11,862 - root - INFO - Experiment args: Namespace(dataset='/capstor/store/cscs/ethz/large-sc-2/datasets/train_data.parquet', tokenizer_name_or_path='unsloth/Mistral-Nemo-Base-2407-bnb-4bit', sequence_length=2048, batch_size=1, fused_optimizer=False, learning_rate=1e-05, lr_warmup_steps=10, training_steps=1000, logging_frequency=5, profile=True, profile_step_start=10, profile_step_end=12, grad_max_norm=1.0, model_dtype='bf16', compile=False, deepspeed=True, deepspeed_config='/iopsstor/scratch/cscs/ldionysiou/project/ds_stage0.json', local_rank=-1)
2025-11-15 20:20:11,862 - root - INFO - Experiment args: Namespace(dataset='/capstor/store/cscs/ethz/large-sc-2/datasets/train_data.parquet', tokenizer_name_or_path='unsloth/Mistral-Nemo-Base-2407-bnb-4bit', sequence_length=2048, batch_size=1, fused_optimizer=False, learning_rate=1e-05, lr_warmup_steps=10, training_steps=1000, logging_frequency=5, profile=True, profile_step_start=10, profile_step_end=12, grad_max_norm=1.0, model_dtype='bf16', compile=False, deepspeed=True, deepspeed_config='/iopsstor/scratch/cscs/ldionysiou/project/ds_stage0.json', local_rank=-1)
[2025-11-15 20:20:11,862] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-11-15 20:20:11,862] [INFO] [comm.py:637:init_distributed] cdb=None
2025-11-15 20:20:11,862 - root - INFO - Experiment args: Namespace(dataset='/capstor/store/cscs/ethz/large-sc-2/datasets/train_data.parquet', tokenizer_name_or_path='unsloth/Mistral-Nemo-Base-2407-bnb-4bit', sequence_length=2048, batch_size=1, fused_optimizer=False, learning_rate=1e-05, lr_warmup_steps=10, training_steps=1000, logging_frequency=5, profile=True, profile_step_start=10, profile_step_end=12, grad_max_norm=1.0, model_dtype='bf16', compile=False, deepspeed=True, deepspeed_config='/iopsstor/scratch/cscs/ldionysiou/project/ds_stage0.json', local_rank=-1)
[2025-11-15 20:20:11,862] [INFO] [comm.py:637:init_distributed] cdb=None
2025-11-15 20:20:11,864 - root - INFO - Experiment args: Namespace(dataset='/capstor/store/cscs/ethz/large-sc-2/datasets/train_data.parquet', tokenizer_name_or_path='unsloth/Mistral-Nemo-Base-2407-bnb-4bit', sequence_length=2048, batch_size=1, fused_optimizer=False, learning_rate=1e-05, lr_warmup_steps=10, training_steps=1000, logging_frequency=5, profile=True, profile_step_start=10, profile_step_end=12, grad_max_norm=1.0, model_dtype='bf16', compile=False, deepspeed=True, deepspeed_config='/iopsstor/scratch/cscs/ldionysiou/project/ds_stage0.json', local_rank=-1)
[2025-11-15 20:20:11,864] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-11-15 20:20:11,864] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
2025-11-15 20:20:11,865 - root - INFO - Setting up DataLoaders...
2025-11-15 20:20:12,349 - root - INFO - Setting up DataLoaders...
2025-11-15 20:20:12,349 - root - INFO - Setting up DataLoaders...
2025-11-15 20:20:12,349 - root - INFO - Setting up DataLoaders...
2025-11-15 20:20:16,440 - root - INFO - Setting up Model...
2025-11-15 20:20:16,440 - root - INFO - Setting up Model...
2025-11-15 20:20:16,440 - root - INFO - Setting up Model...
2025-11-15 20:20:16,440 - root - INFO - Setting up Model...
2025-11-15 20:20:49,996 - root - INFO - Using DeepSpeed
[2025-11-15 20:20:50,001] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.14.5, git-hash=unknown, git-branch=unknown
2025-11-15 20:20:50,143 - root - INFO - Using DeepSpeed
2025-11-15 20:20:50,201 - root - INFO - Using DeepSpeed
2025-11-15 20:20:51,127 - root - INFO - Using DeepSpeed
[2025-11-15 20:20:59,164] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
Using /users/ldionysiou/.cache/torch_extensions/py312_cu126 as PyTorch extensions root...Using /users/ldionysiou/.cache/torch_extensions/py312_cu126 as PyTorch extensions root...Using /users/ldionysiou/.cache/torch_extensions/py312_cu126 as PyTorch extensions root...Using /users/ldionysiou/.cache/torch_extensions/py312_cu126 as PyTorch extensions root...



Detected CUDA files, patching ldflags
Emitting ninja build file /users/ldionysiou/.cache/torch_extensions/py312_cu126/fused_adam/build.ninja...
/usr/local/lib/python3.12/dist-packages/torch/utils/cpp_extension.py:2007: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].
  warnings.warn(
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 0.8353614807128906 seconds
[2025-11-15 20:21:00,002] [INFO] [logging.py:96:log_dist] [Rank 0] Using DeepSpeed Optimizer param name adamw as basic optimizer
[2025-11-15 20:21:00,002] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
[2025-11-15 20:21:00,010] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = FusedAdam
[2025-11-15 20:21:00,010] [INFO] [logging.py:96:log_dist] [Rank 0] Creating BF16 optimizer
Loading extension module fused_adam...
Loading extension module fused_adam...
Loading extension module fused_adam...
Time to load fused_adam op: 0.9103429317474365 secondsTime to load fused_adam op: 0.9103546142578125 secondsTime to load fused_adam op: 0.9103529453277588 seconds


[2025-11-15 20:21:00,105] [INFO] [utils.py:781:see_memory_usage] begin bf16_optimizer
[2025-11-15 20:21:00,106] [INFO] [utils.py:782:see_memory_usage] MA 15.0 GB         Max_MA 15.0 GB         CA 15.13 GB         Max_CA 15 GB 
[2025-11-15 20:21:00,107] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 194.4 GB, percent = 22.8%
[2025-11-15 20:21:00,190] [INFO] [utils.py:781:see_memory_usage] before initializing group 0
[2025-11-15 20:21:00,191] [INFO] [utils.py:782:see_memory_usage] MA 15.0 GB         Max_MA 15.0 GB         CA 15.13 GB         Max_CA 15 GB 
[2025-11-15 20:21:00,191] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 243.24 GB, percent = 28.5%
2025-11-15 20:21:00,442 - root - INFO - Starting training!
2025-11-15 20:21:00,442 - root - INFO - Starting training!
2025-11-15 20:21:00,442 - root - INFO - Starting training!
[2025-11-15 20:21:00,597] [INFO] [utils.py:781:see_memory_usage] after initializing group 0
[2025-11-15 20:21:00,597] [INFO] [utils.py:782:see_memory_usage] MA 52.5 GB         Max_MA 52.5 GB         CA 71.38 GB         Max_CA 71 GB 
[2025-11-15 20:21:00,598] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 405.29 GB, percent = 47.4%
[2025-11-15 20:21:00,703] [INFO] [utils.py:781:see_memory_usage] before initialize_optimizer
[2025-11-15 20:21:00,704] [INFO] [utils.py:782:see_memory_usage] MA 52.5 GB         Max_MA 52.5 GB         CA 71.38 GB         Max_CA 71 GB 
[2025-11-15 20:21:00,705] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 405.79 GB, percent = 47.5%
[2025-11-15 20:21:00,806] [INFO] [utils.py:781:see_memory_usage] end initialize_optimizer
[2025-11-15 20:21:00,807] [INFO] [utils.py:782:see_memory_usage] MA 52.5 GB         Max_MA 52.5 GB         CA 71.38 GB         Max_CA 71 GB 
[2025-11-15 20:21:00,808] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 406.14 GB, percent = 47.5%
[2025-11-15 20:21:00,919] [INFO] [utils.py:781:see_memory_usage] end bf16_ optimizer
[2025-11-15 20:21:00,919] [INFO] [utils.py:782:see_memory_usage] MA 52.5 GB         Max_MA 52.5 GB         CA 71.38 GB         Max_CA 71 GB 
[2025-11-15 20:21:00,920] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 406.38 GB, percent = 47.6%
[2025-11-15 20:21:00,920] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = BF16_Optimizer
[2025-11-15 20:21:00,920] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using configured LR scheduler = WarmupLR
[2025-11-15 20:21:00,920] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = <deepspeed.runtime.lr_schedules.WarmupLR object at 0x400469ce4fe0>
[2025-11-15 20:21:00,920] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[1e-05], mom=[[0.9, 0.95]]
[2025-11-15 20:21:00,921] [INFO] [config.py:997:print] DeepSpeedEngine configuration:
[2025-11-15 20:21:00,922] [INFO] [config.py:1001:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2025-11-15 20:21:00,922] [INFO] [config.py:1001:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2025-11-15 20:21:00,922] [INFO] [config.py:1001:print]   amp_enabled .................. False
[2025-11-15 20:21:00,922] [INFO] [config.py:1001:print]   amp_params ................... False
[2025-11-15 20:21:00,922] [INFO] [config.py:1001:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2025-11-15 20:21:00,922] [INFO] [config.py:1001:print]   bfloat16_enabled ............. True
[2025-11-15 20:21:00,922] [INFO] [config.py:1001:print]   bfloat16_immediate_grad_update  False
[2025-11-15 20:21:00,922] [INFO] [config.py:1001:print]   checkpoint_parallel_write_pipeline  False
[2025-11-15 20:21:00,922] [INFO] [config.py:1001:print]   checkpoint_tag_validation_enabled  True
[2025-11-15 20:21:00,922] [INFO] [config.py:1001:print]   checkpoint_tag_validation_fail  False
[2025-11-15 20:21:00,922] [INFO] [config.py:1001:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x4004698d1b80>
[2025-11-15 20:21:00,922] [INFO] [config.py:1001:print]   communication_data_type ...... None
[2025-11-15 20:21:00,922] [INFO] [config.py:1001:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2025-11-15 20:21:00,922] [INFO] [config.py:1001:print]   curriculum_enabled_legacy .... False
[2025-11-15 20:21:00,922] [INFO] [config.py:1001:print]   curriculum_params_legacy ..... False
[2025-11-15 20:21:00,922] [INFO] [config.py:1001:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2025-11-15 20:21:00,922] [INFO] [config.py:1001:print]   data_efficiency_enabled ...... False
[2025-11-15 20:21:00,922] [INFO] [config.py:1001:print]   dataloader_drop_last ......... False
[2025-11-15 20:21:00,922] [INFO] [config.py:1001:print]   disable_allgather ............ False
[2025-11-15 20:21:00,922] [INFO] [config.py:1001:print]   dump_state ................... False
[2025-11-15 20:21:00,922] [INFO] [config.py:1001:print]   dynamic_loss_scale_args ...... None
[2025-11-15 20:21:00,922] [INFO] [config.py:1001:print]   eigenvalue_enabled ........... False
[2025-11-15 20:21:00,922] [INFO] [config.py:1001:print]   eigenvalue_gas_boundary_resolution  1
[2025-11-15 20:21:00,922] [INFO] [config.py:1001:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2025-11-15 20:21:00,922] [INFO] [config.py:1001:print]   eigenvalue_layer_num ......... 0
[2025-11-15 20:21:00,922] [INFO] [config.py:1001:print]   eigenvalue_max_iter .......... 100
[2025-11-15 20:21:00,922] [INFO] [config.py:1001:print]   eigenvalue_stability ......... 1e-06
[2025-11-15 20:21:00,922] [INFO] [config.py:1001:print]   eigenvalue_tol ............... 0.01
[2025-11-15 20:21:00,922] [INFO] [config.py:1001:print]   eigenvalue_verbose ........... False
[2025-11-15 20:21:00,923] [INFO] [config.py:1001:print]   elasticity_enabled ........... False
[2025-11-15 20:21:00,923] [INFO] [config.py:1001:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 10, 
    "module_depth": -1, 
    "top_modules": 3, 
    "detailed": true, 
    "output_file": null
}
[2025-11-15 20:21:00,923] [INFO] [config.py:1001:print]   fp16_auto_cast ............... None
[2025-11-15 20:21:00,923] [INFO] [config.py:1001:print]   fp16_enabled ................. False
[2025-11-15 20:21:00,923] [INFO] [config.py:1001:print]   fp16_master_weights_and_gradients  False
[2025-11-15 20:21:00,923] [INFO] [config.py:1001:print]   global_rank .................. 0
[2025-11-15 20:21:00,923] [INFO] [config.py:1001:print]   grad_accum_dtype ............. None
[2025-11-15 20:21:00,923] [INFO] [config.py:1001:print]   gradient_accumulation_steps .. 1
[2025-11-15 20:21:00,923] [INFO] [config.py:1001:print]   gradient_clipping ............ 1.0
[2025-11-15 20:21:00,923] [INFO] [config.py:1001:print]   gradient_predivide_factor .... 1.0
[2025-11-15 20:21:00,923] [INFO] [config.py:1001:print]   graph_harvesting ............. False
[2025-11-15 20:21:00,923] [INFO] [config.py:1001:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2025-11-15 20:21:00,923] [INFO] [config.py:1001:print]   initial_dynamic_scale ........ 1
[2025-11-15 20:21:00,923] [INFO] [config.py:1001:print]   load_universal_checkpoint .... False
[2025-11-15 20:21:00,923] [INFO] [config.py:1001:print]   loss_scale ................... 1.0
[2025-11-15 20:21:00,923] [INFO] [config.py:1001:print]   memory_breakdown ............. False
[2025-11-15 20:21:00,923] [INFO] [config.py:1001:print]   mics_hierarchial_params_gather  False
[2025-11-15 20:21:00,923] [INFO] [config.py:1001:print]   mics_shard_size .............. -1
[2025-11-15 20:21:00,923] [INFO] [config.py:1001:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') comet=CometConfig(enabled=False, samples_log_interval=100, project=None, workspace=None, api_key=None, experiment_name=None, experiment_key=None, online=None, mode=None) wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2025-11-15 20:21:00,923] [INFO] [config.py:1001:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2025-11-15 20:21:00,923] [INFO] [config.py:1001:print]   optimizer_legacy_fusion ...... False
[2025-11-15 20:21:00,923] [INFO] [config.py:1001:print]   optimizer_name ............... adamw
[2025-11-15 20:21:00,923] [INFO] [config.py:1001:print]   optimizer_params ............. {'lr': 1e-05, 'betas': [0.9, 0.95], 'eps': 1e-08, 'weight_decay': 0.01}
[2025-11-15 20:21:00,923] [INFO] [config.py:1001:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2025-11-15 20:21:00,923] [INFO] [config.py:1001:print]   pld_enabled .................. False
[2025-11-15 20:21:00,923] [INFO] [config.py:1001:print]   pld_params ................... False
[2025-11-15 20:21:00,923] [INFO] [config.py:1001:print]   prescale_gradients ........... False
[2025-11-15 20:21:00,923] [INFO] [config.py:1001:print]   scheduler_name ............... WarmupLR
[2025-11-15 20:21:00,923] [INFO] [config.py:1001:print]   scheduler_params ............. {'warmup_min_lr': 0, 'warmup_max_lr': 1e-05, 'warmup_num_steps': 10}
[2025-11-15 20:21:00,923] [INFO] [config.py:1001:print]   seq_parallel_communication_data_type  torch.float32
[2025-11-15 20:21:00,923] [INFO] [config.py:1001:print]   sparse_attention ............. None
[2025-11-15 20:21:00,923] [INFO] [config.py:1001:print]   sparse_gradients_enabled ..... False
[2025-11-15 20:21:00,923] [INFO] [config.py:1001:print]   steps_per_print .............. 10
[2025-11-15 20:21:00,923] [INFO] [config.py:1001:print]   timers_config ................ enabled=True synchronized=True
[2025-11-15 20:21:00,923] [INFO] [config.py:1001:print]   train_batch_size ............. 4
[2025-11-15 20:21:00,923] [INFO] [config.py:1001:print]   train_micro_batch_size_per_gpu  1
[2025-11-15 20:21:00,923] [INFO] [config.py:1001:print]   use_data_before_expert_parallel_  False
[2025-11-15 20:21:00,923] [INFO] [config.py:1001:print]   use_node_local_storage ....... False
[2025-11-15 20:21:00,923] [INFO] [config.py:1001:print]   wall_clock_breakdown ......... False
[2025-11-15 20:21:00,924] [INFO] [config.py:1001:print]   weight_quantization_config ... None
[2025-11-15 20:21:00,924] [INFO] [config.py:1001:print]   world_size ................... 4
[2025-11-15 20:21:00,924] [INFO] [config.py:1001:print]   zero_allow_untested_optimizer  False
[2025-11-15 20:21:00,924] [INFO] [config.py:1001:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False use_all_reduce_for_fetch_params=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2025-11-15 20:21:00,924] [INFO] [config.py:1001:print]   zero_enabled ................. False
[2025-11-15 20:21:00,924] [INFO] [config.py:1001:print]   zero_force_ds_cpu_optimizer .. True
[2025-11-15 20:21:00,924] [INFO] [config.py:1001:print]   zero_optimization_stage ...... 0
[2025-11-15 20:21:00,924] [INFO] [config.py:987:print_user_config]   json = {
    "train_micro_batch_size_per_gpu": 1, 
    "train_batch_size": 4, 
    "gradient_accumulation_steps": 1, 
    "optimizer": {
        "type": "AdamW", 
        "params": {
            "lr": 1e-05, 
            "betas": [0.9, 0.95], 
            "eps": 1e-08, 
            "weight_decay": 0.01
        }
    }, 
    "torch_adam": true, 
    "scheduler": {
        "type": "WarmupLR", 
        "params": {
            "warmup_min_lr": 0, 
            "warmup_max_lr": 1e-05, 
            "warmup_num_steps": 10
        }
    }, 
    "bf16": {
        "enabled": true
    }, 
    "zero_optimization": {
        "stage": 0, 
        "contiguous_gradients": true, 
        "overlap_comm": true
    }, 
    "gradient_clipping": 1.0, 
    "flops_profiler": {
        "enabled": false, 
        "profile_step": 10, 
        "module_depth": -1, 
        "top_modules": 3, 
        "detailed": true
    }, 
    "wall_clock_breakdown": false
}
2025-11-15 20:21:00,926 - root - INFO - Starting training!
/usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py:825: UserWarning: cuDNN SDPA backward got grad_output.strides() != output.strides(), attempting to materialize a grad_output with matching strides... (Triggered internally at /opt/pytorch/pytorch/aten/src/ATen/native/cudnn/MHA.cpp:674.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py:825: UserWarning: cuDNN SDPA backward got grad_output.strides() != output.strides(), attempting to materialize a grad_output with matching strides... (Triggered internally at /opt/pytorch/pytorch/aten/src/ATen/native/cudnn/MHA.cpp:674.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py:825: UserWarning: cuDNN SDPA backward got grad_output.strides() != output.strides(), attempting to materialize a grad_output with matching strides... (Triggered internally at /opt/pytorch/pytorch/aten/src/ATen/native/cudnn/MHA.cpp:674.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py:825: UserWarning: cuDNN SDPA backward got grad_output.strides() != output.strides(), attempting to materialize a grad_output with matching strides... (Triggered internally at /opt/pytorch/pytorch/aten/src/ATen/native/cudnn/MHA.cpp:674.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2025-11-15 20:21:02,385 - root - INFO - Step: 1 | Loss: 11.94 | Tokens per second (per-GPU): 1471.84 | Total TFLOPs: 284.48 | MFU (%): 7.19
[rank3]: Traceback (most recent call last):
[rank3]:   File "/iopsstor/scratch/cscs/ldionysiou/project/train.py", line 185, in <module>
[rank3]:     train(args)
[rank3]:   File "/iopsstor/scratch/cscs/ldionysiou/project/train.py", line 137, in train
[rank3]:     logits = model_engine(input_ids)
[rank3]:              ^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
[rank3]:     return self._call_impl(*args, **kwargs)
[rank3]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1747, in _call_impl
[rank3]:     return forward_call(*args, **kwargs)
[rank3]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/usr/local/lib/python3.12/dist-packages/deepspeed/utils/nvtx.py", line 18, in wrapped_fn
[rank3]:     ret_val = func(*args, **kwargs)
[rank3]:               ^^^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/usr/local/lib/python3.12/dist-packages/deepspeed/runtime/engine.py", line 1890, in forward
[rank3]:     loss = self.module(*inputs, **kwargs)
[rank3]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
[rank3]:     return self._call_impl(*args, **kwargs)
[rank3]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1747, in _call_impl
[rank3]:     return forward_call(*args, **kwargs)
[rank3]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/iopsstor/scratch/cscs/ldionysiou/project/model.py", line 370, in forward
[rank3]:     h = layer(h, self.freqs_cis)
[rank3]:         ^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
[rank3]:     return self._call_impl(*args, **kwargs)
[rank3]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1747, in _call_impl
[rank3]:     return forward_call(*args, **kwargs)
[rank3]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/iopsstor/scratch/cscs/ldionysiou/project/model.py", line 308, in forward
[rank3]:     out = h + self.feed_forward(self.ffn_norm(h))
[rank3]:               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
[rank3]:     return self._call_impl(*args, **kwargs)
[rank3]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1747, in _call_impl
[rank3]:     return forward_call(*args, **kwargs)
[rank3]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/iopsstor/scratch/cscs/ldionysiou/project/model.py", line 252, in forward
[rank3]:     return self.w2(F.silu(self.w1(x)) * self.w3(x))
[rank3]:                    ^^^^^^^^^^^^^^^^^^
[rank3]:   File "/usr/local/lib/python3.12/dist-packages/torch/nn/functional.py", line 2380, in silu
[rank3]:     return torch._C._nn.silu(input)
[rank3]:            ^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 56.00 MiB. GPU 3 has a total capacity of 94.50 GiB of which 2.60 GiB is free. Including non-PyTorch memory, this process has 90.86 GiB memory in use. Of the allocated memory 89.19 GiB is allocated by PyTorch, and 206.78 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank1]: Traceback (most recent call last):
[rank1]:   File "/iopsstor/scratch/cscs/ldionysiou/project/train.py", line 185, in <module>
[rank1]:     train(args)
[rank1]:   File "/iopsstor/scratch/cscs/ldionysiou/project/train.py", line 137, in train
[rank1]:     logits = model_engine(input_ids)
[rank1]:              ^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
[rank1]:     return self._call_impl(*args, **kwargs)
[rank1]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1747, in _call_impl
[rank1]:     return forward_call(*args, **kwargs)
[rank1]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/usr/local/lib/python3.12/dist-packages/deepspeed/utils/nvtx.py", line 18, in wrapped_fn
[rank1]:     ret_val = func(*args, **kwargs)
[rank1]:               ^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/usr/local/lib/python3.12/dist-packages/deepspeed/runtime/engine.py", line 1890, in forward
[rank1]:     loss = self.module(*inputs, **kwargs)
[rank1]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
[rank1]:     return self._call_impl(*args, **kwargs)
[rank1]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1747, in _call_impl
[rank1]:     return forward_call(*args, **kwargs)
[rank1]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/iopsstor/scratch/cscs/ldionysiou/project/model.py", line 370, in forward
[rank1]:     h = layer(h, self.freqs_cis)
[rank1]:         ^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
[rank1]:     return self._call_impl(*args, **kwargs)
[rank1]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1747, in _call_impl
[rank1]:     return forward_call(*args, **kwargs)
[rank1]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/iopsstor/scratch/cscs/ldionysiou/project/model.py", line 308, in forward
[rank1]:     out = h + self.feed_forward(self.ffn_norm(h))
[rank1]:               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
[rank1]:     return self._call_impl(*args, **kwargs)
[rank1]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1747, in _call_impl
[rank1]:     return forward_call(*args, **kwargs)
[rank1]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/iopsstor/scratch/cscs/ldionysiou/project/model.py", line 252, in forward
[rank1]:     return self.w2(F.silu(self.w1(x)) * self.w3(x))
[rank1]:                    ^^^^^^^^^^^^^^^^^^
[rank1]:   File "/usr/local/lib/python3.12/dist-packages/torch/nn/functional.py", line 2380, in silu
[rank1]:     return torch._C._nn.silu(input)
[rank1]:            ^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 56.00 MiB. GPU 1 has a total capacity of 94.50 GiB of which 2.59 GiB is free. Including non-PyTorch memory, this process has 90.86 GiB memory in use. Of the allocated memory 89.19 GiB is allocated by PyTorch, and 206.78 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank2]: Traceback (most recent call last):
[rank2]:   File "/iopsstor/scratch/cscs/ldionysiou/project/train.py", line 185, in <module>
[rank2]:     train(args)
[rank2]:   File "/iopsstor/scratch/cscs/ldionysiou/project/train.py", line 137, in train
[rank2]:     logits = model_engine(input_ids)
[rank2]:              ^^^^^^^^^^^^^^^^^^^^^^^
[rank2]:   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
[rank2]:     return self._call_impl(*args, **kwargs)
[rank2]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank2]:   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1747, in _call_impl
[rank2]:     return forward_call(*args, **kwargs)
[rank2]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank2]:   File "/usr/local/lib/python3.12/dist-packages/deepspeed/utils/nvtx.py", line 18, in wrapped_fn
[rank2]:     ret_val = func(*args, **kwargs)
[rank2]:               ^^^^^^^^^^^^^^^^^^^^^
[rank2]:   File "/usr/local/lib/python3.12/dist-packages/deepspeed/runtime/engine.py", line 1890, in forward
[rank2]:     loss = self.module(*inputs, **kwargs)
[rank2]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank2]:   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
[rank2]:     return self._call_impl(*args, **kwargs)
[rank2]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank2]:   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1747, in _call_impl
[rank2]:     return forward_call(*args, **kwargs)
[rank2]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank2]:   File "/iopsstor/scratch/cscs/ldionysiou/project/model.py", line 370, in forward
[rank2]:     h = layer(h, self.freqs_cis)
[rank2]:         ^^^^^^^^^^^^^^^^^^^^^^^^
[rank2]:   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
[rank2]:     return self._call_impl(*args, **kwargs)
[rank2]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank2]:   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1747, in _call_impl
[rank2]:     return forward_call(*args, **kwargs)
[rank2]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank2]:   File "/iopsstor/scratch/cscs/ldionysiou/project/model.py", line 308, in forward
[rank2]:     out = h + self.feed_forward(self.ffn_norm(h))
[rank2]:               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank2]:   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
[rank2]:     return self._call_impl(*args, **kwargs)
[rank2]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank2]:   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1747, in _call_impl
[rank2]:     return forward_call(*args, **kwargs)
[rank2]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank2]:   File "/iopsstor/scratch/cscs/ldionysiou/project/model.py", line 252, in forward
[rank2]:     return self.w2(F.silu(self.w1(x)) * self.w3(x))
[rank2]:                    ^^^^^^^^^^^^^^^^^^
[rank2]:   File "/usr/local/lib/python3.12/dist-packages/torch/nn/functional.py", line 2380, in silu
[rank2]:     return torch._C._nn.silu(input)
[rank2]:            ^^^^^^^^^^^^^^^^^^^^^^^^
[rank2]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 56.00 MiB. GPU 2 has a total capacity of 94.50 GiB of which 2.60 GiB is free. Including non-PyTorch memory, this process has 90.86 GiB memory in use. Of the allocated memory 89.19 GiB is allocated by PyTorch, and 206.78 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank0]: Traceback (most recent call last):
[rank0]:   File "/iopsstor/scratch/cscs/ldionysiou/project/train.py", line 185, in <module>
[rank0]:     train(args)
[rank0]:   File "/iopsstor/scratch/cscs/ldionysiou/project/train.py", line 137, in train
[rank0]:     logits = model_engine(input_ids)
[rank0]:              ^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1747, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/usr/local/lib/python3.12/dist-packages/deepspeed/utils/nvtx.py", line 18, in wrapped_fn
[rank0]:     ret_val = func(*args, **kwargs)
[rank0]:               ^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/usr/local/lib/python3.12/dist-packages/deepspeed/runtime/engine.py", line 1890, in forward
[rank0]:     loss = self.module(*inputs, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1747, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/iopsstor/scratch/cscs/ldionysiou/project/model.py", line 370, in forward
[rank0]:     h = layer(h, self.freqs_cis)
[rank0]:         ^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1747, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/iopsstor/scratch/cscs/ldionysiou/project/model.py", line 308, in forward
[rank0]:     out = h + self.feed_forward(self.ffn_norm(h))
[rank0]:               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1747, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/iopsstor/scratch/cscs/ldionysiou/project/model.py", line 252, in forward
[rank0]:     return self.w2(F.silu(self.w1(x)) * self.w3(x))
[rank0]:                    ^^^^^^^^^^^^^^^^^^
[rank0]:   File "/usr/local/lib/python3.12/dist-packages/torch/nn/functional.py", line 2380, in silu
[rank0]:     return torch._C._nn.silu(input)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 56.00 MiB. GPU 0 has a total capacity of 94.50 GiB of which 2.58 GiB is free. Including non-PyTorch memory, this process has 90.86 GiB memory in use. Of the allocated memory 89.19 GiB is allocated by PyTorch, and 206.78 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank0]:[W1115 20:21:03.195067865 ProcessGroupNCCL.cpp:1294] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
W1115 20:21:03.992000 20045 torch/distributed/elastic/multiprocessing/api.py:897] Sending process 20457 closing signal SIGTERM
W1115 20:21:03.992000 20045 torch/distributed/elastic/multiprocessing/api.py:897] Sending process 20459 closing signal SIGTERM
W1115 20:21:03.992000 20045 torch/distributed/elastic/multiprocessing/api.py:897] Sending process 20460 closing signal SIGTERM
E1115 20:21:04.507000 20045 torch/distributed/elastic/multiprocessing/api.py:869] failed (exitcode: 1) local_rank: 1 (pid: 20458) of binary: /usr/bin/python3
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/usr/local/lib/python3.12/dist-packages/torch/distributed/run.py", line 923, in <module>
    main()
  File "/usr/local/lib/python3.12/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 355, in wrapper
    return f(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/distributed/run.py", line 919, in main
    run(args)
  File "/usr/local/lib/python3.12/dist-packages/torch/distributed/run.py", line 910, in run
    elastic_launch(
  File "/usr/local/lib/python3.12/dist-packages/torch/distributed/launcher/api.py", line 138, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/distributed/launcher/api.py", line 269, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
/iopsstor/scratch/cscs/ldionysiou/project/train.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-11-15_20:21:03
  host      : nid006903
  rank      : 1 (local_rank: 1)
  exitcode  : 1 (pid: 20458)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
srun: error: nid006903: task 0: Exited with exit code 1
srun: Terminating StepId=1100401.0
