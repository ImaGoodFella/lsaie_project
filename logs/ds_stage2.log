[2025-11-15 20:25:18,797] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-11-15 20:25:19,174] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-11-15 20:25:19,445] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-11-15 20:25:19,754] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The default cache directory for DeepSpeed Triton autotune, /users/ldionysiou/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
Warning: The default cache directory for DeepSpeed Triton autotune, /users/ldionysiou/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
Warning: The default cache directory for DeepSpeed Triton autotune, /users/ldionysiou/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
/usr/local/lib/python3.12/dist-packages/deepspeed/runtime/zero/linear.py:47: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  @autocast_custom_fwd
/usr/local/lib/python3.12/dist-packages/deepspeed/runtime/zero/linear.py:66: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  @autocast_custom_bwd
/usr/local/lib/python3.12/dist-packages/deepspeed/runtime/zero/linear.py:47: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  @autocast_custom_fwd
/usr/local/lib/python3.12/dist-packages/deepspeed/runtime/zero/linear.py:66: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  @autocast_custom_bwd
Warning: The default cache directory for DeepSpeed Triton autotune, /users/ldionysiou/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
/usr/local/lib/python3.12/dist-packages/deepspeed/runtime/zero/linear.py:47: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  @autocast_custom_fwd
/usr/local/lib/python3.12/dist-packages/deepspeed/runtime/zero/linear.py:66: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  @autocast_custom_bwd
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
/usr/local/lib/python3.12/dist-packages/deepspeed/runtime/zero/linear.py:47: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  @autocast_custom_fwd
/usr/local/lib/python3.12/dist-packages/deepspeed/runtime/zero/linear.py:66: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  @autocast_custom_bwd
2025-11-15 20:25:22,920 - root - INFO - Experiment args: Namespace(dataset='/capstor/store/cscs/ethz/large-sc-2/datasets/train_data.parquet', tokenizer_name_or_path='unsloth/Mistral-Nemo-Base-2407-bnb-4bit', sequence_length=2048, batch_size=1, fused_optimizer=False, learning_rate=1e-05, lr_warmup_steps=10, training_steps=1000, logging_frequency=5, profile=True, profile_step_start=10, profile_step_end=12, grad_max_norm=1.0, model_dtype='bf16', compile=False, deepspeed=True, deepspeed_config='/iopsstor/scratch/cscs/ldionysiou/project/ds_stage2.json', local_rank=-1)
2025-11-15 20:25:22,920 - root - INFO - Experiment args: Namespace(dataset='/capstor/store/cscs/ethz/large-sc-2/datasets/train_data.parquet', tokenizer_name_or_path='unsloth/Mistral-Nemo-Base-2407-bnb-4bit', sequence_length=2048, batch_size=1, fused_optimizer=False, learning_rate=1e-05, lr_warmup_steps=10, training_steps=1000, logging_frequency=5, profile=True, profile_step_start=10, profile_step_end=12, grad_max_norm=1.0, model_dtype='bf16', compile=False, deepspeed=True, deepspeed_config='/iopsstor/scratch/cscs/ldionysiou/project/ds_stage2.json', local_rank=-1)
2025-11-15 20:25:22,920 - root - INFO - Experiment args: Namespace(dataset='/capstor/store/cscs/ethz/large-sc-2/datasets/train_data.parquet', tokenizer_name_or_path='unsloth/Mistral-Nemo-Base-2407-bnb-4bit', sequence_length=2048, batch_size=1, fused_optimizer=False, learning_rate=1e-05, lr_warmup_steps=10, training_steps=1000, logging_frequency=5, profile=True, profile_step_start=10, profile_step_end=12, grad_max_norm=1.0, model_dtype='bf16', compile=False, deepspeed=True, deepspeed_config='/iopsstor/scratch/cscs/ldionysiou/project/ds_stage2.json', local_rank=-1)
[2025-11-15 20:25:22,921] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-11-15 20:25:22,921] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-11-15 20:25:22,921] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-11-15 20:25:22,921] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
2025-11-15 20:25:22,921 - root - INFO - Experiment args: Namespace(dataset='/capstor/store/cscs/ethz/large-sc-2/datasets/train_data.parquet', tokenizer_name_or_path='unsloth/Mistral-Nemo-Base-2407-bnb-4bit', sequence_length=2048, batch_size=1, fused_optimizer=False, learning_rate=1e-05, lr_warmup_steps=10, training_steps=1000, logging_frequency=5, profile=True, profile_step_start=10, profile_step_end=12, grad_max_norm=1.0, model_dtype='bf16', compile=False, deepspeed=True, deepspeed_config='/iopsstor/scratch/cscs/ldionysiou/project/ds_stage2.json', local_rank=-1)
[2025-11-15 20:25:22,921] [INFO] [comm.py:637:init_distributed] cdb=None
2025-11-15 20:25:22,922 - root - INFO - Setting up DataLoaders...
2025-11-15 20:25:23,399 - root - INFO - Setting up DataLoaders...
2025-11-15 20:25:23,428 - root - INFO - Setting up DataLoaders...
2025-11-15 20:25:23,428 - root - INFO - Setting up DataLoaders...
2025-11-15 20:25:25,711 - root - INFO - Setting up Model...
2025-11-15 20:25:25,843 - root - INFO - Setting up Model...
2025-11-15 20:25:25,954 - root - INFO - Setting up Model...
2025-11-15 20:25:25,954 - root - INFO - Setting up Model...
2025-11-15 20:25:59,349 - root - INFO - Using DeepSpeed
[2025-11-15 20:25:59,352] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.14.5, git-hash=unknown, git-branch=unknown
2025-11-15 20:25:59,455 - root - INFO - Using DeepSpeed
2025-11-15 20:25:59,553 - root - INFO - Using DeepSpeed
2025-11-15 20:25:59,667 - root - INFO - Using DeepSpeed
[2025-11-15 20:26:08,053] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
Using /users/ldionysiou/.cache/torch_extensions/py312_cu126 as PyTorch extensions root...Using /users/ldionysiou/.cache/torch_extensions/py312_cu126 as PyTorch extensions root...

Using /users/ldionysiou/.cache/torch_extensions/py312_cu126 as PyTorch extensions root...
Using /users/ldionysiou/.cache/torch_extensions/py312_cu126 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /users/ldionysiou/.cache/torch_extensions/py312_cu126/fused_adam/build.ninja...
/usr/local/lib/python3.12/dist-packages/torch/utils/cpp_extension.py:2007: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].
  warnings.warn(
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 0.8219137191772461 seconds
Loading extension module fused_adam...
Loading extension module fused_adam...
Loading extension module fused_adam...
Time to load fused_adam op: 0.9069030284881592 secondsTime to load fused_adam op: 0.9080088138580322 seconds

Time to load fused_adam op: 0.903270959854126 seconds
[2025-11-15 20:26:08,962] [INFO] [logging.py:96:log_dist] [Rank 0] Using DeepSpeed Optimizer param name adamw as basic optimizer
[2025-11-15 20:26:08,962] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
[2025-11-15 20:26:08,970] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = FusedAdam
[2025-11-15 20:26:08,970] [INFO] [utils.py:59:is_zero_supported_optimizer] Checking ZeRO support for optimizer=FusedAdam type=<class 'deepspeed.ops.adam.fused_adam.FusedAdam'>
[2025-11-15 20:26:08,970] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer
[2025-11-15 20:26:08,970] [INFO] [stage_1_and_2.py:148:__init__] Reduce bucket size 500,000,000
[2025-11-15 20:26:08,970] [INFO] [stage_1_and_2.py:149:__init__] Allgather bucket size 500,000,000
[2025-11-15 20:26:08,970] [INFO] [stage_1_and_2.py:150:__init__] CPU Offload: False
[2025-11-15 20:26:08,970] [INFO] [stage_1_and_2.py:151:__init__] Round robin gradient partitioning: False
2025-11-15 20:26:58,625 - root - INFO - Starting training!
2025-11-15 20:26:59,225 - root - INFO - Starting training!
2025-11-15 20:26:59,520 - root - INFO - Starting training!
[2025-11-15 20:26:59,736] [INFO] [utils.py:781:see_memory_usage] Before initializing optimizer states
[2025-11-15 20:26:59,736] [INFO] [utils.py:782:see_memory_usage] MA 22.5 GB         Max_MA 26.25 GB         CA 26.26 GB         Max_CA 26 GB 
[2025-11-15 20:26:59,737] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 255.98 GB, percent = 30.0%
[2025-11-15 20:26:59,844] [INFO] [utils.py:781:see_memory_usage] After initializing optimizer states
[2025-11-15 20:26:59,845] [INFO] [utils.py:782:see_memory_usage] MA 22.5 GB         Max_MA 30.0 GB         CA 33.76 GB         Max_CA 34 GB 
[2025-11-15 20:26:59,845] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 271.5 GB, percent = 31.8%
[2025-11-15 20:26:59,846] [INFO] [stage_1_and_2.py:543:__init__] optimizer state initialized
[2025-11-15 20:26:59,927] [INFO] [utils.py:781:see_memory_usage] After initializing ZeRO optimizer
[2025-11-15 20:26:59,928] [INFO] [utils.py:782:see_memory_usage] MA 22.5 GB         Max_MA 22.5 GB         CA 33.76 GB         Max_CA 34 GB 
[2025-11-15 20:26:59,928] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 271.51 GB, percent = 31.8%
[2025-11-15 20:26:59,930] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedZeroOptimizer
[2025-11-15 20:26:59,931] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using configured LR scheduler = WarmupLR
[2025-11-15 20:26:59,931] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = <deepspeed.runtime.lr_schedules.WarmupLR object at 0x400452727da0>
[2025-11-15 20:26:59,931] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[1e-05], mom=[[0.9, 0.95]]
[2025-11-15 20:26:59,931] [INFO] [config.py:997:print] DeepSpeedEngine configuration:
[2025-11-15 20:26:59,932] [INFO] [config.py:1001:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2025-11-15 20:26:59,932] [INFO] [config.py:1001:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2025-11-15 20:26:59,932] [INFO] [config.py:1001:print]   amp_enabled .................. False
[2025-11-15 20:26:59,932] [INFO] [config.py:1001:print]   amp_params ................... False
[2025-11-15 20:26:59,932] [INFO] [config.py:1001:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2025-11-15 20:26:59,932] [INFO] [config.py:1001:print]   bfloat16_enabled ............. True
[2025-11-15 20:26:59,932] [INFO] [config.py:1001:print]   bfloat16_immediate_grad_update  False
[2025-11-15 20:26:59,932] [INFO] [config.py:1001:print]   checkpoint_parallel_write_pipeline  False
[2025-11-15 20:26:59,932] [INFO] [config.py:1001:print]   checkpoint_tag_validation_enabled  True
[2025-11-15 20:26:59,932] [INFO] [config.py:1001:print]   checkpoint_tag_validation_fail  False
[2025-11-15 20:26:59,932] [INFO] [config.py:1001:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x400452d5be00>
[2025-11-15 20:26:59,932] [INFO] [config.py:1001:print]   communication_data_type ...... None
[2025-11-15 20:26:59,932] [INFO] [config.py:1001:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2025-11-15 20:26:59,932] [INFO] [config.py:1001:print]   curriculum_enabled_legacy .... False
[2025-11-15 20:26:59,932] [INFO] [config.py:1001:print]   curriculum_params_legacy ..... False
[2025-11-15 20:26:59,932] [INFO] [config.py:1001:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2025-11-15 20:26:59,932] [INFO] [config.py:1001:print]   data_efficiency_enabled ...... False
[2025-11-15 20:26:59,932] [INFO] [config.py:1001:print]   dataloader_drop_last ......... False
[2025-11-15 20:26:59,932] [INFO] [config.py:1001:print]   disable_allgather ............ False
[2025-11-15 20:26:59,932] [INFO] [config.py:1001:print]   dump_state ................... False
[2025-11-15 20:26:59,932] [INFO] [config.py:1001:print]   dynamic_loss_scale_args ...... None
[2025-11-15 20:26:59,932] [INFO] [config.py:1001:print]   eigenvalue_enabled ........... False
[2025-11-15 20:26:59,932] [INFO] [config.py:1001:print]   eigenvalue_gas_boundary_resolution  1
[2025-11-15 20:26:59,932] [INFO] [config.py:1001:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2025-11-15 20:26:59,932] [INFO] [config.py:1001:print]   eigenvalue_layer_num ......... 0
[2025-11-15 20:26:59,932] [INFO] [config.py:1001:print]   eigenvalue_max_iter .......... 100
[2025-11-15 20:26:59,932] [INFO] [config.py:1001:print]   eigenvalue_stability ......... 1e-06
[2025-11-15 20:26:59,932] [INFO] [config.py:1001:print]   eigenvalue_tol ............... 0.01
[2025-11-15 20:26:59,932] [INFO] [config.py:1001:print]   eigenvalue_verbose ........... False
[2025-11-15 20:26:59,932] [INFO] [config.py:1001:print]   elasticity_enabled ........... False
[2025-11-15 20:26:59,933] [INFO] [config.py:1001:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 10, 
    "module_depth": -1, 
    "top_modules": 3, 
    "detailed": true, 
    "output_file": null
}
[2025-11-15 20:26:59,933] [INFO] [config.py:1001:print]   fp16_auto_cast ............... None
[2025-11-15 20:26:59,933] [INFO] [config.py:1001:print]   fp16_enabled ................. False
[2025-11-15 20:26:59,933] [INFO] [config.py:1001:print]   fp16_master_weights_and_gradients  False
[2025-11-15 20:26:59,933] [INFO] [config.py:1001:print]   global_rank .................. 0
[2025-11-15 20:26:59,933] [INFO] [config.py:1001:print]   grad_accum_dtype ............. None
[2025-11-15 20:26:59,933] [INFO] [config.py:1001:print]   gradient_accumulation_steps .. 1
[2025-11-15 20:26:59,933] [INFO] [config.py:1001:print]   gradient_clipping ............ 1.0
[2025-11-15 20:26:59,933] [INFO] [config.py:1001:print]   gradient_predivide_factor .... 1.0
[2025-11-15 20:26:59,933] [INFO] [config.py:1001:print]   graph_harvesting ............. False
[2025-11-15 20:26:59,933] [INFO] [config.py:1001:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2025-11-15 20:26:59,933] [INFO] [config.py:1001:print]   initial_dynamic_scale ........ 1
[2025-11-15 20:26:59,933] [INFO] [config.py:1001:print]   load_universal_checkpoint .... False
[2025-11-15 20:26:59,933] [INFO] [config.py:1001:print]   loss_scale ................... 1.0
[2025-11-15 20:26:59,933] [INFO] [config.py:1001:print]   memory_breakdown ............. False
[2025-11-15 20:26:59,933] [INFO] [config.py:1001:print]   mics_hierarchial_params_gather  False
[2025-11-15 20:26:59,933] [INFO] [config.py:1001:print]   mics_shard_size .............. -1
[2025-11-15 20:26:59,933] [INFO] [config.py:1001:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') comet=CometConfig(enabled=False, samples_log_interval=100, project=None, workspace=None, api_key=None, experiment_name=None, experiment_key=None, online=None, mode=None) wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2025-11-15 20:26:59,933] [INFO] [config.py:1001:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2025-11-15 20:26:59,933] [INFO] [config.py:1001:print]   optimizer_legacy_fusion ...... False
[2025-11-15 20:26:59,933] [INFO] [config.py:1001:print]   optimizer_name ............... adamw
[2025-11-15 20:26:59,933] [INFO] [config.py:1001:print]   optimizer_params ............. {'lr': 1e-05, 'betas': [0.9, 0.95], 'eps': 1e-08, 'weight_decay': 0.01}
[2025-11-15 20:26:59,933] [INFO] [config.py:1001:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2025-11-15 20:26:59,933] [INFO] [config.py:1001:print]   pld_enabled .................. False
[2025-11-15 20:26:59,933] [INFO] [config.py:1001:print]   pld_params ................... False
[2025-11-15 20:26:59,933] [INFO] [config.py:1001:print]   prescale_gradients ........... False
[2025-11-15 20:26:59,933] [INFO] [config.py:1001:print]   scheduler_name ............... WarmupLR
[2025-11-15 20:26:59,933] [INFO] [config.py:1001:print]   scheduler_params ............. {'warmup_min_lr': 0, 'warmup_max_lr': 1e-05, 'warmup_num_steps': 10}
[2025-11-15 20:26:59,933] [INFO] [config.py:1001:print]   seq_parallel_communication_data_type  torch.float32
[2025-11-15 20:26:59,933] [INFO] [config.py:1001:print]   sparse_attention ............. None
[2025-11-15 20:26:59,933] [INFO] [config.py:1001:print]   sparse_gradients_enabled ..... False
[2025-11-15 20:26:59,933] [INFO] [config.py:1001:print]   steps_per_print .............. 10
[2025-11-15 20:26:59,933] [INFO] [config.py:1001:print]   timers_config ................ enabled=True synchronized=True
[2025-11-15 20:26:59,933] [INFO] [config.py:1001:print]   train_batch_size ............. 4
[2025-11-15 20:26:59,933] [INFO] [config.py:1001:print]   train_micro_batch_size_per_gpu  1
[2025-11-15 20:26:59,933] [INFO] [config.py:1001:print]   use_data_before_expert_parallel_  False
[2025-11-15 20:26:59,933] [INFO] [config.py:1001:print]   use_node_local_storage ....... False
[2025-11-15 20:26:59,933] [INFO] [config.py:1001:print]   wall_clock_breakdown ......... False
[2025-11-15 20:26:59,933] [INFO] [config.py:1001:print]   weight_quantization_config ... None
[2025-11-15 20:26:59,933] [INFO] [config.py:1001:print]   world_size ................... 4
[2025-11-15 20:26:59,933] [INFO] [config.py:1001:print]   zero_allow_untested_optimizer  False
[2025-11-15 20:26:59,933] [INFO] [config.py:1001:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False use_all_reduce_for_fetch_params=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2025-11-15 20:26:59,934] [INFO] [config.py:1001:print]   zero_enabled ................. True
[2025-11-15 20:26:59,934] [INFO] [config.py:1001:print]   zero_force_ds_cpu_optimizer .. True
[2025-11-15 20:26:59,934] [INFO] [config.py:1001:print]   zero_optimization_stage ...... 2
[2025-11-15 20:26:59,934] [INFO] [config.py:987:print_user_config]   json = {
    "train_micro_batch_size_per_gpu": 1, 
    "train_batch_size": 4, 
    "gradient_accumulation_steps": 1, 
    "optimizer": {
        "type": "AdamW", 
        "params": {
            "lr": 1e-05, 
            "betas": [0.9, 0.95], 
            "eps": 1e-08, 
            "weight_decay": 0.01
        }
    }, 
    "torch_adam": true, 
    "scheduler": {
        "type": "WarmupLR", 
        "params": {
            "warmup_min_lr": 0, 
            "warmup_max_lr": 1e-05, 
            "warmup_num_steps": 10
        }
    }, 
    "bf16": {
        "enabled": true
    }, 
    "zero_optimization": {
        "stage": 2, 
        "contiguous_gradients": true, 
        "overlap_comm": true
    }, 
    "gradient_clipping": 1.0, 
    "flops_profiler": {
        "enabled": false, 
        "profile_step": 10, 
        "module_depth": -1, 
        "top_modules": 3, 
        "detailed": true
    }, 
    "wall_clock_breakdown": false
}
2025-11-15 20:26:59,935 - root - INFO - Starting training!
/usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py:825: UserWarning: cuDNN SDPA backward got grad_output.strides() != output.strides(), attempting to materialize a grad_output with matching strides... (Triggered internally at /opt/pytorch/pytorch/aten/src/ATen/native/cudnn/MHA.cpp:674.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py:825: UserWarning: cuDNN SDPA backward got grad_output.strides() != output.strides(), attempting to materialize a grad_output with matching strides... (Triggered internally at /opt/pytorch/pytorch/aten/src/ATen/native/cudnn/MHA.cpp:674.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py:825: UserWarning: cuDNN SDPA backward got grad_output.strides() != output.strides(), attempting to materialize a grad_output with matching strides... (Triggered internally at /opt/pytorch/pytorch/aten/src/ATen/native/cudnn/MHA.cpp:674.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py:825: UserWarning: cuDNN SDPA backward got grad_output.strides() != output.strides(), attempting to materialize a grad_output with matching strides... (Triggered internally at /opt/pytorch/pytorch/aten/src/ATen/native/cudnn/MHA.cpp:674.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2025-11-15 20:27:01,430 - root - INFO - Step: 1 | Loss: 11.88 | Tokens per second (per-GPU): 1424.80 | Total TFLOPs: 275.38 | MFU (%): 6.96
2025-11-15 20:27:03,020 - root - INFO - Step: 5 | Loss: 10.85 | Tokens per second (per-GPU): 5153.97 | Total TFLOPs: 996.16 | MFU (%): 25.18
[2025-11-15 20:27:04,854] [INFO] [logging.py:96:log_dist] [Rank 0] step=10, skipped=0, lr=[9.999999999999999e-06], mom=[[0.9, 0.95]]
[2025-11-15 20:27:04,928] [INFO] [timer.py:259:stop] epoch=0/micro_step=10/global_step=10, RunningAvgSamplesPerSec=10.545974217163058, CurrSamplesPerSec=10.304622800933942, MemAllocated=37.56GB, MaxMemAllocated=56.0GB
2025-11-15 20:27:04,929 - root - INFO - Step: 10 | Loss: 10.38 | Tokens per second (per-GPU): 5366.52 | Total TFLOPs: 1037.24 | MFU (%): 26.22
2025-11-15 20:27:05,690 - root - INFO - Ending profiling range
2025-11-15 20:27:05,690 - root - INFO - Ending profiling range
2025-11-15 20:27:05,690 - root - INFO - Ending profiling range
2025-11-15 20:27:05,690 - root - INFO - Ending profiling range
2025-11-15 20:27:06,817 - root - INFO - Step: 15 | Loss: 9.99 | Tokens per second (per-GPU): 5423.12 | Total TFLOPs: 1048.18 | MFU (%): 26.50
[2025-11-15 20:27:08,624] [INFO] [logging.py:96:log_dist] [Rank 0] step=20, skipped=0, lr=[1e-05], mom=[[0.9, 0.95]]
[2025-11-15 20:27:08,698] [INFO] [timer.py:259:stop] epoch=0/micro_step=20/global_step=20, RunningAvgSamplesPerSec=10.61944218422093, CurrSamplesPerSec=10.743729059665435, MemAllocated=37.56GB, MaxMemAllocated=56.0GB
2025-11-15 20:27:08,699 - root - INFO - Step: 20 | Loss: 8.55 | Tokens per second (per-GPU): 5443.00 | Total TFLOPs: 1052.02 | MFU (%): 26.59
2025-11-15 20:27:10,574 - root - INFO - Step: 25 | Loss: 9.71 | Tokens per second (per-GPU): 5462.54 | Total TFLOPs: 1055.80 | MFU (%): 26.69
[2025-11-15 20:27:12,396] [INFO] [logging.py:96:log_dist] [Rank 0] step=30, skipped=0, lr=[1e-05], mom=[[0.9, 0.95]]
[2025-11-15 20:27:12,471] [INFO] [timer.py:259:stop] epoch=0/micro_step=30/global_step=30, RunningAvgSamplesPerSec=10.633806079438322, CurrSamplesPerSec=10.878810087332631, MemAllocated=37.56GB, MaxMemAllocated=56.0GB
2025-11-15 20:27:12,471 - root - INFO - Step: 30 | Loss: 8.79 | Tokens per second (per-GPU): 5396.75 | Total TFLOPs: 1043.08 | MFU (%): 26.37
2025-11-15 20:27:14,372 - root - INFO - Step: 35 | Loss: 8.89 | Tokens per second (per-GPU): 5389.07 | Total TFLOPs: 1041.60 | MFU (%): 26.33
[2025-11-15 20:27:16,187] [INFO] [logging.py:96:log_dist] [Rank 0] step=40, skipped=0, lr=[1e-05], mom=[[0.9, 0.95]]
[2025-11-15 20:27:16,262] [INFO] [timer.py:259:stop] epoch=0/micro_step=40/global_step=40, RunningAvgSamplesPerSec=10.631242432733318, CurrSamplesPerSec=10.499769354640609, MemAllocated=37.56GB, MaxMemAllocated=56.0GB
2025-11-15 20:27:16,262 - root - INFO - Step: 40 | Loss: 8.72 | Tokens per second (per-GPU): 5417.10 | Total TFLOPs: 1047.02 | MFU (%): 26.47
2025-11-15 20:27:18,186 - root - INFO - Step: 45 | Loss: 8.34 | Tokens per second (per-GPU): 5325.00 | Total TFLOPs: 1029.22 | MFU (%): 26.02
[2025-11-15 20:27:20,044] [INFO] [logging.py:96:log_dist] [Rank 0] step=50, skipped=0, lr=[1e-05], mom=[[0.9, 0.95]]
[2025-11-15 20:27:20,118] [INFO] [timer.py:259:stop] epoch=0/micro_step=50/global_step=50, RunningAvgSamplesPerSec=10.587080427480132, CurrSamplesPerSec=10.36571085237896, MemAllocated=37.56GB, MaxMemAllocated=56.0GB
2025-11-15 20:27:20,119 - root - INFO - Step: 50 | Loss: 7.87 | Tokens per second (per-GPU): 5299.07 | Total TFLOPs: 1024.20 | MFU (%): 25.89
2025-11-15 20:27:22,039 - root - INFO - Step: 55 | Loss: 8.10 | Tokens per second (per-GPU): 5334.06 | Total TFLOPs: 1030.97 | MFU (%): 26.06
[2025-11-15 20:27:23,905] [INFO] [logging.py:96:log_dist] [Rank 0] step=60, skipped=0, lr=[1e-05], mom=[[0.9, 0.95]]
[2025-11-15 20:27:23,981] [INFO] [timer.py:259:stop] epoch=0/micro_step=60/global_step=60, RunningAvgSamplesPerSec=10.56067925669367, CurrSamplesPerSec=10.51583400822063, MemAllocated=37.56GB, MaxMemAllocated=56.0GB
2025-11-15 20:27:23,981 - root - INFO - Step: 60 | Loss: 8.11 | Tokens per second (per-GPU): 5272.46 | Total TFLOPs: 1019.06 | MFU (%): 25.76
2025-11-15 20:27:25,890 - root - INFO - Step: 65 | Loss: 8.55 | Tokens per second (per-GPU): 5366.17 | Total TFLOPs: 1037.17 | MFU (%): 26.22
[2025-11-15 20:27:27,680] [INFO] [logging.py:96:log_dist] [Rank 0] step=70, skipped=0, lr=[1e-05], mom=[[0.9, 0.95]]
[2025-11-15 20:27:27,755] [INFO] [timer.py:259:stop] epoch=0/micro_step=70/global_step=70, RunningAvgSamplesPerSec=10.574470868942266, CurrSamplesPerSec=10.810652580487309, MemAllocated=37.56GB, MaxMemAllocated=56.0GB
2025-11-15 20:27:27,755 - root - INFO - Step: 70 | Loss: 7.09 | Tokens per second (per-GPU): 5489.33 | Total TFLOPs: 1060.98 | MFU (%): 26.82
2025-11-15 20:27:29,651 - root - INFO - Step: 75 | Loss: 6.67 | Tokens per second (per-GPU): 5403.68 | Total TFLOPs: 1044.42 | MFU (%): 26.40
[2025-11-15 20:27:31,500] [INFO] [logging.py:96:log_dist] [Rank 0] step=80, skipped=0, lr=[1e-05], mom=[[0.9, 0.95]]
[2025-11-15 20:27:31,575] [INFO] [timer.py:259:stop] epoch=0/micro_step=80/global_step=80, RunningAvgSamplesPerSec=10.574581058025332, CurrSamplesPerSec=10.422934496560815, MemAllocated=37.56GB, MaxMemAllocated=56.0GB
2025-11-15 20:27:31,575 - root - INFO - Step: 80 | Loss: 7.67 | Tokens per second (per-GPU): 5321.70 | Total TFLOPs: 1028.58 | MFU (%): 26.00
2025-11-15 20:27:33,492 - root - INFO - Step: 85 | Loss: 7.83 | Tokens per second (per-GPU): 5342.22 | Total TFLOPs: 1032.54 | MFU (%): 26.10
[2025-11-15 20:27:35,298] [INFO] [logging.py:96:log_dist] [Rank 0] step=90, skipped=0, lr=[1e-05], mom=[[0.9, 0.95]]
[2025-11-15 20:27:35,373] [INFO] [timer.py:259:stop] epoch=0/micro_step=90/global_step=90, RunningAvgSamplesPerSec=10.577004012505064, CurrSamplesPerSec=10.770925681217571, MemAllocated=37.56GB, MaxMemAllocated=56.0GB
2025-11-15 20:27:35,374 - root - INFO - Step: 90 | Loss: 7.93 | Tokens per second (per-GPU): 5444.75 | Total TFLOPs: 1052.36 | MFU (%): 26.60
2025-11-15 20:27:37,289 - root - INFO - Step: 95 | Loss: 7.59 | Tokens per second (per-GPU): 5345.40 | Total TFLOPs: 1033.16 | MFU (%): 26.12
[2025-11-15 20:27:39,176] [INFO] [logging.py:96:log_dist] [Rank 0] step=100, skipped=0, lr=[1e-05], mom=[[0.9, 0.95]]
[2025-11-15 20:27:39,251] [INFO] [timer.py:259:stop] epoch=0/micro_step=100/global_step=100, RunningAvgSamplesPerSec=10.559989113270053, CurrSamplesPerSec=10.382851700192953, MemAllocated=37.56GB, MaxMemAllocated=56.0GB
2025-11-15 20:27:39,251 - root - INFO - Step: 100 | Loss: 7.08 | Tokens per second (per-GPU): 5221.10 | Total TFLOPs: 1009.13 | MFU (%): 25.51
2025-11-15 20:27:41,174 - root - INFO - Step: 105 | Loss: 7.41 | Tokens per second (per-GPU): 5326.68 | Total TFLOPs: 1029.54 | MFU (%): 26.02
[2025-11-15 20:27:42,999] [INFO] [logging.py:96:log_dist] [Rank 0] step=110, skipped=0, lr=[1e-05], mom=[[0.9, 0.95]]
[2025-11-15 20:27:43,074] [INFO] [timer.py:259:stop] epoch=0/micro_step=110/global_step=110, RunningAvgSamplesPerSec=10.55735142841611, CurrSamplesPerSec=10.323924772992928, MemAllocated=37.56GB, MaxMemAllocated=56.0GB
2025-11-15 20:27:43,074 - root - INFO - Step: 110 | Loss: 7.21 | Tokens per second (per-GPU): 5389.76 | Total TFLOPs: 1041.73 | MFU (%): 26.33
2025-11-15 20:27:44,970 - root - INFO - Step: 115 | Loss: 7.06 | Tokens per second (per-GPU): 5401.19 | Total TFLOPs: 1043.94 | MFU (%): 26.39
[2025-11-15 20:27:46,774] [INFO] [logging.py:96:log_dist] [Rank 0] step=120, skipped=0, lr=[1e-05], mom=[[0.9, 0.95]]
[2025-11-15 20:27:46,849] [INFO] [timer.py:259:stop] epoch=0/micro_step=120/global_step=120, RunningAvgSamplesPerSec=10.564840009829295, CurrSamplesPerSec=10.594217010078971, MemAllocated=37.56GB, MaxMemAllocated=56.0GB
2025-11-15 20:27:46,849 - root - INFO - Step: 120 | Loss: 7.34 | Tokens per second (per-GPU): 5451.14 | Total TFLOPs: 1053.60 | MFU (%): 26.63
2025-11-15 20:27:48,752 - root - INFO - Step: 125 | Loss: 7.29 | Tokens per second (per-GPU): 5381.24 | Total TFLOPs: 1040.09 | MFU (%): 26.29
[2025-11-15 20:27:50,623] [INFO] [logging.py:96:log_dist] [Rank 0] step=130, skipped=0, lr=[1e-05], mom=[[0.9, 0.95]]
[2025-11-15 20:27:50,698] [INFO] [timer.py:259:stop] epoch=0/micro_step=130/global_step=130, RunningAvgSamplesPerSec=10.556750227737504, CurrSamplesPerSec=10.148007701307188, MemAllocated=37.56GB, MaxMemAllocated=56.0GB
2025-11-15 20:27:50,698 - root - INFO - Step: 130 | Loss: 7.37 | Tokens per second (per-GPU): 5263.84 | Total TFLOPs: 1017.39 | MFU (%): 25.72
2025-11-15 20:27:52,598 - root - INFO - Step: 135 | Loss: 7.12 | Tokens per second (per-GPU): 5391.01 | Total TFLOPs: 1041.97 | MFU (%): 26.34
[2025-11-15 20:27:54,434] [INFO] [logging.py:96:log_dist] [Rank 0] step=140, skipped=0, lr=[1e-05], mom=[[0.9, 0.95]]
[2025-11-15 20:27:54,509] [INFO] [timer.py:259:stop] epoch=0/micro_step=140/global_step=140, RunningAvgSamplesPerSec=10.556793402053733, CurrSamplesPerSec=10.861695540740136, MemAllocated=37.56GB, MaxMemAllocated=56.0GB
2025-11-15 20:27:54,510 - root - INFO - Step: 140 | Loss: 7.50 | Tokens per second (per-GPU): 5357.06 | Total TFLOPs: 1035.41 | MFU (%): 26.17
2025-11-15 20:27:56,430 - root - INFO - Step: 145 | Loss: 7.73 | Tokens per second (per-GPU): 5333.87 | Total TFLOPs: 1030.93 | MFU (%): 26.06
[2025-11-15 20:27:58,259] [INFO] [logging.py:96:log_dist] [Rank 0] step=150, skipped=0, lr=[1e-05], mom=[[0.9, 0.95]]
[2025-11-15 20:27:58,334] [INFO] [timer.py:259:stop] epoch=0/micro_step=150/global_step=150, RunningAvgSamplesPerSec=10.554129903103213, CurrSamplesPerSec=10.629913625253762, MemAllocated=37.56GB, MaxMemAllocated=56.0GB
2025-11-15 20:27:58,334 - root - INFO - Step: 150 | Loss: 7.71 | Tokens per second (per-GPU): 5378.58 | Total TFLOPs: 1039.57 | MFU (%): 26.28
2025-11-15 20:28:00,274 - root - INFO - Step: 155 | Loss: 7.40 | Tokens per second (per-GPU): 5278.68 | Total TFLOPs: 1020.26 | MFU (%): 25.79
[2025-11-15 20:28:02,140] [INFO] [logging.py:96:log_dist] [Rank 0] step=160, skipped=0, lr=[1e-05], mom=[[0.9, 0.95]]
[2025-11-15 20:28:02,215] [INFO] [timer.py:259:stop] epoch=0/micro_step=160/global_step=160, RunningAvgSamplesPerSec=10.546913574882947, CurrSamplesPerSec=10.412474438849179, MemAllocated=37.56GB, MaxMemAllocated=56.0GB
2025-11-15 20:28:02,216 - root - INFO - Step: 160 | Loss: 7.02 | Tokens per second (per-GPU): 5275.75 | Total TFLOPs: 1019.70 | MFU (%): 25.78
2025-11-15 20:28:04,135 - root - INFO - Step: 165 | Loss: 7.65 | Tokens per second (per-GPU): 5334.45 | Total TFLOPs: 1031.04 | MFU (%): 26.06
[2025-11-15 20:28:06,015] [INFO] [logging.py:96:log_dist] [Rank 0] step=170, skipped=0, lr=[1e-05], mom=[[0.9, 0.95]]
[2025-11-15 20:28:06,090] [INFO] [timer.py:259:stop] epoch=0/micro_step=170/global_step=170, RunningAvgSamplesPerSec=10.537399766326581, CurrSamplesPerSec=9.923587559626677, MemAllocated=37.56GB, MaxMemAllocated=56.0GB
2025-11-15 20:28:06,090 - root - INFO - Step: 170 | Loss: 6.92 | Tokens per second (per-GPU): 5239.16 | Total TFLOPs: 1012.62 | MFU (%): 25.60
2025-11-15 20:28:08,033 - root - INFO - Step: 175 | Loss: 7.23 | Tokens per second (per-GPU): 5272.33 | Total TFLOPs: 1019.04 | MFU (%): 25.76
[2025-11-15 20:28:09,842] [INFO] [logging.py:96:log_dist] [Rank 0] step=180, skipped=0, lr=[1e-05], mom=[[0.9, 0.95]]
[2025-11-15 20:28:09,917] [INFO] [timer.py:259:stop] epoch=0/micro_step=180/global_step=180, RunningAvgSamplesPerSec=10.539807414471044, CurrSamplesPerSec=10.40408665627595, MemAllocated=37.56GB, MaxMemAllocated=56.0GB
2025-11-15 20:28:09,918 - root - INFO - Step: 180 | Loss: 6.99 | Tokens per second (per-GPU): 5434.21 | Total TFLOPs: 1050.32 | MFU (%): 26.55
2025-11-15 20:28:11,846 - root - INFO - Step: 185 | Loss: 7.19 | Tokens per second (per-GPU): 5312.08 | Total TFLOPs: 1026.72 | MFU (%): 25.95
[2025-11-15 20:28:13,729] [INFO] [logging.py:96:log_dist] [Rank 0] step=190, skipped=0, lr=[1e-05], mom=[[0.9, 0.95]]
[2025-11-15 20:28:13,804] [INFO] [timer.py:259:stop] epoch=0/micro_step=190/global_step=190, RunningAvgSamplesPerSec=10.529167367569828, CurrSamplesPerSec=10.755465433944222, MemAllocated=37.56GB, MaxMemAllocated=56.0GB
2025-11-15 20:28:13,804 - root - INFO - Step: 190 | Loss: 7.05 | Tokens per second (per-GPU): 5229.48 | Total TFLOPs: 1010.75 | MFU (%): 25.55
2025-11-15 20:28:15,732 - root - INFO - Step: 195 | Loss: 6.86 | Tokens per second (per-GPU): 5311.64 | Total TFLOPs: 1026.63 | MFU (%): 25.95
[2025-11-15 20:28:17,572] [INFO] [logging.py:96:log_dist] [Rank 0] step=200, skipped=0, lr=[1e-05], mom=[[0.9, 0.95]]
[2025-11-15 20:28:17,647] [INFO] [timer.py:259:stop] epoch=0/micro_step=200/global_step=200, RunningAvgSamplesPerSec=10.526035509175962, CurrSamplesPerSec=10.551766613615763, MemAllocated=37.56GB, MaxMemAllocated=56.0GB
2025-11-15 20:28:17,647 - root - INFO - Step: 200 | Loss: 7.37 | Tokens per second (per-GPU): 5347.36 | Total TFLOPs: 1033.54 | MFU (%): 26.13
2025-11-15 20:28:19,555 - root - INFO - Step: 205 | Loss: 6.73 | Tokens per second (per-GPU): 5367.68 | Total TFLOPs: 1037.46 | MFU (%): 26.23
[2025-11-15 20:28:21,358] [INFO] [logging.py:96:log_dist] [Rank 0] step=210, skipped=0, lr=[1e-05], mom=[[0.9, 0.95]]
[2025-11-15 20:28:21,433] [INFO] [timer.py:259:stop] epoch=0/micro_step=210/global_step=210, RunningAvgSamplesPerSec=10.531431826220189, CurrSamplesPerSec=10.374781004528776, MemAllocated=37.56GB, MaxMemAllocated=56.0GB
2025-11-15 20:28:21,433 - root - INFO - Step: 210 | Loss: 6.57 | Tokens per second (per-GPU): 5454.48 | Total TFLOPs: 1054.24 | MFU (%): 26.65
2025-11-15 20:28:23,329 - root - INFO - Step: 215 | Loss: 7.67 | Tokens per second (per-GPU): 5403.21 | Total TFLOPs: 1044.33 | MFU (%): 26.40
[2025-11-15 20:28:25,209] [INFO] [logging.py:96:log_dist] [Rank 0] step=220, skipped=0, lr=[1e-05], mom=[[0.9, 0.95]]
[2025-11-15 20:28:25,284] [INFO] [timer.py:259:stop] epoch=0/micro_step=220/global_step=220, RunningAvgSamplesPerSec=10.527589390447007, CurrSamplesPerSec=10.288022949532479, MemAllocated=37.56GB, MaxMemAllocated=56.0GB
2025-11-15 20:28:25,284 - root - INFO - Step: 220 | Loss: 6.82 | Tokens per second (per-GPU): 5237.23 | Total TFLOPs: 1012.25 | MFU (%): 25.59
2025-11-15 20:28:27,193 - root - INFO - Step: 225 | Loss: 7.29 | Tokens per second (per-GPU): 5365.71 | Total TFLOPs: 1037.08 | MFU (%): 26.22
[2025-11-15 20:28:29,035] [INFO] [logging.py:96:log_dist] [Rank 0] step=230, skipped=0, lr=[1e-05], mom=[[0.9, 0.95]]
[2025-11-15 20:28:29,110] [INFO] [timer.py:259:stop] epoch=0/micro_step=230/global_step=230, RunningAvgSamplesPerSec=10.527753529235865, CurrSamplesPerSec=10.490466322800128, MemAllocated=37.56GB, MaxMemAllocated=56.0GB
2025-11-15 20:28:29,110 - root - INFO - Step: 230 | Loss: 6.65 | Tokens per second (per-GPU): 5341.40 | Total TFLOPs: 1032.39 | MFU (%): 26.10
2025-11-15 20:28:31,012 - root - INFO - Step: 235 | Loss: 6.92 | Tokens per second (per-GPU): 5386.76 | Total TFLOPs: 1041.15 | MFU (%): 26.32
[2025-11-15 20:28:32,842] [INFO] [logging.py:96:log_dist] [Rank 0] step=240, skipped=0, lr=[1e-05], mom=[[0.9, 0.95]]
[2025-11-15 20:28:32,917] [INFO] [timer.py:259:stop] epoch=0/micro_step=240/global_step=240, RunningAvgSamplesPerSec=10.529441004425225, CurrSamplesPerSec=10.237905464113489, MemAllocated=37.56GB, MaxMemAllocated=56.0GB
2025-11-15 20:28:32,917 - root - INFO - Step: 240 | Loss: 6.62 | Tokens per second (per-GPU): 5374.24 | Total TFLOPs: 1038.73 | MFU (%): 26.26
2025-11-15 20:28:34,841 - root - INFO - Step: 245 | Loss: 7.03 | Tokens per second (per-GPU): 5325.22 | Total TFLOPs: 1029.26 | MFU (%): 26.02
[2025-11-15 20:28:36,659] [INFO] [logging.py:96:log_dist] [Rank 0] step=250, skipped=0, lr=[1e-05], mom=[[0.9, 0.95]]
2025-11-15 20:28:36,733 - root - INFO - Training completed
2025-11-15 20:28:36,733 - root - INFO - Training completed
2025-11-15 20:28:36,733 - root - INFO - Training completed
[2025-11-15 20:28:36,733] [INFO] [timer.py:259:stop] epoch=0/micro_step=250/global_step=250, RunningAvgSamplesPerSec=10.530548695312273, CurrSamplesPerSec=10.846983573845574, MemAllocated=37.56GB, MaxMemAllocated=56.0GB
2025-11-15 20:28:36,734 - root - INFO - Step: 250 | Loss: 7.14 | Tokens per second (per-GPU): 5410.10 | Total TFLOPs: 1045.66 | MFU (%): 26.43
2025-11-15 20:28:36,734 - root - INFO - Training completed
[rank0]:[W1115 20:28:37.024780872 ProcessGroupNCCL.cpp:1294] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
