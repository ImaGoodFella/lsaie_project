START TIME: Wed Dec 17 17:44:19 CET 2025
Running DeepSpeed Stage: 3_compile
Job ID: 1254469
Output will be in: logs/deepspeed/1254469.out
df: /users/bzuidema/.triton/autotune: No such file or directory
[DeepCompile PATCH] Loading patched patch_compiled_func.py (PyTorch 2.6.0a0+ecf3bae40a.nv25.01)
[DeepCompile PATCH] Using PyTorch 2.6 fallback code path (handles NGC alpha versions)
[2025-12-17 17:44:43,452] [WARNING] [runner.py:232:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected VISIBLE_DEVICES=0,1,2,3 but ignoring it because one or several of --include/--exclude/--num_gpus/--num_nodes cl args were used. If you want to use CUDA_VISIBLE_DEVICES don't pass any of these arguments to deepspeed.
[2025-12-17 17:44:43,452] [INFO] [runner.py:630:main] cmd = /usr/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgM119 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None --bind_cores_to_rank --log_level=info /users/bzuidema/scratch/project/src/train.py --deepspeed_config /users/bzuidema/scratch/project/configs/deepspeed/stage_3_compile.json --batch-size 1 --learning-rate 5e-5 --lr-warmup-steps 100 --training-steps 1000 --sequence-length 2048 --deepspeed
[DeepCompile PATCH] Loading patched patch_compiled_func.py (PyTorch 2.6.0a0+ecf3bae40a.nv25.01)
[DeepCompile PATCH] Using PyTorch 2.6 fallback code path (handles NGC alpha versions)
[2025-12-17 17:44:50,132] [INFO] [launch.py:155:main] 0 NCCL_NET_PLUGIN=ofi
[2025-12-17 17:44:50,132] [INFO] [launch.py:155:main] 0 NCCL_VERSION=2.25.1
[2025-12-17 17:44:50,132] [INFO] [launch.py:155:main] 0 NCCL_SOCKET_IFNAME=hsn
[2025-12-17 17:44:50,132] [INFO] [launch.py:155:main] 0 NCCL_NVLS_ENABLE=0
[2025-12-17 17:44:50,132] [INFO] [launch.py:155:main] 0 NCCL_NET_GDR_LEVEL=PHB
[2025-12-17 17:44:50,132] [INFO] [launch.py:155:main] 0 TORCH_NCCL_USE_COMM_NONBLOCKING=0
[2025-12-17 17:44:50,132] [INFO] [launch.py:155:main] 0 NCCL_NET=AWS Libfabric
[2025-12-17 17:44:50,132] [INFO] [launch.py:155:main] 0 AWS_OFI_NCCL_VERSION=1.12.1
[2025-12-17 17:44:50,132] [INFO] [launch.py:155:main] 0 NCCL_CROSS_NIC=1
[2025-12-17 17:44:50,132] [INFO] [launch.py:162:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3]}
[2025-12-17 17:44:50,132] [INFO] [launch.py:168:main] nnodes=1, num_local_procs=4, node_rank=0
[2025-12-17 17:44:50,132] [INFO] [launch.py:179:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3]})
[2025-12-17 17:44:50,132] [INFO] [launch.py:180:main] dist_world_size=4
[2025-12-17 17:44:50,132] [INFO] [launch.py:184:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
[2025-12-17 17:44:50,163] [INFO] [launch.py:272:main] process 37628 spawned with command: ['numactl', '-m', '0', '-C', '0-71', '/usr/bin/python', '-u', '/users/bzuidema/scratch/project/src/train.py', '--local_rank=0', '--deepspeed_config', '/users/bzuidema/scratch/project/configs/deepspeed/stage_3_compile.json', '--batch-size', '1', '--learning-rate', '5e-5', '--lr-warmup-steps', '100', '--training-steps', '1000', '--sequence-length', '2048', '--deepspeed']
[2025-12-17 17:44:50,183] [INFO] [launch.py:272:main] process 37631 spawned with command: ['numactl', '-m', '1', '-C', '72-143', '/usr/bin/python', '-u', '/users/bzuidema/scratch/project/src/train.py', '--local_rank=1', '--deepspeed_config', '/users/bzuidema/scratch/project/configs/deepspeed/stage_3_compile.json', '--batch-size', '1', '--learning-rate', '5e-5', '--lr-warmup-steps', '100', '--training-steps', '1000', '--sequence-length', '2048', '--deepspeed']
[2025-12-17 17:44:50,204] [INFO] [launch.py:272:main] process 37634 spawned with command: ['numactl', '-m', '2', '-C', '144-215', '/usr/bin/python', '-u', '/users/bzuidema/scratch/project/src/train.py', '--local_rank=2', '--deepspeed_config', '/users/bzuidema/scratch/project/configs/deepspeed/stage_3_compile.json', '--batch-size', '1', '--learning-rate', '5e-5', '--lr-warmup-steps', '100', '--training-steps', '1000', '--sequence-length', '2048', '--deepspeed']
[2025-12-17 17:44:50,227] [INFO] [launch.py:272:main] process 37638 spawned with command: ['numactl', '-m', '3', '-C', '216-287', '/usr/bin/python', '-u', '/users/bzuidema/scratch/project/src/train.py', '--local_rank=3', '--deepspeed_config', '/users/bzuidema/scratch/project/configs/deepspeed/stage_3_compile.json', '--batch-size', '1', '--learning-rate', '5e-5', '--lr-warmup-steps', '100', '--training-steps', '1000', '--sequence-length', '2048', '--deepspeed']
[DeepCompile PATCH] Loading patched patch_compiled_func.py (PyTorch 2.6.0a0+ecf3bae40a.nv25.01)
[DeepCompile PATCH] Using PyTorch 2.6 fallback code path (handles NGC alpha versions)
[DeepCompile PATCH] Loading patched patch_compiled_func.py (PyTorch 2.6.0a0+ecf3bae40a.nv25.01)
[DeepCompile PATCH] Using PyTorch 2.6 fallback code path (handles NGC alpha versions)
[DeepCompile PATCH] Loading patched patch_compiled_func.py (PyTorch 2.6.0a0+ecf3bae40a.nv25.01)
[DeepCompile PATCH] Using PyTorch 2.6 fallback code path (handles NGC alpha versions)
[DeepCompile PATCH] Loading patched patch_compiled_func.py (PyTorch 2.6.0a0+ecf3bae40a.nv25.01)
[DeepCompile PATCH] Using PyTorch 2.6 fallback code path (handles NGC alpha versions)
2025-12-17 17:44:59,464 - root - INFO - Experiment args: Namespace(dataset='/capstor/store/cscs/ethz/large-sc-2/datasets/train_data.parquet', tokenizer_name_or_path='unsloth/Mistral-Nemo-Base-2407-bnb-4bit', sequence_length=2048, batch_size=1, fused_optimizer=False, learning_rate=5e-05, lr_warmup_steps=100, training_steps=1000, logging_frequency=5, profile=False, profile_step_start=10, profile_step_end=12, grad_max_norm=1, model_dtype='bf16', compile=False, deepspeed=True, deepspeed_config='/users/bzuidema/scratch/project/configs/deepspeed/stage_3_compile.json', local_rank=2)
2025-12-17 17:44:59,563 - root - INFO - Experiment args: Namespace(dataset='/capstor/store/cscs/ethz/large-sc-2/datasets/train_data.parquet', tokenizer_name_or_path='unsloth/Mistral-Nemo-Base-2407-bnb-4bit', sequence_length=2048, batch_size=1, fused_optimizer=False, learning_rate=5e-05, lr_warmup_steps=100, training_steps=1000, logging_frequency=5, profile=False, profile_step_start=10, profile_step_end=12, grad_max_norm=1, model_dtype='bf16', compile=False, deepspeed=True, deepspeed_config='/users/bzuidema/scratch/project/configs/deepspeed/stage_3_compile.json', local_rank=1)
2025-12-17 17:44:59,741 - root - INFO - Experiment args: Namespace(dataset='/capstor/store/cscs/ethz/large-sc-2/datasets/train_data.parquet', tokenizer_name_or_path='unsloth/Mistral-Nemo-Base-2407-bnb-4bit', sequence_length=2048, batch_size=1, fused_optimizer=False, learning_rate=5e-05, lr_warmup_steps=100, training_steps=1000, logging_frequency=5, profile=False, profile_step_start=10, profile_step_end=12, grad_max_norm=1, model_dtype='bf16', compile=False, deepspeed=True, deepspeed_config='/users/bzuidema/scratch/project/configs/deepspeed/stage_3_compile.json', local_rank=0)
2025-12-17 17:44:59,803 - root - INFO - Experiment args: Namespace(dataset='/capstor/store/cscs/ethz/large-sc-2/datasets/train_data.parquet', tokenizer_name_or_path='unsloth/Mistral-Nemo-Base-2407-bnb-4bit', sequence_length=2048, batch_size=1, fused_optimizer=False, learning_rate=5e-05, lr_warmup_steps=100, training_steps=1000, logging_frequency=5, profile=False, profile_step_start=10, profile_step_end=12, grad_max_norm=1, model_dtype='bf16', compile=False, deepspeed=True, deepspeed_config='/users/bzuidema/scratch/project/configs/deepspeed/stage_3_compile.json', local_rank=3)
2025-12-17 17:45:04,488 - root - INFO - Setting up DataLoaders...
2025-12-17 17:45:04,488 - root - INFO - Setting up DataLoaders...
2025-12-17 17:45:04,488 - root - INFO - Setting up DataLoaders...
2025-12-17 17:45:04,490 - root - INFO - Setting up DataLoaders...
2025-12-17 17:45:07,663 - root - INFO - Setting up Model...
2025-12-17 17:45:07,663 - root - INFO - Setting up Model...
2025-12-17 17:45:07,663 - root - INFO - Setting up Model...
2025-12-17 17:45:07,663 - root - INFO - Setting up Model...
2025-12-17 17:45:07,665 - root - INFO - Using ZeRO Stage 3 partition-at-init
2025-12-17 17:45:07,665 - root - INFO - Using ZeRO Stage 3 partition-at-init
2025-12-17 17:45:07,665 - root - INFO - Using ZeRO Stage 3 partition-at-init
2025-12-17 17:45:07,665 - root - INFO - Using ZeRO Stage 3 partition-at-init
2025-12-17 17:45:12,091 - root - INFO - Model parameters (excluding embedding): 8,053,329,920
2025-12-17 17:45:12,091 - root - INFO - FLOPs per token: 51,541,204,992
2025-12-17 17:45:12,091 - root - INFO - Using DeepSpeed
2025-12-17 17:45:12,092 - root - INFO - Letting DeepSpeed create optimizer from config
2025-12-17 17:45:12,104 - root - INFO - Model parameters (excluding embedding): 8,053,329,920
2025-12-17 17:45:12,104 - root - INFO - FLOPs per token: 51,541,204,992
2025-12-17 17:45:12,104 - root - INFO - Using DeepSpeed
2025-12-17 17:45:12,104 - root - INFO - Letting DeepSpeed create optimizer from config
2025-12-17 17:45:12,104 - root - INFO - Model parameters (excluding embedding): 8,053,329,920
2025-12-17 17:45:12,104 - root - INFO - FLOPs per token: 51,541,204,992
2025-12-17 17:45:12,104 - root - INFO - Using DeepSpeed
2025-12-17 17:45:12,105 - root - INFO - Letting DeepSpeed create optimizer from config
2025-12-17 17:45:12,114 - root - INFO - Model parameters (excluding embedding): 8,053,329,920
2025-12-17 17:45:12,114 - root - INFO - FLOPs per token: 51,541,204,992
2025-12-17 17:45:12,114 - root - INFO - Using DeepSpeed
2025-12-17 17:45:12,115 - root - INFO - Letting DeepSpeed create optimizer from config
/usr/local/lib/python3.12/dist-packages/torch/utils/cpp_extension.py:2011: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].
  warnings.warn(
Stage 3 initialize beginning
MA 3.75 GB         Max_MA 4.75 GB         CA 4.88 GB         Max_CA 5 GB 
CPU Virtual Memory:  used = 176.41 GB, percent = 20.6%
DeepSpeedZeRoOffload initialize [begin]
MA 3.75 GB         Max_MA 3.75 GB         CA 4.88 GB         Max_CA 5 GB 
CPU Virtual Memory:  used = 176.41 GB, percent = 20.6%
Parameter Offload - Persistent parameters statistics: param_count = 65, numel = 266240
DeepSpeedZeRoOffload initialize [end]
MA 3.75 GB         Max_MA 3.75 GB         CA 4.88 GB         Max_CA 5 GB 
CPU Virtual Memory:  used = 176.41 GB, percent = 20.6%
Before creating fp16 partitions
MA 3.75 GB         Max_MA 3.75 GB         CA 4.88 GB         Max_CA 5 GB 
CPU Virtual Memory:  used = 176.41 GB, percent = 20.6%
After creating fp16 partitions: 2
MA 3.75 GB         Max_MA 3.75 GB         CA 3.75 GB         Max_CA 5 GB 
CPU Virtual Memory:  used = 205.87 GB, percent = 24.1%
Before creating fp32 partitions
MA 3.75 GB         Max_MA 3.75 GB         CA 3.75 GB         Max_CA 4 GB 
CPU Virtual Memory:  used = 205.87 GB, percent = 24.1%
After creating fp32 partitions
MA 11.25 GB         Max_MA 15.0 GB         CA 15.01 GB         Max_CA 15 GB 
CPU Virtual Memory:  used = 216.69 GB, percent = 25.4%
Before initializing optimizer states
MA 11.25 GB         Max_MA 11.25 GB         CA 15.01 GB         Max_CA 15 GB 
CPU Virtual Memory:  used = 216.69 GB, percent = 25.4%
After initializing optimizer states
MA 11.25 GB         Max_MA 15.0 GB         CA 15.01 GB         Max_CA 15 GB 
CPU Virtual Memory:  used = 216.69 GB, percent = 25.4%
[2025-12-17 17:45:34,114] [WARNING] [lr_schedules.py:690:get_lr] Attempting to get learning rate from scheduler before it has started
2025-12-17 17:45:34,115 - root - INFO - Enabling Compile in DeepSpeed
[2025-12-17 17:45:34,116] [WARNING] [lr_schedules.py:690:get_lr] Attempting to get learning rate from scheduler before it has started
2025-12-17 17:45:34,117 - root - INFO - Enabling Compile in DeepSpeed
[2025-12-17 17:45:34,119] [WARNING] [lr_schedules.py:690:get_lr] Attempting to get learning rate from scheduler before it has started
2025-12-17 17:45:34,120 - root - INFO - Enabling Compile in DeepSpeed
2025-12-17 17:45:34,160 - root - INFO - Starting training!
2025-12-17 17:45:34,163 - root - INFO - Starting training!
2025-12-17 17:45:34,163 - root - INFO - Starting training!
After initializing ZeRO optimizer
MA 15.93 GB         Max_MA 17.93 GB         CA 18.76 GB         Max_CA 19 GB 
CPU Virtual Memory:  used = 217.9 GB, percent = 25.5%
[2025-12-17 17:45:34,218] [WARNING] [lr_schedules.py:690:get_lr] Attempting to get learning rate from scheduler before it has started
2025-12-17 17:45:34,220 - root - INFO - Enabling Compile in DeepSpeed
2025-12-17 17:45:34,265 - root - INFO - Starting training!
[rank2]:W1217 17:45:36.875000 37634 torch/_dynamo/convert_frame.py:915] [2/8] torch._dynamo hit config.cache_size_limit (8)
[rank2]:W1217 17:45:36.875000 37634 torch/_dynamo/convert_frame.py:915] [2/8]    function: 'torch_dynamo_resume_in__forward_prologue_at_2187' (/users/bzuidema/lsaie_deepspeed/deepspeed/runtime/engine.py:2187)
[rank2]:W1217 17:45:36.875000 37634 torch/_dynamo/convert_frame.py:915] [2/8]    last reason: 2/0: ___tuple_iterator_len(L['___stack0']) == 388                
[rank2]:W1217 17:45:36.875000 37634 torch/_dynamo/convert_frame.py:915] [2/8] To log all recompilation reasons, use TORCH_LOGS="recompiles".
[rank2]:W1217 17:45:36.875000 37634 torch/_dynamo/convert_frame.py:915] [2/8] To diagnose recompilation issues, see https://pytorch.org/docs/main/torch.compiler_troubleshooting.html.
[rank1]:W1217 17:45:36.879000 37631 torch/_dynamo/convert_frame.py:915] [2/8] torch._dynamo hit config.cache_size_limit (8)
[rank1]:W1217 17:45:36.879000 37631 torch/_dynamo/convert_frame.py:915] [2/8]    function: 'torch_dynamo_resume_in__forward_prologue_at_2187' (/users/bzuidema/lsaie_deepspeed/deepspeed/runtime/engine.py:2187)
[rank1]:W1217 17:45:36.879000 37631 torch/_dynamo/convert_frame.py:915] [2/8]    last reason: 2/0: ___tuple_iterator_len(L['___stack0']) == 388                
[rank1]:W1217 17:45:36.879000 37631 torch/_dynamo/convert_frame.py:915] [2/8] To log all recompilation reasons, use TORCH_LOGS="recompiles".
[rank1]:W1217 17:45:36.879000 37631 torch/_dynamo/convert_frame.py:915] [2/8] To diagnose recompilation issues, see https://pytorch.org/docs/main/torch.compiler_troubleshooting.html.
[rank3]:W1217 17:45:36.913000 37638 torch/_dynamo/convert_frame.py:915] [2/8] torch._dynamo hit config.cache_size_limit (8)
[rank3]:W1217 17:45:36.913000 37638 torch/_dynamo/convert_frame.py:915] [2/8]    function: 'torch_dynamo_resume_in__forward_prologue_at_2187' (/users/bzuidema/lsaie_deepspeed/deepspeed/runtime/engine.py:2187)
[rank3]:W1217 17:45:36.913000 37638 torch/_dynamo/convert_frame.py:915] [2/8]    last reason: 2/0: ___tuple_iterator_len(L['___stack0']) == 388                
[rank3]:W1217 17:45:36.913000 37638 torch/_dynamo/convert_frame.py:915] [2/8] To log all recompilation reasons, use TORCH_LOGS="recompiles".
[rank3]:W1217 17:45:36.913000 37638 torch/_dynamo/convert_frame.py:915] [2/8] To diagnose recompilation issues, see https://pytorch.org/docs/main/torch.compiler_troubleshooting.html.
[rank0]:W1217 17:45:36.954000 37628 torch/_dynamo/convert_frame.py:915] [2/8] torch._dynamo hit config.cache_size_limit (8)
[rank0]:W1217 17:45:36.954000 37628 torch/_dynamo/convert_frame.py:915] [2/8]    function: 'torch_dynamo_resume_in__forward_prologue_at_2187' (/users/bzuidema/lsaie_deepspeed/deepspeed/runtime/engine.py:2187)
[rank0]:W1217 17:45:36.954000 37628 torch/_dynamo/convert_frame.py:915] [2/8]    last reason: 2/0: ___tuple_iterator_len(L['___stack0']) == 388                
[rank0]:W1217 17:45:36.954000 37628 torch/_dynamo/convert_frame.py:915] [2/8] To log all recompilation reasons, use TORCH_LOGS="recompiles".
[rank0]:W1217 17:45:36.954000 37628 torch/_dynamo/convert_frame.py:915] [2/8] To diagnose recompilation issues, see https://pytorch.org/docs/main/torch.compiler_troubleshooting.html.
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/misc.py:651: UserWarning: The config.capture_autograd_function flag is deprecated, it's now always true.
  warnings.warn(
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/misc.py:651: UserWarning: The config.capture_autograd_function flag is deprecated, it's now always true.
  warnings.warn(
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/misc.py:651: UserWarning: The config.capture_autograd_function flag is deprecated, it's now always true.
  warnings.warn(
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/misc.py:651: UserWarning: The config.capture_autograd_function flag is deprecated, it's now always true.
  warnings.warn(
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/misc.py:651: UserWarning: The config.capture_autograd_function flag is deprecated, it's now always true.
  warnings.warn(
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/misc.py:651: UserWarning: The config.capture_autograd_function flag is deprecated, it's now always true.
  warnings.warn(
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/misc.py:651: UserWarning: The config.capture_autograd_function flag is deprecated, it's now always true.
  warnings.warn(
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/misc.py:651: UserWarning: The config.capture_autograd_function flag is deprecated, it's now always true.
  warnings.warn(
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/misc.py:651: UserWarning: The config.capture_autograd_function flag is deprecated, it's now always true.
  warnings.warn(
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/misc.py:651: UserWarning: The config.capture_autograd_function flag is deprecated, it's now always true.
  warnings.warn(
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/misc.py:651: UserWarning: The config.capture_autograd_function flag is deprecated, it's now always true.
  warnings.warn(
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/misc.py:651: UserWarning: The config.capture_autograd_function flag is deprecated, it's now always true.
  warnings.warn(
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/misc.py:651: UserWarning: The config.capture_autograd_function flag is deprecated, it's now always true.
  warnings.warn(
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/misc.py:651: UserWarning: The config.capture_autograd_function flag is deprecated, it's now always true.
  warnings.warn(
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/misc.py:651: UserWarning: The config.capture_autograd_function flag is deprecated, it's now always true.
  warnings.warn(
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/misc.py:651: UserWarning: The config.capture_autograd_function flag is deprecated, it's now always true.
  warnings.warn(
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/misc.py:651: UserWarning: The config.capture_autograd_function flag is deprecated, it's now always true.
  warnings.warn(
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/misc.py:651: UserWarning: The config.capture_autograd_function flag is deprecated, it's now always true.
  warnings.warn(
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/misc.py:651: UserWarning: The config.capture_autograd_function flag is deprecated, it's now always true.
  warnings.warn(
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/misc.py:651: UserWarning: The config.capture_autograd_function flag is deprecated, it's now always true.
  warnings.warn(
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/misc.py:651: UserWarning: The config.capture_autograd_function flag is deprecated, it's now always true.
  warnings.warn(
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/misc.py:651: UserWarning: The config.capture_autograd_function flag is deprecated, it's now always true.
  warnings.warn(
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/misc.py:651: UserWarning: The config.capture_autograd_function flag is deprecated, it's now always true.
  warnings.warn(
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/misc.py:651: UserWarning: The config.capture_autograd_function flag is deprecated, it's now always true.
  warnings.warn(
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/misc.py:651: UserWarning: The config.capture_autograd_function flag is deprecated, it's now always true.
  warnings.warn(
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/misc.py:651: UserWarning: The config.capture_autograd_function flag is deprecated, it's now always true.
  warnings.warn(
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/misc.py:651: UserWarning: The config.capture_autograd_function flag is deprecated, it's now always true.
  warnings.warn(
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/misc.py:651: UserWarning: The config.capture_autograd_function flag is deprecated, it's now always true.
  warnings.warn(
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/misc.py:651: UserWarning: The config.capture_autograd_function flag is deprecated, it's now always true.
  warnings.warn(
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/misc.py:651: UserWarning: The config.capture_autograd_function flag is deprecated, it's now always true.
  warnings.warn(
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/misc.py:651: UserWarning: The config.capture_autograd_function flag is deprecated, it's now always true.
  warnings.warn(
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/misc.py:651: UserWarning: The config.capture_autograd_function flag is deprecated, it's now always true.
  warnings.warn(
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/misc.py:651: UserWarning: The config.capture_autograd_function flag is deprecated, it's now always true.
  warnings.warn(
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/misc.py:651: UserWarning: The config.capture_autograd_function flag is deprecated, it's now always true.
  warnings.warn(
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/misc.py:651: UserWarning: The config.capture_autograd_function flag is deprecated, it's now always true.
  warnings.warn(
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/misc.py:651: UserWarning: The config.capture_autograd_function flag is deprecated, it's now always true.
  warnings.warn(
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/misc.py:651: UserWarning: The config.capture_autograd_function flag is deprecated, it's now always true.
  warnings.warn(
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/misc.py:651: UserWarning: The config.capture_autograd_function flag is deprecated, it's now always true.
  warnings.warn(
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/misc.py:651: UserWarning: The config.capture_autograd_function flag is deprecated, it's now always true.
  warnings.warn(
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/misc.py:651: UserWarning: The config.capture_autograd_function flag is deprecated, it's now always true.
  warnings.warn(
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/misc.py:651: UserWarning: The config.capture_autograd_function flag is deprecated, it's now always true.
  warnings.warn(
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/misc.py:651: UserWarning: The config.capture_autograd_function flag is deprecated, it's now always true.
  warnings.warn(
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/misc.py:651: UserWarning: The config.capture_autograd_function flag is deprecated, it's now always true.
  warnings.warn(
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/misc.py:651: UserWarning: The config.capture_autograd_function flag is deprecated, it's now always true.
  warnings.warn(
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/misc.py:651: UserWarning: The config.capture_autograd_function flag is deprecated, it's now always true.
  warnings.warn(
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/misc.py:651: UserWarning: The config.capture_autograd_function flag is deprecated, it's now always true.
  warnings.warn(
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/misc.py:651: UserWarning: The config.capture_autograd_function flag is deprecated, it's now always true.
  warnings.warn(
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/misc.py:651: UserWarning: The config.capture_autograd_function flag is deprecated, it's now always true.
  warnings.warn(
/usr/local/lib/python3.12/dist-packages/torch/_inductor/lowering.py:1782: UserWarning: Torchinductor does not support code generation for complex operators. Performance may be worse than eager.
  warnings.warn(
/usr/local/lib/python3.12/dist-packages/torch/_inductor/lowering.py:1782: UserWarning: Torchinductor does not support code generation for complex operators. Performance may be worse than eager.
  warnings.warn(
/usr/local/lib/python3.12/dist-packages/torch/_inductor/lowering.py:1782: UserWarning: Torchinductor does not support code generation for complex operators. Performance may be worse than eager.
  warnings.warn(
/usr/local/lib/python3.12/dist-packages/torch/_inductor/lowering.py:1782: UserWarning: Torchinductor does not support code generation for complex operators. Performance may be worse than eager.
  warnings.warn(
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/misc.py:651: UserWarning: The config.capture_autograd_function flag is deprecated, it's now always true.
  warnings.warn(
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/misc.py:651: UserWarning: The config.capture_autograd_function flag is deprecated, it's now always true.
  warnings.warn(
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/misc.py:651: UserWarning: The config.capture_autograd_function flag is deprecated, it's now always true.
  warnings.warn(
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/misc.py:651: UserWarning: The config.capture_autograd_function flag is deprecated, it's now always true.
  warnings.warn(
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/misc.py:651: UserWarning: The config.capture_autograd_function flag is deprecated, it's now always true.
  warnings.warn(
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/misc.py:651: UserWarning: The config.capture_autograd_function flag is deprecated, it's now always true.
  warnings.warn(
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/misc.py:651: UserWarning: The config.capture_autograd_function flag is deprecated, it's now always true.
  warnings.warn(
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/misc.py:651: UserWarning: The config.capture_autograd_function flag is deprecated, it's now always true.
  warnings.warn(
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/misc.py:651: UserWarning: The config.capture_autograd_function flag is deprecated, it's now always true.
  warnings.warn(
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/misc.py:651: UserWarning: The config.capture_autograd_function flag is deprecated, it's now always true.
  warnings.warn(
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/misc.py:651: UserWarning: The config.capture_autograd_function flag is deprecated, it's now always true.
  warnings.warn(
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/misc.py:651: UserWarning: The config.capture_autograd_function flag is deprecated, it's now always true.
  warnings.warn(
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/misc.py:651: UserWarning: The config.capture_autograd_function flag is deprecated, it's now always true.
  warnings.warn(
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/misc.py:651: UserWarning: The config.capture_autograd_function flag is deprecated, it's now always true.
  warnings.warn(
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/misc.py:651: UserWarning: The config.capture_autograd_function flag is deprecated, it's now always true.
  warnings.warn(
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/misc.py:651: UserWarning: The config.capture_autograd_function flag is deprecated, it's now always true.
  warnings.warn(
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/misc.py:651: UserWarning: The config.capture_autograd_function flag is deprecated, it's now always true.
  warnings.warn(
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/misc.py:651: UserWarning: The config.capture_autograd_function flag is deprecated, it's now always true.
  warnings.warn(
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/misc.py:651: UserWarning: The config.capture_autograd_function flag is deprecated, it's now always true.
  warnings.warn(
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/misc.py:651: UserWarning: The config.capture_autograd_function flag is deprecated, it's now always true.
  warnings.warn(
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/misc.py:651: UserWarning: The config.capture_autograd_function flag is deprecated, it's now always true.
  warnings.warn(
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/misc.py:651: UserWarning: The config.capture_autograd_function flag is deprecated, it's now always true.
  warnings.warn(
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/misc.py:651: UserWarning: The config.capture_autograd_function flag is deprecated, it's now always true.
  warnings.warn(
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/misc.py:651: UserWarning: The config.capture_autograd_function flag is deprecated, it's now always true.
  warnings.warn(
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/misc.py:651: UserWarning: The config.capture_autograd_function flag is deprecated, it's now always true.
  warnings.warn(
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/misc.py:651: UserWarning: The config.capture_autograd_function flag is deprecated, it's now always true.
  warnings.warn(
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/misc.py:651: UserWarning: The config.capture_autograd_function flag is deprecated, it's now always true.
  warnings.warn(
[rank2]:W1217 17:45:46.944000 37634 torch/_dynamo/convert_frame.py:915] [7/8] torch._dynamo hit config.cache_size_limit (8)
[rank2]:W1217 17:45:46.944000 37634 torch/_dynamo/convert_frame.py:915] [7/8]    function: 'post_sub_module_forward_function' (/users/bzuidema/lsaie_deepspeed/deepspeed/runtime/zero/parameter_offload.py:484)
[rank2]:W1217 17:45:46.944000 37634 torch/_dynamo/convert_frame.py:915] [7/8]    last reason: 7/0: ___check_type_id(L['sub_module'], 365966960)                
[rank2]:W1217 17:45:46.944000 37634 torch/_dynamo/convert_frame.py:915] [7/8] To log all recompilation reasons, use TORCH_LOGS="recompiles".
[rank2]:W1217 17:45:46.944000 37634 torch/_dynamo/convert_frame.py:915] [7/8] To diagnose recompilation issues, see https://pytorch.org/docs/main/torch.compiler_troubleshooting.html.
[rank2]:W1217 17:45:46.947000 37634 torch/_dynamo/convert_frame.py:915] [17/8] torch._dynamo hit config.cache_size_limit (8)
[rank2]:W1217 17:45:46.947000 37634 torch/_dynamo/convert_frame.py:915] [17/8]    function: '_pre_backward_module_hook' (/users/bzuidema/lsaie_deepspeed/deepspeed/runtime/zero/parameter_offload.py:350)
[rank2]:W1217 17:45:46.947000 37634 torch/_dynamo/convert_frame.py:915] [17/8]    last reason: 17/0: ___check_type_id(L['module'], 365966960)                    
[rank2]:W1217 17:45:46.947000 37634 torch/_dynamo/convert_frame.py:915] [17/8] To log all recompilation reasons, use TORCH_LOGS="recompiles".
[rank2]:W1217 17:45:46.947000 37634 torch/_dynamo/convert_frame.py:915] [17/8] To diagnose recompilation issues, see https://pytorch.org/docs/main/torch.compiler_troubleshooting.html.
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/misc.py:651: UserWarning: The config.capture_autograd_function flag is deprecated, it's now always true.
  warnings.warn(
[rank0]:W1217 17:45:47.068000 37628 torch/_dynamo/convert_frame.py:915] [7/8] torch._dynamo hit config.cache_size_limit (8)
[rank0]:W1217 17:45:47.068000 37628 torch/_dynamo/convert_frame.py:915] [7/8]    function: 'post_sub_module_forward_function' (/users/bzuidema/lsaie_deepspeed/deepspeed/runtime/zero/parameter_offload.py:484)
[rank0]:W1217 17:45:47.068000 37628 torch/_dynamo/convert_frame.py:915] [7/8]    last reason: 7/0: ___check_type_id(L['sub_module'], 1009268336)               
[rank0]:W1217 17:45:47.068000 37628 torch/_dynamo/convert_frame.py:915] [7/8] To log all recompilation reasons, use TORCH_LOGS="recompiles".
[rank0]:W1217 17:45:47.068000 37628 torch/_dynamo/convert_frame.py:915] [7/8] To diagnose recompilation issues, see https://pytorch.org/docs/main/torch.compiler_troubleshooting.html.
[rank0]:W1217 17:45:47.071000 37628 torch/_dynamo/convert_frame.py:915] [17/8] torch._dynamo hit config.cache_size_limit (8)
[rank0]:W1217 17:45:47.071000 37628 torch/_dynamo/convert_frame.py:915] [17/8]    function: '_pre_backward_module_hook' (/users/bzuidema/lsaie_deepspeed/deepspeed/runtime/zero/parameter_offload.py:350)
[rank0]:W1217 17:45:47.071000 37628 torch/_dynamo/convert_frame.py:915] [17/8]    last reason: 17/0: ___check_type_id(L['module'], 1009268336)                   
[rank0]:W1217 17:45:47.071000 37628 torch/_dynamo/convert_frame.py:915] [17/8] To log all recompilation reasons, use TORCH_LOGS="recompiles".
[rank0]:W1217 17:45:47.071000 37628 torch/_dynamo/convert_frame.py:915] [17/8] To diagnose recompilation issues, see https://pytorch.org/docs/main/torch.compiler_troubleshooting.html.
[rank3]:W1217 17:45:47.102000 37638 torch/_dynamo/convert_frame.py:915] [7/8] torch._dynamo hit config.cache_size_limit (8)
[rank3]:W1217 17:45:47.102000 37638 torch/_dynamo/convert_frame.py:915] [7/8]    function: 'post_sub_module_forward_function' (/users/bzuidema/lsaie_deepspeed/deepspeed/runtime/zero/parameter_offload.py:484)
[rank3]:W1217 17:45:47.102000 37638 torch/_dynamo/convert_frame.py:915] [7/8]    last reason: 7/0: ___check_type_id(L['sub_module'], 794441328)                
[rank3]:W1217 17:45:47.102000 37638 torch/_dynamo/convert_frame.py:915] [7/8] To log all recompilation reasons, use TORCH_LOGS="recompiles".
[rank3]:W1217 17:45:47.102000 37638 torch/_dynamo/convert_frame.py:915] [7/8] To diagnose recompilation issues, see https://pytorch.org/docs/main/torch.compiler_troubleshooting.html.
[rank3]:W1217 17:45:47.105000 37638 torch/_dynamo/convert_frame.py:915] [17/8] torch._dynamo hit config.cache_size_limit (8)
[rank3]:W1217 17:45:47.105000 37638 torch/_dynamo/convert_frame.py:915] [17/8]    function: '_pre_backward_module_hook' (/users/bzuidema/lsaie_deepspeed/deepspeed/runtime/zero/parameter_offload.py:350)
[rank3]:W1217 17:45:47.105000 37638 torch/_dynamo/convert_frame.py:915] [17/8]    last reason: 17/0: ___check_type_id(L['module'], 794441328)                    
[rank3]:W1217 17:45:47.105000 37638 torch/_dynamo/convert_frame.py:915] [17/8] To log all recompilation reasons, use TORCH_LOGS="recompiles".
[rank3]:W1217 17:45:47.105000 37638 torch/_dynamo/convert_frame.py:915] [17/8] To diagnose recompilation issues, see https://pytorch.org/docs/main/torch.compiler_troubleshooting.html.
[rank1]:W1217 17:45:47.106000 37631 torch/_dynamo/convert_frame.py:915] [7/8] torch._dynamo hit config.cache_size_limit (8)
[rank1]:W1217 17:45:47.106000 37631 torch/_dynamo/convert_frame.py:915] [7/8]    function: 'post_sub_module_forward_function' (/users/bzuidema/lsaie_deepspeed/deepspeed/runtime/zero/parameter_offload.py:484)
[rank1]:W1217 17:45:47.106000 37631 torch/_dynamo/convert_frame.py:915] [7/8]    last reason: 7/0: ___check_type_id(L['sub_module'], 1105540720)               
[rank1]:W1217 17:45:47.106000 37631 torch/_dynamo/convert_frame.py:915] [7/8] To log all recompilation reasons, use TORCH_LOGS="recompiles".
[rank1]:W1217 17:45:47.106000 37631 torch/_dynamo/convert_frame.py:915] [7/8] To diagnose recompilation issues, see https://pytorch.org/docs/main/torch.compiler_troubleshooting.html.
[rank1]:W1217 17:45:47.109000 37631 torch/_dynamo/convert_frame.py:915] [17/8] torch._dynamo hit config.cache_size_limit (8)
[rank1]:W1217 17:45:47.109000 37631 torch/_dynamo/convert_frame.py:915] [17/8]    function: '_pre_backward_module_hook' (/users/bzuidema/lsaie_deepspeed/deepspeed/runtime/zero/parameter_offload.py:350)
[rank1]:W1217 17:45:47.109000 37631 torch/_dynamo/convert_frame.py:915] [17/8]    last reason: 17/0: ___check_type_id(L['module'], 1105540720)                   
[rank1]:W1217 17:45:47.109000 37631 torch/_dynamo/convert_frame.py:915] [17/8] To log all recompilation reasons, use TORCH_LOGS="recompiles".
[rank1]:W1217 17:45:47.109000 37631 torch/_dynamo/convert_frame.py:915] [17/8] To diagnose recompilation issues, see https://pytorch.org/docs/main/torch.compiler_troubleshooting.html.
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/misc.py:651: UserWarning: The config.capture_autograd_function flag is deprecated, it's now always true.
  warnings.warn(
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/misc.py:651: UserWarning: The config.capture_autograd_function flag is deprecated, it's now always true.
  warnings.warn(
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/misc.py:651: UserWarning: The config.capture_autograd_function flag is deprecated, it's now always true.
  warnings.warn(
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/misc.py:651: UserWarning: The config.capture_autograd_function flag is deprecated, it's now always true.
  warnings.warn(
[rank0]:W1217 17:45:48.274000 37628 torch/_dynamo/convert_frame.py:915] [6/8] torch._dynamo hit config.cache_size_limit (8)
[rank0]:W1217 17:45:48.274000 37628 torch/_dynamo/convert_frame.py:915] [6/8]    function: '_post_forward_module_hook' (/users/bzuidema/lsaie_deepspeed/deepspeed/runtime/zero/parameter_offload.py:302)
[rank0]:W1217 17:45:48.274000 37628 torch/_dynamo/convert_frame.py:915] [6/8]    last reason: 6/0: ___check_type_id(L['module'], 1009268336)                   
[rank0]:W1217 17:45:48.274000 37628 torch/_dynamo/convert_frame.py:915] [6/8] To log all recompilation reasons, use TORCH_LOGS="recompiles".
[rank0]:W1217 17:45:48.274000 37628 torch/_dynamo/convert_frame.py:915] [6/8] To diagnose recompilation issues, see https://pytorch.org/docs/main/torch.compiler_troubleshooting.html.
[rank2]:W1217 17:45:48.274000 37634 torch/_dynamo/convert_frame.py:915] [6/8] torch._dynamo hit config.cache_size_limit (8)
[rank2]:W1217 17:45:48.274000 37634 torch/_dynamo/convert_frame.py:915] [6/8]    function: '_post_forward_module_hook' (/users/bzuidema/lsaie_deepspeed/deepspeed/runtime/zero/parameter_offload.py:302)
[rank2]:W1217 17:45:48.274000 37634 torch/_dynamo/convert_frame.py:915] [6/8]    last reason: 6/0: ___check_type_id(L['module'], 365966960)                    
[rank2]:W1217 17:45:48.274000 37634 torch/_dynamo/convert_frame.py:915] [6/8] To log all recompilation reasons, use TORCH_LOGS="recompiles".
[rank2]:W1217 17:45:48.274000 37634 torch/_dynamo/convert_frame.py:915] [6/8] To diagnose recompilation issues, see https://pytorch.org/docs/main/torch.compiler_troubleshooting.html.
[rank1]:W1217 17:45:48.274000 37631 torch/_dynamo/convert_frame.py:915] [6/8] torch._dynamo hit config.cache_size_limit (8)
[rank1]:W1217 17:45:48.274000 37631 torch/_dynamo/convert_frame.py:915] [6/8]    function: '_post_forward_module_hook' (/users/bzuidema/lsaie_deepspeed/deepspeed/runtime/zero/parameter_offload.py:302)
[rank1]:W1217 17:45:48.274000 37631 torch/_dynamo/convert_frame.py:915] [6/8]    last reason: 6/0: ___check_type_id(L['module'], 1105540720)                   
[rank1]:W1217 17:45:48.274000 37631 torch/_dynamo/convert_frame.py:915] [6/8] To log all recompilation reasons, use TORCH_LOGS="recompiles".
[rank1]:W1217 17:45:48.274000 37631 torch/_dynamo/convert_frame.py:915] [6/8] To diagnose recompilation issues, see https://pytorch.org/docs/main/torch.compiler_troubleshooting.html.
[rank3]:W1217 17:45:48.277000 37638 torch/_dynamo/convert_frame.py:915] [6/8] torch._dynamo hit config.cache_size_limit (8)
[rank3]:W1217 17:45:48.277000 37638 torch/_dynamo/convert_frame.py:915] [6/8]    function: '_post_forward_module_hook' (/users/bzuidema/lsaie_deepspeed/deepspeed/runtime/zero/parameter_offload.py:302)
[rank3]:W1217 17:45:48.277000 37638 torch/_dynamo/convert_frame.py:915] [6/8]    last reason: 6/0: ___check_type_id(L['module'], 794441328)                    
[rank3]:W1217 17:45:48.277000 37638 torch/_dynamo/convert_frame.py:915] [6/8] To log all recompilation reasons, use TORCH_LOGS="recompiles".
[rank3]:W1217 17:45:48.277000 37638 torch/_dynamo/convert_frame.py:915] [6/8] To diagnose recompilation issues, see https://pytorch.org/docs/main/torch.compiler_troubleshooting.html.
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/misc.py:651: UserWarning: The config.capture_autograd_function flag is deprecated, it's now always true.
  warnings.warn(
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/misc.py:651: UserWarning: The config.capture_autograd_function flag is deprecated, it's now always true.
  warnings.warn(
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/misc.py:651: UserWarning: The config.capture_autograd_function flag is deprecated, it's now always true.
  warnings.warn(
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/misc.py:651: UserWarning: The config.capture_autograd_function flag is deprecated, it's now always true.
  warnings.warn(
[rank2]:W1217 17:45:51.223000 37634 torch/_dynamo/convert_frame.py:915] [39/8] torch._dynamo hit config.cache_size_limit (8)
[rank2]:W1217 17:45:51.223000 37634 torch/_dynamo/convert_frame.py:915] [39/8]    function: 'torch_dynamo_resume_in__forward_epilogue_at_2200' (/users/bzuidema/lsaie_deepspeed/deepspeed/runtime/engine.py:2200)
[rank2]:W1217 17:45:51.223000 37634 torch/_dynamo/convert_frame.py:915] [39/8]    last reason: 39/0: ___tuple_iterator_len(L['___stack0']) == 388                
[rank2]:W1217 17:45:51.223000 37634 torch/_dynamo/convert_frame.py:915] [39/8] To log all recompilation reasons, use TORCH_LOGS="recompiles".
[rank2]:W1217 17:45:51.223000 37634 torch/_dynamo/convert_frame.py:915] [39/8] To diagnose recompilation issues, see https://pytorch.org/docs/main/torch.compiler_troubleshooting.html.
[rank0]:W1217 17:45:51.247000 37628 torch/_dynamo/convert_frame.py:915] [39/8] torch._dynamo hit config.cache_size_limit (8)
[rank0]:W1217 17:45:51.247000 37628 torch/_dynamo/convert_frame.py:915] [39/8]    function: 'torch_dynamo_resume_in__forward_epilogue_at_2200' (/users/bzuidema/lsaie_deepspeed/deepspeed/runtime/engine.py:2200)
[rank0]:W1217 17:45:51.247000 37628 torch/_dynamo/convert_frame.py:915] [39/8]    last reason: 39/0: ___tuple_iterator_len(L['___stack0']) == 388                
[rank0]:W1217 17:45:51.247000 37628 torch/_dynamo/convert_frame.py:915] [39/8] To log all recompilation reasons, use TORCH_LOGS="recompiles".
[rank0]:W1217 17:45:51.247000 37628 torch/_dynamo/convert_frame.py:915] [39/8] To diagnose recompilation issues, see https://pytorch.org/docs/main/torch.compiler_troubleshooting.html.
[rank1]:W1217 17:45:51.250000 37631 torch/_dynamo/convert_frame.py:915] [39/8] torch._dynamo hit config.cache_size_limit (8)
[rank1]:W1217 17:45:51.250000 37631 torch/_dynamo/convert_frame.py:915] [39/8]    function: 'torch_dynamo_resume_in__forward_epilogue_at_2200' (/users/bzuidema/lsaie_deepspeed/deepspeed/runtime/engine.py:2200)
[rank1]:W1217 17:45:51.250000 37631 torch/_dynamo/convert_frame.py:915] [39/8]    last reason: 39/0: ___tuple_iterator_len(L['___stack0']) == 388                
[rank1]:W1217 17:45:51.250000 37631 torch/_dynamo/convert_frame.py:915] [39/8] To log all recompilation reasons, use TORCH_LOGS="recompiles".
[rank1]:W1217 17:45:51.250000 37631 torch/_dynamo/convert_frame.py:915] [39/8] To diagnose recompilation issues, see https://pytorch.org/docs/main/torch.compiler_troubleshooting.html.
[rank3]:W1217 17:45:51.277000 37638 torch/_dynamo/convert_frame.py:915] [39/8] torch._dynamo hit config.cache_size_limit (8)
[rank3]:W1217 17:45:51.277000 37638 torch/_dynamo/convert_frame.py:915] [39/8]    function: 'torch_dynamo_resume_in__forward_epilogue_at_2200' (/users/bzuidema/lsaie_deepspeed/deepspeed/runtime/engine.py:2200)
[rank3]:W1217 17:45:51.277000 37638 torch/_dynamo/convert_frame.py:915] [39/8]    last reason: 39/0: ___tuple_iterator_len(L['___stack0']) == 388                
[rank3]:W1217 17:45:51.277000 37638 torch/_dynamo/convert_frame.py:915] [39/8] To log all recompilation reasons, use TORCH_LOGS="recompiles".
[rank3]:W1217 17:45:51.277000 37638 torch/_dynamo/convert_frame.py:915] [39/8] To diagnose recompilation issues, see https://pytorch.org/docs/main/torch.compiler_troubleshooting.html.
2025-12-17 17:45:57,670 - root - INFO - Step: 1 | Loss: 11.93 | Tokens per second: 348.50 | Training tokens per second (%): 28.12 | MFU (%): 0.45 | TFLOPs: 4.49 | Mem Allocated (GB): 33.28 | Mem Reserved (GB): 58.11 | Max Mem Allocated (GB): 37.31
2025-12-17 17:45:57,670 - root - INFO - Step: 1 | Loss: 11.98 | Tokens per second: 350.02 | Training tokens per second (%): 8.84 | MFU (%): 0.46 | TFLOPs: 4.51 | Mem Allocated (GB): 33.28 | Mem Reserved (GB): 58.11 | Max Mem Allocated (GB): 37.31
2025-12-17 17:45:57,670 - root - INFO - Step: 1 | Loss: 11.79 | Tokens per second: 348.46 | Training tokens per second (%): 6.40 | MFU (%): 0.45 | TFLOPs: 4.49 | Mem Allocated (GB): 33.28 | Mem Reserved (GB): 58.11 | Max Mem Allocated (GB): 37.31
2025-12-17 17:45:57,670 - root - INFO - Step: 1 | Loss: 11.91 | Tokens per second: 348.51 | Training tokens per second (%): 35.25 | MFU (%): 0.45 | TFLOPs: 4.49 | Mem Allocated (GB): 33.28 | Mem Reserved (GB): 58.11 | Max Mem Allocated (GB): 37.31
[rank1]:W1217 17:45:59.437000 37631 torch/_dynamo/convert_frame.py:915] [18/8] torch._dynamo hit config.cache_size_limit (8)
[rank1]:W1217 17:45:59.437000 37631 torch/_dynamo/convert_frame.py:915] [18/8]    function: 'apply_to_tensors_only' (/users/bzuidema/lsaie_deepspeed/deepspeed/runtime/zero/utils.py:146)
[rank1]:W1217 17:45:59.437000 37631 torch/_dynamo/convert_frame.py:915] [18/8]    last reason: 18/7: Cache line invalidated because L['function'] got deallocated
[rank1]:W1217 17:45:59.437000 37631 torch/_dynamo/convert_frame.py:915] [18/8] To log all recompilation reasons, use TORCH_LOGS="recompiles".
[rank1]:W1217 17:45:59.437000 37631 torch/_dynamo/convert_frame.py:915] [18/8] To diagnose recompilation issues, see https://pytorch.org/docs/main/torch.compiler_troubleshooting.html.
[rank3]:W1217 17:45:59.438000 37638 torch/_dynamo/convert_frame.py:915] [18/8] torch._dynamo hit config.cache_size_limit (8)
[rank3]:W1217 17:45:59.438000 37638 torch/_dynamo/convert_frame.py:915] [18/8]    function: 'apply_to_tensors_only' (/users/bzuidema/lsaie_deepspeed/deepspeed/runtime/zero/utils.py:146)
[rank3]:W1217 17:45:59.438000 37638 torch/_dynamo/convert_frame.py:915] [18/8]    last reason: 18/7: Cache line invalidated because L['function'] got deallocated
[rank3]:W1217 17:45:59.438000 37638 torch/_dynamo/convert_frame.py:915] [18/8] To log all recompilation reasons, use TORCH_LOGS="recompiles".
[rank3]:W1217 17:45:59.438000 37638 torch/_dynamo/convert_frame.py:915] [18/8] To diagnose recompilation issues, see https://pytorch.org/docs/main/torch.compiler_troubleshooting.html.
[rank0]:W1217 17:45:59.438000 37628 torch/_dynamo/convert_frame.py:915] [18/8] torch._dynamo hit config.cache_size_limit (8)
[rank0]:W1217 17:45:59.438000 37628 torch/_dynamo/convert_frame.py:915] [18/8]    function: 'apply_to_tensors_only' (/users/bzuidema/lsaie_deepspeed/deepspeed/runtime/zero/utils.py:146)
[rank0]:W1217 17:45:59.438000 37628 torch/_dynamo/convert_frame.py:915] [18/8]    last reason: 18/7: Cache line invalidated because L['function'] got deallocated
[rank0]:W1217 17:45:59.438000 37628 torch/_dynamo/convert_frame.py:915] [18/8] To log all recompilation reasons, use TORCH_LOGS="recompiles".
[rank0]:W1217 17:45:59.438000 37628 torch/_dynamo/convert_frame.py:915] [18/8] To diagnose recompilation issues, see https://pytorch.org/docs/main/torch.compiler_troubleshooting.html.
[rank2]:W1217 17:45:59.438000 37634 torch/_dynamo/convert_frame.py:915] [18/8] torch._dynamo hit config.cache_size_limit (8)
[rank2]:W1217 17:45:59.438000 37634 torch/_dynamo/convert_frame.py:915] [18/8]    function: 'apply_to_tensors_only' (/users/bzuidema/lsaie_deepspeed/deepspeed/runtime/zero/utils.py:146)
[rank2]:W1217 17:45:59.438000 37634 torch/_dynamo/convert_frame.py:915] [18/8]    last reason: 18/7: Cache line invalidated because L['function'] got deallocated
[rank2]:W1217 17:45:59.438000 37634 torch/_dynamo/convert_frame.py:915] [18/8] To log all recompilation reasons, use TORCH_LOGS="recompiles".
[rank2]:W1217 17:45:59.438000 37634 torch/_dynamo/convert_frame.py:915] [18/8] To diagnose recompilation issues, see https://pytorch.org/docs/main/torch.compiler_troubleshooting.html.
2025-12-17 17:46:02,112 - root - INFO - Step: 5 | Loss: 11.81 | Tokens per second: 7377.89 | Training tokens per second (%): 27.73 | MFU (%): 9.61 | TFLOPs: 95.07 | Mem Allocated (GB): 33.28 | Mem Reserved (GB): 59.70 | Max Mem Allocated (GB): 50.50
2025-12-17 17:46:02,112 - root - INFO - Step: 5 | Loss: 11.82 | Tokens per second: 7377.88 | Training tokens per second (%): 60.00 | MFU (%): 9.61 | TFLOPs: 95.07 | Mem Allocated (GB): 33.28 | Mem Reserved (GB): 60.69 | Max Mem Allocated (GB): 50.50
2025-12-17 17:46:02,112 - root - INFO - Step: 5 | Loss: 11.84 | Tokens per second: 7377.88 | Training tokens per second (%): 49.78 | MFU (%): 9.61 | TFLOPs: 95.07 | Mem Allocated (GB): 33.28 | Mem Reserved (GB): 60.57 | Max Mem Allocated (GB): 50.50
2025-12-17 17:46:02,112 - root - INFO - Step: 5 | Loss: 11.79 | Tokens per second: 7377.87 | Training tokens per second (%): 39.77 | MFU (%): 9.61 | TFLOPs: 95.07 | Mem Allocated (GB): 33.28 | Mem Reserved (GB): 59.82 | Max Mem Allocated (GB): 50.50
2025-12-17 17:46:04,874 - root - INFO - Step: 10 | Loss: 9.92 | Tokens per second: 14835.94 | Training tokens per second (%): 40.84 | MFU (%): 19.33 | TFLOPs: 191.17 | Mem Allocated (GB): 33.28 | Mem Reserved (GB): 60.90 | Max Mem Allocated (GB): 50.50
2025-12-17 17:46:04,874 - root - INFO - Step: 10 | Loss: 10.61 | Tokens per second: 14835.89 | Training tokens per second (%): 27.79 | MFU (%): 19.33 | TFLOPs: 191.16 | Mem Allocated (GB): 33.28 | Mem Reserved (GB): 60.69 | Max Mem Allocated (GB): 50.50
2025-12-17 17:46:04,874 - root - INFO - Step: 10 | Loss: 10.24 | Tokens per second: 14835.94 | Training tokens per second (%): 30.87 | MFU (%): 19.33 | TFLOPs: 191.17 | Mem Allocated (GB): 33.28 | Mem Reserved (GB): 60.57 | Max Mem Allocated (GB): 50.50
2025-12-17 17:46:04,874 - root - INFO - Step: 10 | Loss: 10.35 | Tokens per second: 14835.87 | Training tokens per second (%): 39.41 | MFU (%): 19.33 | TFLOPs: 191.16 | Mem Allocated (GB): 33.28 | Mem Reserved (GB): 59.70 | Max Mem Allocated (GB): 50.50
2025-12-17 17:46:07,720 - root - INFO - Step: 15 | Loss: 9.38 | Tokens per second: 14394.52 | Training tokens per second (%): 31.92 | MFU (%): 18.75 | TFLOPs: 185.48 | Mem Allocated (GB): 33.28 | Mem Reserved (GB): 60.90 | Max Mem Allocated (GB): 50.50
2025-12-17 17:46:07,720 - root - INFO - Step: 15 | Loss: 8.98 | Tokens per second: 14394.50 | Training tokens per second (%): 27.36 | MFU (%): 18.75 | TFLOPs: 185.48 | Mem Allocated (GB): 33.28 | Mem Reserved (GB): 60.57 | Max Mem Allocated (GB): 50.50
2025-12-17 17:46:07,720 - root - INFO - Step: 15 | Loss: 10.10 | Tokens per second: 14394.52 | Training tokens per second (%): 52.16 | MFU (%): 18.75 | TFLOPs: 185.48 | Mem Allocated (GB): 33.28 | Mem Reserved (GB): 60.69 | Max Mem Allocated (GB): 50.50
2025-12-17 17:46:07,720 - root - INFO - Step: 15 | Loss: 9.61 | Tokens per second: 14394.26 | Training tokens per second (%): 25.31 | MFU (%): 18.75 | TFLOPs: 185.47 | Mem Allocated (GB): 33.28 | Mem Reserved (GB): 59.70 | Max Mem Allocated (GB): 50.50
2025-12-17 17:46:10,498 - root - INFO - Step: 20 | Loss: 9.52 | Tokens per second: 14745.72 | Training tokens per second (%): 36.23 | MFU (%): 19.21 | TFLOPs: 190.00 | Mem Allocated (GB): 33.28 | Mem Reserved (GB): 59.70 | Max Mem Allocated (GB): 50.50
2025-12-17 17:46:10,498 - root - INFO - Step: 20 | Loss: 8.43 | Tokens per second: 14745.65 | Training tokens per second (%): 20.62 | MFU (%): 19.21 | TFLOPs: 190.00 | Mem Allocated (GB): 33.28 | Mem Reserved (GB): 60.57 | Max Mem Allocated (GB): 50.50
2025-12-17 17:46:10,498 - root - INFO - Step: 20 | Loss: 9.56 | Tokens per second: 14745.64 | Training tokens per second (%): 40.03 | MFU (%): 19.21 | TFLOPs: 190.00 | Mem Allocated (GB): 33.28 | Mem Reserved (GB): 60.90 | Max Mem Allocated (GB): 50.50
2025-12-17 17:46:10,498 - root - INFO - Step: 20 | Loss: 8.60 | Tokens per second: 14745.68 | Training tokens per second (%): 30.31 | MFU (%): 19.21 | TFLOPs: 190.00 | Mem Allocated (GB): 33.28 | Mem Reserved (GB): 60.69 | Max Mem Allocated (GB): 50.50
2025-12-17 17:46:13,273 - root - INFO - Step: 25 | Loss: 9.28 | Tokens per second: 14769.03 | Training tokens per second (%): 25.36 | MFU (%): 19.24 | TFLOPs: 190.30 | Mem Allocated (GB): 33.28 | Mem Reserved (GB): 59.70 | Max Mem Allocated (GB): 50.50
2025-12-17 17:46:13,273 - root - INFO - Step: 25 | Loss: 9.06 | Tokens per second: 14769.06 | Training tokens per second (%): 26.37 | MFU (%): 19.24 | TFLOPs: 190.30 | Mem Allocated (GB): 33.28 | Mem Reserved (GB): 60.57 | Max Mem Allocated (GB): 50.50
2025-12-17 17:46:13,273 - root - INFO - Step: 25 | Loss: 8.75 | Tokens per second: 14769.07 | Training tokens per second (%): 30.03 | MFU (%): 19.24 | TFLOPs: 190.30 | Mem Allocated (GB): 33.28 | Mem Reserved (GB): 60.90 | Max Mem Allocated (GB): 50.50
2025-12-17 17:46:13,273 - root - INFO - Step: 25 | Loss: 9.65 | Tokens per second: 14769.06 | Training tokens per second (%): 17.13 | MFU (%): 19.24 | TFLOPs: 190.30 | Mem Allocated (GB): 33.28 | Mem Reserved (GB): 60.69 | Max Mem Allocated (GB): 50.50
2025-12-17 17:46:16,045 - root - INFO - Step: 30 | Loss: 8.94 | Tokens per second: 14778.00 | Training tokens per second (%): 38.74 | MFU (%): 19.25 | TFLOPs: 190.42 | Mem Allocated (GB): 33.28 | Mem Reserved (GB): 59.91 | Max Mem Allocated (GB): 50.50
2025-12-17 17:46:16,045 - root - INFO - Step: 30 | Loss: 8.87 | Tokens per second: 14778.07 | Training tokens per second (%): 34.33 | MFU (%): 19.25 | TFLOPs: 190.42 | Mem Allocated (GB): 33.28 | Mem Reserved (GB): 60.69 | Max Mem Allocated (GB): 50.50
2025-12-17 17:46:16,045 - root - INFO - Step: 30 | Loss: 9.29 | Tokens per second: 14778.01 | Training tokens per second (%): 46.38 | MFU (%): 19.25 | TFLOPs: 190.42 | Mem Allocated (GB): 33.28 | Mem Reserved (GB): 60.57 | Max Mem Allocated (GB): 50.50
2025-12-17 17:46:16,045 - root - INFO - Step: 30 | Loss: 9.19 | Tokens per second: 14778.05 | Training tokens per second (%): 34.40 | MFU (%): 19.25 | TFLOPs: 190.42 | Mem Allocated (GB): 33.28 | Mem Reserved (GB): 60.90 | Max Mem Allocated (GB): 50.50
2025-12-17 17:46:18,821 - root - INFO - Step: 35 | Loss: 8.94 | Tokens per second: 14761.03 | Training tokens per second (%): 30.36 | MFU (%): 19.23 | TFLOPs: 190.20 | Mem Allocated (GB): 33.28 | Mem Reserved (GB): 60.69 | Max Mem Allocated (GB): 50.50
2025-12-17 17:46:18,821 - root - INFO - Step: 35 | Loss: 8.52 | Tokens per second: 14761.09 | Training tokens per second (%): 29.41 | MFU (%): 19.23 | TFLOPs: 190.20 | Mem Allocated (GB): 33.28 | Mem Reserved (GB): 60.90 | Max Mem Allocated (GB): 50.50
2025-12-17 17:46:18,821 - root - INFO - Step: 35 | Loss: 8.72 | Tokens per second: 14761.06 | Training tokens per second (%): 39.22 | MFU (%): 19.23 | TFLOPs: 190.20 | Mem Allocated (GB): 33.28 | Mem Reserved (GB): 60.57 | Max Mem Allocated (GB): 50.50
2025-12-17 17:46:18,821 - root - INFO - Step: 35 | Loss: 8.11 | Tokens per second: 14761.01 | Training tokens per second (%): 42.95 | MFU (%): 19.23 | TFLOPs: 190.20 | Mem Allocated (GB): 33.28 | Mem Reserved (GB): 59.91 | Max Mem Allocated (GB): 50.50
2025-12-17 17:46:21,599 - root - INFO - Step: 40 | Loss: 8.46 | Tokens per second: 14748.53 | Training tokens per second (%): 22.34 | MFU (%): 19.22 | TFLOPs: 190.04 | Mem Allocated (GB): 33.28 | Mem Reserved (GB): 60.90 | Max Mem Allocated (GB): 50.50
2025-12-17 17:46:21,599 - root - INFO - Step: 40 | Loss: 8.71 | Tokens per second: 14748.55 | Training tokens per second (%): 29.43 | MFU (%): 19.22 | TFLOPs: 190.04 | Mem Allocated (GB): 33.28 | Mem Reserved (GB): 60.57 | Max Mem Allocated (GB): 50.50
2025-12-17 17:46:21,599 - root - INFO - Step: 40 | Loss: 8.65 | Tokens per second: 14748.54 | Training tokens per second (%): 46.49 | MFU (%): 19.22 | TFLOPs: 190.04 | Mem Allocated (GB): 33.28 | Mem Reserved (GB): 59.91 | Max Mem Allocated (GB): 50.50
2025-12-17 17:46:21,599 - root - INFO - Step: 40 | Loss: 8.76 | Tokens per second: 14748.50 | Training tokens per second (%): 33.12 | MFU (%): 19.22 | TFLOPs: 190.04 | Mem Allocated (GB): 33.28 | Mem Reserved (GB): 60.69 | Max Mem Allocated (GB): 50.50
2025-12-17 17:46:24,374 - root - INFO - Step: 45 | Loss: 8.30 | Tokens per second: 14764.21 | Training tokens per second (%): 18.80 | MFU (%): 19.24 | TFLOPs: 190.24 | Mem Allocated (GB): 33.28 | Mem Reserved (GB): 60.69 | Max Mem Allocated (GB): 50.50
2025-12-17 17:46:24,374 - root - INFO - Step: 45 | Loss: 8.66 | Tokens per second: 14764.21 | Training tokens per second (%): 47.55 | MFU (%): 19.24 | TFLOPs: 190.24 | Mem Allocated (GB): 33.28 | Mem Reserved (GB): 60.57 | Max Mem Allocated (GB): 50.50
2025-12-17 17:46:24,374 - root - INFO - Step: 45 | Loss: 8.20 | Tokens per second: 14764.19 | Training tokens per second (%): 53.08 | MFU (%): 19.24 | TFLOPs: 190.24 | Mem Allocated (GB): 33.28 | Mem Reserved (GB): 60.90 | Max Mem Allocated (GB): 50.50
2025-12-17 17:46:24,374 - root - INFO - Step: 45 | Loss: 7.95 | Tokens per second: 14764.22 | Training tokens per second (%): 59.87 | MFU (%): 19.24 | TFLOPs: 190.24 | Mem Allocated (GB): 33.28 | Mem Reserved (GB): 59.91 | Max Mem Allocated (GB): 50.50
2025-12-17 17:46:27,328 - root - INFO - Step: 50 | Loss: 8.09 | Tokens per second: 13868.37 | Training tokens per second (%): 50.39 | MFU (%): 18.07 | TFLOPs: 178.70 | Mem Allocated (GB): 33.28 | Mem Reserved (GB): 60.90 | Max Mem Allocated (GB): 50.50
2025-12-17 17:46:27,328 - root - INFO - Step: 50 | Loss: 8.00 | Tokens per second: 13868.37 | Training tokens per second (%): 56.77 | MFU (%): 18.07 | TFLOPs: 178.70 | Mem Allocated (GB): 33.28 | Mem Reserved (GB): 60.57 | Max Mem Allocated (GB): 50.50
2025-12-17 17:46:27,328 - root - INFO - Step: 50 | Loss: 7.93 | Tokens per second: 13868.35 | Training tokens per second (%): 20.11 | MFU (%): 18.07 | TFLOPs: 178.70 | Mem Allocated (GB): 33.28 | Mem Reserved (GB): 60.69 | Max Mem Allocated (GB): 50.50
2025-12-17 17:46:27,328 - root - INFO - Step: 50 | Loss: 8.20 | Tokens per second: 13868.38 | Training tokens per second (%): 37.09 | MFU (%): 18.07 | TFLOPs: 178.70 | Mem Allocated (GB): 33.28 | Mem Reserved (GB): 59.91 | Max Mem Allocated (GB): 50.50
2025-12-17 17:46:30,311 - root - INFO - Step: 55 | Loss: 8.16 | Tokens per second: 13734.27 | Training tokens per second (%): 28.41 | MFU (%): 17.89 | TFLOPs: 176.97 | Mem Allocated (GB): 33.28 | Mem Reserved (GB): 60.98 | Max Mem Allocated (GB): 50.50
2025-12-17 17:46:30,311 - root - INFO - Step: 55 | Loss: 8.17 | Tokens per second: 13734.27 | Training tokens per second (%): 42.23 | MFU (%): 17.89 | TFLOPs: 176.97 | Mem Allocated (GB): 33.28 | Mem Reserved (GB): 60.69 | Max Mem Allocated (GB): 50.50
2025-12-17 17:46:30,311 - root - INFO - Step: 55 | Loss: 8.81 | Tokens per second: 13734.24 | Training tokens per second (%): 43.58 | MFU (%): 17.89 | TFLOPs: 176.97 | Mem Allocated (GB): 33.28 | Mem Reserved (GB): 60.57 | Max Mem Allocated (GB): 50.50
2025-12-17 17:46:30,311 - root - INFO - Step: 55 | Loss: 8.02 | Tokens per second: 13734.24 | Training tokens per second (%): 46.30 | MFU (%): 17.89 | TFLOPs: 176.97 | Mem Allocated (GB): 33.28 | Mem Reserved (GB): 60.90 | Max Mem Allocated (GB): 50.50
2025-12-17 17:46:33,306 - root - INFO - Step: 60 | Loss: 8.11 | Tokens per second: 13677.06 | Training tokens per second (%): 42.74 | MFU (%): 17.82 | TFLOPs: 176.23 | Mem Allocated (GB): 33.28 | Mem Reserved (GB): 60.69 | Max Mem Allocated (GB): 50.50
2025-12-17 17:46:33,306 - root - INFO - Step: 60 | Loss: 7.74 | Tokens per second: 13677.15 | Training tokens per second (%): 49.20 | MFU (%): 17.82 | TFLOPs: 176.23 | Mem Allocated (GB): 33.28 | Mem Reserved (GB): 60.90 | Max Mem Allocated (GB): 50.50
2025-12-17 17:46:33,306 - root - INFO - Step: 60 | Loss: 8.12 | Tokens per second: 13677.15 | Training tokens per second (%): 60.13 | MFU (%): 17.82 | TFLOPs: 176.23 | Mem Allocated (GB): 33.28 | Mem Reserved (GB): 60.57 | Max Mem Allocated (GB): 50.50
2025-12-17 17:46:33,306 - root - INFO - Step: 60 | Loss: 7.65 | Tokens per second: 13677.04 | Training tokens per second (%): 35.04 | MFU (%): 17.82 | TFLOPs: 176.23 | Mem Allocated (GB): 33.28 | Mem Reserved (GB): 60.98 | Max Mem Allocated (GB): 50.50
2025-12-17 17:46:36,070 - root - INFO - Step: 65 | Loss: 8.46 | Tokens per second: 14826.44 | Training tokens per second (%): 34.57 | MFU (%): 19.32 | TFLOPs: 191.04 | Mem Allocated (GB): 33.28 | Mem Reserved (GB): 60.90 | Max Mem Allocated (GB): 50.50
2025-12-17 17:46:36,070 - root - INFO - Step: 65 | Loss: 7.81 | Tokens per second: 14826.50 | Training tokens per second (%): 49.06 | MFU (%): 19.32 | TFLOPs: 191.04 | Mem Allocated (GB): 33.28 | Mem Reserved (GB): 60.98 | Max Mem Allocated (GB): 50.50
2025-12-17 17:46:36,070 - root - INFO - Step: 65 | Loss: 8.49 | Tokens per second: 14826.43 | Training tokens per second (%): 32.40 | MFU (%): 19.32 | TFLOPs: 191.04 | Mem Allocated (GB): 33.28 | Mem Reserved (GB): 60.69 | Max Mem Allocated (GB): 50.50
2025-12-17 17:46:36,070 - root - INFO - Step: 65 | Loss: 8.16 | Tokens per second: 14826.46 | Training tokens per second (%): 44.82 | MFU (%): 19.32 | TFLOPs: 191.04 | Mem Allocated (GB): 33.28 | Mem Reserved (GB): 60.57 | Max Mem Allocated (GB): 50.50
2025-12-17 17:46:38,846 - root - INFO - Step: 70 | Loss: 7.14 | Tokens per second: 14755.81 | Training tokens per second (%): 23.64 | MFU (%): 19.22 | TFLOPs: 190.13 | Mem Allocated (GB): 33.28 | Mem Reserved (GB): 60.69 | Max Mem Allocated (GB): 50.50
2025-12-17 17:46:38,846 - root - INFO - Step: 70 | Loss: 7.79 | Tokens per second: 14755.81 | Training tokens per second (%): 37.08 | MFU (%): 19.22 | TFLOPs: 190.13 | Mem Allocated (GB): 33.28 | Mem Reserved (GB): 60.98 | Max Mem Allocated (GB): 50.50
2025-12-17 17:46:38,846 - root - INFO - Step: 70 | Loss: 7.87 | Tokens per second: 14755.84 | Training tokens per second (%): 25.62 | MFU (%): 19.22 | TFLOPs: 190.13 | Mem Allocated (GB): 33.28 | Mem Reserved (GB): 60.57 | Max Mem Allocated (GB): 50.50
2025-12-17 17:46:38,846 - root - INFO - Step: 70 | Loss: 7.43 | Tokens per second: 14755.80 | Training tokens per second (%): 26.19 | MFU (%): 19.22 | TFLOPs: 190.13 | Mem Allocated (GB): 33.28 | Mem Reserved (GB): 60.90 | Max Mem Allocated (GB): 50.50
2025-12-17 17:46:41,619 - root - INFO - Step: 75 | Loss: 7.60 | Tokens per second: 14774.54 | Training tokens per second (%): 40.97 | MFU (%): 19.25 | TFLOPs: 190.37 | Mem Allocated (GB): 33.28 | Mem Reserved (GB): 60.90 | Max Mem Allocated (GB): 50.50
2025-12-17 17:46:41,619 - root - INFO - Step: 75 | Loss: 6.69 | Tokens per second: 14774.47 | Training tokens per second (%): 36.48 | MFU (%): 19.25 | TFLOPs: 190.37 | Mem Allocated (GB): 33.28 | Mem Reserved (GB): 60.69 | Max Mem Allocated (GB): 50.50
2025-12-17 17:46:41,619 - root - INFO - Step: 75 | Loss: 7.05 | Tokens per second: 14774.49 | Training tokens per second (%): 40.82 | MFU (%): 19.25 | TFLOPs: 190.37 | Mem Allocated (GB): 33.28 | Mem Reserved (GB): 60.98 | Max Mem Allocated (GB): 50.50
2025-12-17 17:46:41,619 - root - INFO - Step: 75 | Loss: 8.03 | Tokens per second: 14774.51 | Training tokens per second (%): 37.45 | MFU (%): 19.25 | TFLOPs: 190.37 | Mem Allocated (GB): 33.28 | Mem Reserved (GB): 60.57 | Max Mem Allocated (GB): 50.50
2025-12-17 17:46:44,385 - root - INFO - Step: 80 | Loss: 7.66 | Tokens per second: 14815.40 | Training tokens per second (%): 45.63 | MFU (%): 19.30 | TFLOPs: 190.90 | Mem Allocated (GB): 33.28 | Mem Reserved (GB): 60.69 | Max Mem Allocated (GB): 50.50
2025-12-17 17:46:44,385 - root - INFO - Step: 80 | Loss: 7.90 | Tokens per second: 14815.42 | Training tokens per second (%): 45.07 | MFU (%): 19.30 | TFLOPs: 190.90 | Mem Allocated (GB): 33.28 | Mem Reserved (GB): 60.57 | Max Mem Allocated (GB): 50.50
2025-12-17 17:46:44,385 - root - INFO - Step: 80 | Loss: 7.47 | Tokens per second: 14815.38 | Training tokens per second (%): 35.16 | MFU (%): 19.30 | TFLOPs: 190.90 | Mem Allocated (GB): 33.28 | Mem Reserved (GB): 60.90 | Max Mem Allocated (GB): 50.50
2025-12-17 17:46:44,385 - root - INFO - Step: 80 | Loss: 7.65 | Tokens per second: 14815.42 | Training tokens per second (%): 29.85 | MFU (%): 19.30 | TFLOPs: 190.90 | Mem Allocated (GB): 33.28 | Mem Reserved (GB): 60.98 | Max Mem Allocated (GB): 50.50
2025-12-17 17:46:47,151 - root - INFO - Step: 85 | Loss: 7.81 | Tokens per second: 14807.68 | Training tokens per second (%): 35.73 | MFU (%): 19.29 | TFLOPs: 190.80 | Mem Allocated (GB): 33.28 | Mem Reserved (GB): 60.69 | Max Mem Allocated (GB): 50.50
2025-12-17 17:46:47,151 - root - INFO - Step: 85 | Loss: 7.96 | Tokens per second: 14807.72 | Training tokens per second (%): 35.36 | MFU (%): 19.29 | TFLOPs: 190.80 | Mem Allocated (GB): 33.28 | Mem Reserved (GB): 60.98 | Max Mem Allocated (GB): 50.50
2025-12-17 17:46:47,151 - root - INFO - Step: 85 | Loss: 7.15 | Tokens per second: 14807.71 | Training tokens per second (%): 43.90 | MFU (%): 19.29 | TFLOPs: 190.80 | Mem Allocated (GB): 33.28 | Mem Reserved (GB): 60.90 | Max Mem Allocated (GB): 50.50
2025-12-17 17:46:47,151 - root - INFO - Step: 85 | Loss: 7.71 | Tokens per second: 14807.70 | Training tokens per second (%): 37.54 | MFU (%): 19.29 | TFLOPs: 190.80 | Mem Allocated (GB): 33.28 | Mem Reserved (GB): 60.57 | Max Mem Allocated (GB): 50.50
2025-12-17 17:46:49,933 - root - INFO - Step: 90 | Loss: 7.97 | Tokens per second: 14726.91 | Training tokens per second (%): 35.06 | MFU (%): 19.19 | TFLOPs: 189.76 | Mem Allocated (GB): 33.28 | Mem Reserved (GB): 60.69 | Max Mem Allocated (GB): 50.50
2025-12-17 17:46:49,933 - root - INFO - Step: 90 | Loss: 6.74 | Tokens per second: 14726.97 | Training tokens per second (%): 27.01 | MFU (%): 19.19 | TFLOPs: 189.76 | Mem Allocated (GB): 33.28 | Mem Reserved (GB): 60.57 | Max Mem Allocated (GB): 50.50
2025-12-17 17:46:49,933 - root - INFO - Step: 90 | Loss: 7.88 | Tokens per second: 14726.95 | Training tokens per second (%): 36.42 | MFU (%): 19.19 | TFLOPs: 189.76 | Mem Allocated (GB): 33.28 | Mem Reserved (GB): 60.90 | Max Mem Allocated (GB): 50.50
2025-12-17 17:46:49,933 - root - INFO - Step: 90 | Loss: 7.81 | Tokens per second: 14726.92 | Training tokens per second (%): 23.58 | MFU (%): 19.19 | TFLOPs: 189.76 | Mem Allocated (GB): 33.28 | Mem Reserved (GB): 60.98 | Max Mem Allocated (GB): 50.50
2025-12-17 17:46:52,713 - root - INFO - Step: 95 | Loss: 7.65 | Tokens per second: 14740.31 | Training tokens per second (%): 38.12 | MFU (%): 19.20 | TFLOPs: 189.93 | Mem Allocated (GB): 33.28 | Mem Reserved (GB): 60.57 | Max Mem Allocated (GB): 50.50
2025-12-17 17:46:52,713 - root - INFO - Step: 95 | Loss: 7.27 | Tokens per second: 14740.34 | Training tokens per second (%): 57.74 | MFU (%): 19.20 | TFLOPs: 189.93 | Mem Allocated (GB): 33.28 | Mem Reserved (GB): 60.90 | Max Mem Allocated (GB): 50.50
2025-12-17 17:46:52,713 - root - INFO - Step: 95 | Loss: 7.67 | Tokens per second: 14740.29 | Training tokens per second (%): 19.16 | MFU (%): 19.20 | TFLOPs: 189.93 | Mem Allocated (GB): 33.28 | Mem Reserved (GB): 60.69 | Max Mem Allocated (GB): 50.50
2025-12-17 17:46:52,713 - root - INFO - Step: 95 | Loss: 7.54 | Tokens per second: 14740.35 | Training tokens per second (%): 52.12 | MFU (%): 19.20 | TFLOPs: 189.93 | Mem Allocated (GB): 33.28 | Mem Reserved (GB): 60.98 | Max Mem Allocated (GB): 50.50
2025-12-17 17:46:55,492 - root - INFO - Step: 100 | Loss: 6.97 | Tokens per second: 14741.98 | Training tokens per second (%): 38.41 | MFU (%): 19.21 | TFLOPs: 189.95 | Mem Allocated (GB): 33.28 | Mem Reserved (GB): 60.98 | Max Mem Allocated (GB): 50.50
2025-12-17 17:46:55,492 - root - INFO - Step: 100 | Loss: 7.56 | Tokens per second: 14741.93 | Training tokens per second (%): 32.33 | MFU (%): 19.21 | TFLOPs: 189.95 | Mem Allocated (GB): 33.28 | Mem Reserved (GB): 60.57 | Max Mem Allocated (GB): 50.50
2025-12-17 17:46:55,492 - root - INFO - Step: 100 | Loss: 7.12 | Tokens per second: 14741.97 | Training tokens per second (%): 64.75 | MFU (%): 19.21 | TFLOPs: 189.95 | Mem Allocated (GB): 33.28 | Mem Reserved (GB): 60.69 | Max Mem Allocated (GB): 50.50
2025-12-17 17:46:55,492 - root - INFO - Step: 100 | Loss: 7.59 | Tokens per second: 14741.93 | Training tokens per second (%): 58.66 | MFU (%): 19.21 | TFLOPs: 189.95 | Mem Allocated (GB): 33.28 | Mem Reserved (GB): 60.90 | Max Mem Allocated (GB): 50.50
2025-12-17 17:46:58,265 - root - INFO - Step: 105 | Loss: 7.41 | Tokens per second: 14775.61 | Training tokens per second (%): 56.19 | MFU (%): 19.25 | TFLOPs: 190.39 | Mem Allocated (GB): 33.28 | Mem Reserved (GB): 60.69 | Max Mem Allocated (GB): 50.50
2025-12-17 17:46:58,265 - root - INFO - Step: 105 | Loss: 7.41 | Tokens per second: 14775.55 | Training tokens per second (%): 55.27 | MFU (%): 19.25 | TFLOPs: 190.39 | Mem Allocated (GB): 33.28 | Mem Reserved (GB): 60.98 | Max Mem Allocated (GB): 50.50
2025-12-17 17:46:58,265 - root - INFO - Step: 105 | Loss: 8.33 | Tokens per second: 14775.13 | Training tokens per second (%): 13.50 | MFU (%): 19.25 | TFLOPs: 190.38 | Mem Allocated (GB): 33.28 | Mem Reserved (GB): 60.57 | Max Mem Allocated (GB): 50.50
2025-12-17 17:46:58,265 - root - INFO - Step: 105 | Loss: 7.44 | Tokens per second: 14775.62 | Training tokens per second (%): 41.88 | MFU (%): 19.25 | TFLOPs: 190.39 | Mem Allocated (GB): 33.28 | Mem Reserved (GB): 60.90 | Max Mem Allocated (GB): 50.50
2025-12-17 17:47:01,047 - root - INFO - Step: 110 | Loss: 7.08 | Tokens per second: 14728.77 | Training tokens per second (%): 30.21 | MFU (%): 19.19 | TFLOPs: 189.78 | Mem Allocated (GB): 33.28 | Mem Reserved (GB): 60.90 | Max Mem Allocated (GB): 50.50
2025-12-17 17:47:01,047 - root - INFO - Step: 110 | Loss: 7.22 | Tokens per second: 14728.72 | Training tokens per second (%): 22.74 | MFU (%): 19.19 | TFLOPs: 189.78 | Mem Allocated (GB): 33.28 | Mem Reserved (GB): 60.69 | Max Mem Allocated (GB): 50.50
2025-12-17 17:47:01,047 - root - INFO - Step: 110 | Loss: 7.57 | Tokens per second: 14728.72 | Training tokens per second (%): 53.11 | MFU (%): 19.19 | TFLOPs: 189.78 | Mem Allocated (GB): 33.28 | Mem Reserved (GB): 60.98 | Max Mem Allocated (GB): 50.50
2025-12-17 17:47:01,047 - root - INFO - Step: 110 | Loss: 7.76 | Tokens per second: 14728.73 | Training tokens per second (%): 38.24 | MFU (%): 19.19 | TFLOPs: 189.78 | Mem Allocated (GB): 33.28 | Mem Reserved (GB): 60.57 | Max Mem Allocated (GB): 50.50
2025-12-17 17:47:03,824 - root - INFO - Step: 115 | Loss: 7.04 | Tokens per second: 14750.82 | Training tokens per second (%): 23.98 | MFU (%): 19.22 | TFLOPs: 190.07 | Mem Allocated (GB): 33.28 | Mem Reserved (GB): 60.69 | Max Mem Allocated (GB): 50.50
2025-12-17 17:47:03,824 - root - INFO - Step: 115 | Loss: 6.83 | Tokens per second: 14750.85 | Training tokens per second (%): 32.16 | MFU (%): 19.22 | TFLOPs: 190.07 | Mem Allocated (GB): 33.28 | Mem Reserved (GB): 60.57 | Max Mem Allocated (GB): 50.50
2025-12-17 17:47:03,824 - root - INFO - Step: 115 | Loss: 6.92 | Tokens per second: 14750.81 | Training tokens per second (%): 33.34 | MFU (%): 19.22 | TFLOPs: 190.07 | Mem Allocated (GB): 33.28 | Mem Reserved (GB): 60.90 | Max Mem Allocated (GB): 50.50
2025-12-17 17:47:03,824 - root - INFO - Step: 115 | Loss: 7.46 | Tokens per second: 14750.85 | Training tokens per second (%): 60.48 | MFU (%): 19.22 | TFLOPs: 190.07 | Mem Allocated (GB): 33.28 | Mem Reserved (GB): 60.98 | Max Mem Allocated (GB): 50.50
2025-12-17 17:47:06,971 - root - INFO - Step: 120 | Loss: 6.22 | Tokens per second: 13017.97 | Training tokens per second (%): 55.53 | MFU (%): 16.96 | TFLOPs: 167.74 | Mem Allocated (GB): 33.28 | Mem Reserved (GB): 60.98 | Max Mem Allocated (GB): 50.50
2025-12-17 17:47:06,971 - root - INFO - Step: 120 | Loss: 7.59 | Tokens per second: 13017.97 | Training tokens per second (%): 29.38 | MFU (%): 16.96 | TFLOPs: 167.74 | Mem Allocated (GB): 33.28 | Mem Reserved (GB): 60.90 | Max Mem Allocated (GB): 50.50
2025-12-17 17:47:06,971 - root - INFO - Step: 120 | Loss: 7.35 | Tokens per second: 13017.94 | Training tokens per second (%): 14.89 | MFU (%): 16.96 | TFLOPs: 167.74 | Mem Allocated (GB): 33.28 | Mem Reserved (GB): 60.69 | Max Mem Allocated (GB): 50.50
2025-12-17 17:47:06,971 - root - INFO - Step: 120 | Loss: 6.49 | Tokens per second: 13017.95 | Training tokens per second (%): 14.97 | MFU (%): 16.96 | TFLOPs: 167.74 | Mem Allocated (GB): 33.28 | Mem Reserved (GB): 60.57 | Max Mem Allocated (GB): 50.50
2025-12-17 17:47:09,955 - root - INFO - Step: 125 | Loss: 7.28 | Tokens per second: 13730.62 | Training tokens per second (%): 28.20 | MFU (%): 17.89 | TFLOPs: 176.92 | Mem Allocated (GB): 33.28 | Mem Reserved (GB): 60.69 | Max Mem Allocated (GB): 50.50
2025-12-17 17:47:09,955 - root - INFO - Step: 125 | Loss: 7.33 | Tokens per second: 13730.62 | Training tokens per second (%): 49.58 | MFU (%): 17.89 | TFLOPs: 176.92 | Mem Allocated (GB): 33.28 | Mem Reserved (GB): 60.90 | Max Mem Allocated (GB): 50.50
2025-12-17 17:47:09,955 - root - INFO - Step: 125 | Loss: 7.25 | Tokens per second: 13730.53 | Training tokens per second (%): 38.47 | MFU (%): 17.89 | TFLOPs: 176.92 | Mem Allocated (GB): 33.28 | Mem Reserved (GB): 60.98 | Max Mem Allocated (GB): 50.50
2025-12-17 17:47:09,955 - root - INFO - Step: 125 | Loss: 7.13 | Tokens per second: 13730.64 | Training tokens per second (%): 24.38 | MFU (%): 17.89 | TFLOPs: 176.92 | Mem Allocated (GB): 33.28 | Mem Reserved (GB): 60.57 | Max Mem Allocated (GB): 50.50
2025-12-17 17:47:12,736 - root - INFO - Step: 130 | Loss: 7.85 | Tokens per second: 14732.40 | Training tokens per second (%): 76.61 | MFU (%): 19.19 | TFLOPs: 189.83 | Mem Allocated (GB): 33.28 | Mem Reserved (GB): 60.57 | Max Mem Allocated (GB): 50.50
2025-12-17 17:47:12,736 - root - INFO - Step: 130 | Loss: 7.34 | Tokens per second: 14732.36 | Training tokens per second (%): 39.61 | MFU (%): 19.19 | TFLOPs: 189.83 | Mem Allocated (GB): 33.28 | Mem Reserved (GB): 60.90 | Max Mem Allocated (GB): 50.50
2025-12-17 17:47:12,736 - root - INFO - Step: 130 | Loss: 7.20 | Tokens per second: 14732.39 | Training tokens per second (%): 23.79 | MFU (%): 19.19 | TFLOPs: 189.83 | Mem Allocated (GB): 33.28 | Mem Reserved (GB): 60.98 | Max Mem Allocated (GB): 50.50
2025-12-17 17:47:12,736 - root - INFO - Step: 130 | Loss: 7.36 | Tokens per second: 14732.35 | Training tokens per second (%): 29.72 | MFU (%): 19.19 | TFLOPs: 189.83 | Mem Allocated (GB): 33.28 | Mem Reserved (GB): 60.69 | Max Mem Allocated (GB): 50.50
2025-12-17 17:47:15,514 - root - INFO - Step: 135 | Loss: 7.01 | Tokens per second: 14746.90 | Training tokens per second (%): 47.35 | MFU (%): 19.21 | TFLOPs: 190.02 | Mem Allocated (GB): 33.28 | Mem Reserved (GB): 60.57 | Max Mem Allocated (GB): 50.50
2025-12-17 17:47:15,514 - root - INFO - Step: 135 | Loss: 7.13 | Tokens per second: 14746.96 | Training tokens per second (%): 15.90 | MFU (%): 19.21 | TFLOPs: 190.02 | Mem Allocated (GB): 33.28 | Mem Reserved (GB): 60.69 | Max Mem Allocated (GB): 50.50
2025-12-17 17:47:15,514 - root - INFO - Step: 135 | Loss: 8.11 | Tokens per second: 14746.94 | Training tokens per second (%): 43.57 | MFU (%): 19.21 | TFLOPs: 190.02 | Mem Allocated (GB): 33.28 | Mem Reserved (GB): 60.90 | Max Mem Allocated (GB): 50.50
2025-12-17 17:47:15,514 - root - INFO - Step: 135 | Loss: 7.44 | Tokens per second: 14746.96 | Training tokens per second (%): 44.98 | MFU (%): 19.21 | TFLOPs: 190.02 | Mem Allocated (GB): 33.28 | Mem Reserved (GB): 60.98 | Max Mem Allocated (GB): 50.50
2025-12-17 17:47:18,284 - root - INFO - Step: 140 | Loss: 6.97 | Tokens per second: 14792.63 | Training tokens per second (%): 41.25 | MFU (%): 19.27 | TFLOPs: 190.61 | Mem Allocated (GB): 33.28 | Mem Reserved (GB): 60.98 | Max Mem Allocated (GB): 50.50
2025-12-17 17:47:18,284 - root - INFO - Step: 140 | Loss: 7.37 | Tokens per second: 14792.60 | Training tokens per second (%): 51.46 | MFU (%): 19.27 | TFLOPs: 190.61 | Mem Allocated (GB): 33.28 | Mem Reserved (GB): 60.57 | Max Mem Allocated (GB): 50.50
2025-12-17 17:47:18,284 - root - INFO - Step: 140 | Loss: 7.69 | Tokens per second: 14792.62 | Training tokens per second (%): 29.57 | MFU (%): 19.27 | TFLOPs: 190.61 | Mem Allocated (GB): 33.28 | Mem Reserved (GB): 60.90 | Max Mem Allocated (GB): 50.50
2025-12-17 17:47:18,284 - root - INFO - Step: 140 | Loss: 7.53 | Tokens per second: 14792.60 | Training tokens per second (%): 44.90 | MFU (%): 19.27 | TFLOPs: 190.61 | Mem Allocated (GB): 33.28 | Mem Reserved (GB): 60.69 | Max Mem Allocated (GB): 50.50
2025-12-17 17:47:21,071 - root - INFO - Step: 145 | Loss: 6.69 | Tokens per second: 14702.41 | Training tokens per second (%): 40.22 | MFU (%): 19.16 | TFLOPs: 189.44 | Mem Allocated (GB): 33.28 | Mem Reserved (GB): 60.98 | Max Mem Allocated (GB): 50.50
2025-12-17 17:47:21,071 - root - INFO - Step: 145 | Loss: 7.71 | Tokens per second: 14702.44 | Training tokens per second (%): 28.04 | MFU (%): 19.16 | TFLOPs: 189.45 | Mem Allocated (GB): 33.28 | Mem Reserved (GB): 60.69 | Max Mem Allocated (GB): 50.50
2025-12-17 17:47:21,071 - root - INFO - Step: 145 | Loss: 7.52 | Tokens per second: 14702.42 | Training tokens per second (%): 23.10 | MFU (%): 19.16 | TFLOPs: 189.45 | Mem Allocated (GB): 33.28 | Mem Reserved (GB): 60.90 | Max Mem Allocated (GB): 50.50
2025-12-17 17:47:21,071 - root - INFO - Step: 145 | Loss: 7.26 | Tokens per second: 14702.41 | Training tokens per second (%): 40.03 | MFU (%): 19.16 | TFLOPs: 189.44 | Mem Allocated (GB): 33.28 | Mem Reserved (GB): 60.57 | Max Mem Allocated (GB): 50.50
2025-12-17 17:47:23,838 - root - INFO - Step: 150 | Loss: 7.29 | Tokens per second: 14804.80 | Training tokens per second (%): 61.47 | MFU (%): 19.29 | TFLOPs: 190.76 | Mem Allocated (GB): 33.28 | Mem Reserved (GB): 60.90 | Max Mem Allocated (GB): 50.50
2025-12-17 17:47:23,838 - root - INFO - Step: 150 | Loss: 7.71 | Tokens per second: 14804.78 | Training tokens per second (%): 25.32 | MFU (%): 19.29 | TFLOPs: 190.76 | Mem Allocated (GB): 33.28 | Mem Reserved (GB): 60.69 | Max Mem Allocated (GB): 50.50
2025-12-17 17:47:23,838 - root - INFO - Step: 150 | Loss: 7.21 | Tokens per second: 14804.82 | Training tokens per second (%): 41.10 | MFU (%): 19.29 | TFLOPs: 190.76 | Mem Allocated (GB): 33.28 | Mem Reserved (GB): 60.57 | Max Mem Allocated (GB): 50.50
2025-12-17 17:47:23,838 - root - INFO - Step: 150 | Loss: 7.55 | Tokens per second: 14804.77 | Training tokens per second (%): 50.62 | MFU (%): 19.29 | TFLOPs: 190.76 | Mem Allocated (GB): 33.28 | Mem Reserved (GB): 60.98 | Max Mem Allocated (GB): 50.50
2025-12-17 17:47:26,608 - root - INFO - Step: 155 | Loss: 7.41 | Tokens per second: 14788.26 | Training tokens per second (%): 61.52 | MFU (%): 19.27 | TFLOPs: 190.55 | Mem Allocated (GB): 33.28 | Mem Reserved (GB): 60.69 | Max Mem Allocated (GB): 50.50
2025-12-17 17:47:26,608 - root - INFO - Step: 155 | Loss: 7.22 | Tokens per second: 14788.30 | Training tokens per second (%): 37.36 | MFU (%): 19.27 | TFLOPs: 190.55 | Mem Allocated (GB): 33.28 | Mem Reserved (GB): 60.57 | Max Mem Allocated (GB): 50.50
2025-12-17 17:47:26,608 - root - INFO - Step: 155 | Loss: 7.48 | Tokens per second: 14788.24 | Training tokens per second (%): 36.85 | MFU (%): 19.27 | TFLOPs: 190.55 | Mem Allocated (GB): 33.28 | Mem Reserved (GB): 60.90 | Max Mem Allocated (GB): 50.50
2025-12-17 17:47:26,608 - root - INFO - Step: 155 | Loss: 7.33 | Tokens per second: 14788.30 | Training tokens per second (%): 35.81 | MFU (%): 19.27 | TFLOPs: 190.55 | Mem Allocated (GB): 33.28 | Mem Reserved (GB): 60.98 | Max Mem Allocated (GB): 50.50
2025-12-17 17:47:29,382 - root - INFO - Step: 160 | Loss: 7.01 | Tokens per second: 14771.81 | Training tokens per second (%): 38.63 | MFU (%): 19.25 | TFLOPs: 190.34 | Mem Allocated (GB): 33.28 | Mem Reserved (GB): 60.90 | Max Mem Allocated (GB): 50.50
2025-12-17 17:47:29,382 - root - INFO - Step: 160 | Loss: 6.54 | Tokens per second: 14771.79 | Training tokens per second (%): 28.84 | MFU (%): 19.25 | TFLOPs: 190.34 | Mem Allocated (GB): 33.28 | Mem Reserved (GB): 60.57 | Max Mem Allocated (GB): 50.50
2025-12-17 17:47:29,382 - root - INFO - Step: 160 | Loss: 7.02 | Tokens per second: 14771.78 | Training tokens per second (%): 66.84 | MFU (%): 19.25 | TFLOPs: 190.34 | Mem Allocated (GB): 33.28 | Mem Reserved (GB): 60.69 | Max Mem Allocated (GB): 50.50
2025-12-17 17:47:29,382 - root - INFO - Step: 160 | Loss: 7.02 | Tokens per second: 14771.83 | Training tokens per second (%): 39.92 | MFU (%): 19.25 | TFLOPs: 190.34 | Mem Allocated (GB): 33.28 | Mem Reserved (GB): 60.98 | Max Mem Allocated (GB): 50.50
2025-12-17 17:47:32,166 - root - INFO - Step: 165 | Loss: 6.76 | Tokens per second: 14716.53 | Training tokens per second (%): 18.36 | MFU (%): 19.17 | TFLOPs: 189.63 | Mem Allocated (GB): 33.28 | Mem Reserved (GB): 60.90 | Max Mem Allocated (GB): 50.50
2025-12-17 17:47:32,166 - root - INFO - Step: 165 | Loss: 7.31 | Tokens per second: 14716.56 | Training tokens per second (%): 35.44 | MFU (%): 19.17 | TFLOPs: 189.63 | Mem Allocated (GB): 33.28 | Mem Reserved (GB): 60.57 | Max Mem Allocated (GB): 50.50
2025-12-17 17:47:32,166 - root - INFO - Step: 165 | Loss: 6.70 | Tokens per second: 14716.59 | Training tokens per second (%): 49.42 | MFU (%): 19.17 | TFLOPs: 189.63 | Mem Allocated (GB): 33.28 | Mem Reserved (GB): 60.98 | Max Mem Allocated (GB): 50.50
2025-12-17 17:47:32,166 - root - INFO - Step: 165 | Loss: 7.62 | Tokens per second: 14716.57 | Training tokens per second (%): 41.67 | MFU (%): 19.17 | TFLOPs: 189.63 | Mem Allocated (GB): 33.28 | Mem Reserved (GB): 60.69 | Max Mem Allocated (GB): 50.50
2025-12-17 17:47:34,944 - root - INFO - Step: 170 | Loss: 7.17 | Tokens per second: 14750.73 | Training tokens per second (%): 50.27 | MFU (%): 19.22 | TFLOPs: 190.07 | Mem Allocated (GB): 33.28 | Mem Reserved (GB): 60.90 | Max Mem Allocated (GB): 50.50
2025-12-17 17:47:34,944 - root - INFO - Step: 170 | Loss: 6.93 | Tokens per second: 14750.77 | Training tokens per second (%): 36.91 | MFU (%): 19.22 | TFLOPs: 190.07 | Mem Allocated (GB): 33.28 | Mem Reserved (GB): 60.69 | Max Mem Allocated (GB): 50.50
2025-12-17 17:47:34,944 - root - INFO - Step: 170 | Loss: 7.68 | Tokens per second: 14750.72 | Training tokens per second (%): 32.23 | MFU (%): 19.22 | TFLOPs: 190.07 | Mem Allocated (GB): 33.28 | Mem Reserved (GB): 60.98 | Max Mem Allocated (GB): 50.50
2025-12-17 17:47:34,944 - root - INFO - Step: 170 | Loss: 5.96 | Tokens per second: 14750.73 | Training tokens per second (%): 66.35 | MFU (%): 19.22 | TFLOPs: 190.07 | Mem Allocated (GB): 33.28 | Mem Reserved (GB): 60.57 | Max Mem Allocated (GB): 50.50
2025-12-17 17:47:37,724 - root - INFO - Step: 175 | Loss: 6.47 | Tokens per second: 14734.11 | Training tokens per second (%): 32.18 | MFU (%): 19.20 | TFLOPs: 189.85 | Mem Allocated (GB): 33.28 | Mem Reserved (GB): 60.57 | Max Mem Allocated (GB): 50.50
2025-12-17 17:47:37,724 - root - INFO - Step: 175 | Loss: 7.21 | Tokens per second: 14734.07 | Training tokens per second (%): 62.35 | MFU (%): 19.20 | TFLOPs: 189.85 | Mem Allocated (GB): 33.28 | Mem Reserved (GB): 60.69 | Max Mem Allocated (GB): 50.50
2025-12-17 17:47:37,724 - root - INFO - Step: 175 | Loss: 7.23 | Tokens per second: 14734.07 | Training tokens per second (%): 48.25 | MFU (%): 19.20 | TFLOPs: 189.85 | Mem Allocated (GB): 33.28 | Mem Reserved (GB): 60.90 | Max Mem Allocated (GB): 50.50
2025-12-17 17:47:37,724 - root - INFO - Step: 175 | Loss: 6.37 | Tokens per second: 14734.06 | Training tokens per second (%): 40.45 | MFU (%): 19.20 | TFLOPs: 189.85 | Mem Allocated (GB): 33.28 | Mem Reserved (GB): 60.98 | Max Mem Allocated (GB): 50.50
2025-12-17 17:47:40,498 - root - INFO - Step: 180 | Loss: 7.08 | Tokens per second: 14768.82 | Training tokens per second (%): 23.68 | MFU (%): 19.24 | TFLOPs: 190.30 | Mem Allocated (GB): 33.28 | Mem Reserved (GB): 60.98 | Max Mem Allocated (GB): 50.50
2025-12-17 17:47:40,498 - root - INFO - Step: 180 | Loss: 6.86 | Tokens per second: 14768.78 | Training tokens per second (%): 29.83 | MFU (%): 19.24 | TFLOPs: 190.30 | Mem Allocated (GB): 33.28 | Mem Reserved (GB): 60.57 | Max Mem Allocated (GB): 50.50
2025-12-17 17:47:40,498 - root - INFO - Step: 180 | Loss: 6.76 | Tokens per second: 14768.80 | Training tokens per second (%): 21.20 | MFU (%): 19.24 | TFLOPs: 190.30 | Mem Allocated (GB): 33.28 | Mem Reserved (GB): 60.90 | Max Mem Allocated (GB): 50.50
2025-12-17 17:47:40,498 - root - INFO - Step: 180 | Loss: 7.00 | Tokens per second: 14768.78 | Training tokens per second (%): 31.24 | MFU (%): 19.24 | TFLOPs: 190.30 | Mem Allocated (GB): 33.28 | Mem Reserved (GB): 60.69 | Max Mem Allocated (GB): 50.50
2025-12-17 17:47:43,633 - root - INFO - Step: 185 | Loss: 6.91 | Tokens per second: 13069.62 | Training tokens per second (%): 68.74 | MFU (%): 17.03 | TFLOPs: 168.41 | Mem Allocated (GB): 33.28 | Mem Reserved (GB): 60.98 | Max Mem Allocated (GB): 50.50
2025-12-17 17:47:43,633 - root - INFO - Step: 185 | Loss: 8.03 | Tokens per second: 13069.59 | Training tokens per second (%): 36.61 | MFU (%): 17.03 | TFLOPs: 168.41 | Mem Allocated (GB): 33.28 | Mem Reserved (GB): 60.90 | Max Mem Allocated (GB): 50.50
2025-12-17 17:47:43,633 - root - INFO - Step: 185 | Loss: 7.09 | Tokens per second: 13069.53 | Training tokens per second (%): 36.83 | MFU (%): 17.03 | TFLOPs: 168.40 | Mem Allocated (GB): 33.28 | Mem Reserved (GB): 60.57 | Max Mem Allocated (GB): 50.50
2025-12-17 17:47:43,633 - root - INFO - Step: 185 | Loss: 7.14 | Tokens per second: 13069.63 | Training tokens per second (%): 26.38 | MFU (%): 17.03 | TFLOPs: 168.41 | Mem Allocated (GB): 33.28 | Mem Reserved (GB): 60.69 | Max Mem Allocated (GB): 50.50
2025-12-17 17:47:46,598 - root - INFO - Step: 190 | Loss: 6.97 | Tokens per second: 13817.81 | Training tokens per second (%): 70.87 | MFU (%): 18.00 | TFLOPs: 178.05 | Mem Allocated (GB): 33.28 | Mem Reserved (GB): 60.57 | Max Mem Allocated (GB): 50.50
2025-12-17 17:47:46,598 - root - INFO - Step: 190 | Loss: 7.14 | Tokens per second: 13817.79 | Training tokens per second (%): 46.43 | MFU (%): 18.00 | TFLOPs: 178.05 | Mem Allocated (GB): 33.28 | Mem Reserved (GB): 60.90 | Max Mem Allocated (GB): 50.50
2025-12-17 17:47:46,598 - root - INFO - Step: 190 | Loss: 7.01 | Tokens per second: 13817.81 | Training tokens per second (%): 25.28 | MFU (%): 18.00 | TFLOPs: 178.05 | Mem Allocated (GB): 33.28 | Mem Reserved (GB): 60.69 | Max Mem Allocated (GB): 50.50
2025-12-17 17:47:46,598 - root - INFO - Step: 190 | Loss: 7.28 | Tokens per second: 13817.65 | Training tokens per second (%): 35.03 | MFU (%): 18.00 | TFLOPs: 178.04 | Mem Allocated (GB): 33.28 | Mem Reserved (GB): 60.98 | Max Mem Allocated (GB): 50.50
2025-12-17 17:47:49,345 - root - INFO - Step: 195 | Loss: 7.41 | Tokens per second: 14916.81 | Training tokens per second (%): 53.39 | MFU (%): 19.43 | TFLOPs: 192.21 | Mem Allocated (GB): 33.28 | Mem Reserved (GB): 60.90 | Max Mem Allocated (GB): 50.50
2025-12-17 17:47:49,345 - root - INFO - Step: 195 | Loss: 6.85 | Tokens per second: 14916.82 | Training tokens per second (%): 26.32 | MFU (%): 19.43 | TFLOPs: 192.21 | Mem Allocated (GB): 33.28 | Mem Reserved (GB): 60.69 | Max Mem Allocated (GB): 50.50
2025-12-17 17:47:49,345 - root - INFO - Step: 195 | Loss: 6.73 | Tokens per second: 14916.79 | Training tokens per second (%): 31.45 | MFU (%): 19.43 | TFLOPs: 192.21 | Mem Allocated (GB): 33.28 | Mem Reserved (GB): 60.57 | Max Mem Allocated (GB): 50.50
2025-12-17 17:47:49,345 - root - INFO - Step: 195 | Loss: 7.23 | Tokens per second: 14916.84 | Training tokens per second (%): 49.31 | MFU (%): 19.43 | TFLOPs: 192.21 | Mem Allocated (GB): 33.28 | Mem Reserved (GB): 60.98 | Max Mem Allocated (GB): 50.50
2025-12-17 17:47:52,113 - root - INFO - Step: 200 | Loss: 6.92 | Tokens per second: 14801.67 | Training tokens per second (%): 52.17 | MFU (%): 19.28 | TFLOPs: 190.72 | Mem Allocated (GB): 33.28 | Mem Reserved (GB): 60.98 | Max Mem Allocated (GB): 50.50
2025-12-17 17:47:52,113 - root - INFO - Step: 200 | Loss: 6.71 | Tokens per second: 14801.65 | Training tokens per second (%): 41.25 | MFU (%): 19.28 | TFLOPs: 190.72 | Mem Allocated (GB): 33.28 | Mem Reserved (GB): 60.57 | Max Mem Allocated (GB): 50.50
2025-12-17 17:47:52,113 - root - INFO - Step: 200 | Loss: 6.90 | Tokens per second: 14801.65 | Training tokens per second (%): 59.52 | MFU (%): 19.28 | TFLOPs: 190.72 | Mem Allocated (GB): 33.28 | Mem Reserved (GB): 60.90 | Max Mem Allocated (GB): 50.50
2025-12-17 17:47:52,113 - root - INFO - Step: 200 | Loss: 7.37 | Tokens per second: 14801.64 | Training tokens per second (%): 35.04 | MFU (%): 19.28 | TFLOPs: 190.72 | Mem Allocated (GB): 33.28 | Mem Reserved (GB): 60.69 | Max Mem Allocated (GB): 50.50
2025-12-17 17:47:54,854 - root - INFO - Step: 205 | Loss: 6.83 | Tokens per second: 14944.44 | Training tokens per second (%): 27.65 | MFU (%): 19.47 | TFLOPs: 192.56 | Mem Allocated (GB): 33.28 | Mem Reserved (GB): 60.90 | Max Mem Allocated (GB): 50.50
2025-12-17 17:47:54,854 - root - INFO - Step: 205 | Loss: 6.69 | Tokens per second: 14944.41 | Training tokens per second (%): 26.38 | MFU (%): 19.47 | TFLOPs: 192.56 | Mem Allocated (GB): 33.28 | Mem Reserved (GB): 60.57 | Max Mem Allocated (GB): 50.50
2025-12-17 17:47:54,854 - root - INFO - Step: 205 | Loss: 6.74 | Tokens per second: 14944.45 | Training tokens per second (%): 63.92 | MFU (%): 19.47 | TFLOPs: 192.56 | Mem Allocated (GB): 33.28 | Mem Reserved (GB): 60.69 | Max Mem Allocated (GB): 50.50
2025-12-17 17:47:54,854 - root - INFO - Step: 205 | Loss: 7.03 | Tokens per second: 14944.42 | Training tokens per second (%): 25.59 | MFU (%): 19.47 | TFLOPs: 192.56 | Mem Allocated (GB): 33.28 | Mem Reserved (GB): 60.98 | Max Mem Allocated (GB): 50.50
2025-12-17 17:47:57,614 - root - INFO - Step: 210 | Loss: 7.39 | Tokens per second: 14844.55 | Training tokens per second (%): 45.62 | MFU (%): 19.34 | TFLOPs: 191.28 | Mem Allocated (GB): 33.28 | Mem Reserved (GB): 60.98 | Max Mem Allocated (GB): 50.50
2025-12-17 17:47:57,614 - root - INFO - Step: 210 | Loss: 8.01 | Tokens per second: 14844.49 | Training tokens per second (%): 15.79 | MFU (%): 19.34 | TFLOPs: 191.28 | Mem Allocated (GB): 33.28 | Mem Reserved (GB): 60.90 | Max Mem Allocated (GB): 50.50
2025-12-17 17:47:57,614 - root - INFO - Step: 210 | Loss: 7.88 | Tokens per second: 14844.51 | Training tokens per second (%): 22.44 | MFU (%): 19.34 | TFLOPs: 191.28 | Mem Allocated (GB): 33.28 | Mem Reserved (GB): 60.57 | Max Mem Allocated (GB): 50.50
2025-12-17 17:47:57,614 - root - INFO - Step: 210 | Loss: 6.57 | Tokens per second: 14844.52 | Training tokens per second (%): 21.88 | MFU (%): 19.34 | TFLOPs: 191.28 | Mem Allocated (GB): 33.28 | Mem Reserved (GB): 60.69 | Max Mem Allocated (GB): 50.50
2025-12-17 17:48:00,355 - root - INFO - Step: 215 | Loss: 6.53 | Tokens per second: 14946.28 | Training tokens per second (%): 38.17 | MFU (%): 19.47 | TFLOPs: 192.59 | Mem Allocated (GB): 33.28 | Mem Reserved (GB): 60.90 | Max Mem Allocated (GB): 50.50
2025-12-17 17:48:00,355 - root - INFO - Step: 215 | Loss: 7.44 | Tokens per second: 14946.33 | Training tokens per second (%): 20.32 | MFU (%): 19.47 | TFLOPs: 192.59 | Mem Allocated (GB): 33.28 | Mem Reserved (GB): 60.57 | Max Mem Allocated (GB): 50.50
2025-12-17 17:48:00,355 - root - INFO - Step: 215 | Loss: 6.67 | Tokens per second: 14946.28 | Training tokens per second (%): 56.14 | MFU (%): 19.47 | TFLOPs: 192.59 | Mem Allocated (GB): 33.28 | Mem Reserved (GB): 60.98 | Max Mem Allocated (GB): 50.50
2025-12-17 17:48:00,355 - root - INFO - Step: 215 | Loss: 7.69 | Tokens per second: 14946.33 | Training tokens per second (%): 28.92 | MFU (%): 19.47 | TFLOPs: 192.59 | Mem Allocated (GB): 33.28 | Mem Reserved (GB): 60.69 | Max Mem Allocated (GB): 50.50
2025-12-17 17:48:03,108 - root - INFO - Step: 220 | Loss: 6.82 | Tokens per second: 14883.14 | Training tokens per second (%): 36.65 | MFU (%): 19.39 | TFLOPs: 191.77 | Mem Allocated (GB): 33.28 | Mem Reserved (GB): 60.69 | Max Mem Allocated (GB): 50.50
2025-12-17 17:48:03,108 - root - INFO - Step: 220 | Loss: 7.08 | Tokens per second: 14883.08 | Training tokens per second (%): 62.31 | MFU (%): 19.39 | TFLOPs: 191.77 | Mem Allocated (GB): 33.28 | Mem Reserved (GB): 60.57 | Max Mem Allocated (GB): 50.50
2025-12-17 17:48:03,108 - root - INFO - Step: 220 | Loss: 7.23 | Tokens per second: 14883.09 | Training tokens per second (%): 52.39 | MFU (%): 19.39 | TFLOPs: 191.77 | Mem Allocated (GB): 33.28 | Mem Reserved (GB): 60.90 | Max Mem Allocated (GB): 50.50
2025-12-17 17:48:03,108 - root - INFO - Step: 220 | Loss: 6.98 | Tokens per second: 14883.12 | Training tokens per second (%): 53.53 | MFU (%): 19.39 | TFLOPs: 191.77 | Mem Allocated (GB): 33.28 | Mem Reserved (GB): 60.98 | Max Mem Allocated (GB): 50.50
2025-12-17 17:48:05,863 - root - INFO - Step: 225 | Loss: 7.57 | Tokens per second: 14871.72 | Training tokens per second (%): 60.49 | MFU (%): 19.38 | TFLOPs: 191.63 | Mem Allocated (GB): 33.28 | Mem Reserved (GB): 60.98 | Max Mem Allocated (GB): 50.50
2025-12-17 17:48:05,863 - root - INFO - Step: 225 | Loss: 6.46 | Tokens per second: 14871.71 | Training tokens per second (%): 33.09 | MFU (%): 19.38 | TFLOPs: 191.63 | Mem Allocated (GB): 33.28 | Mem Reserved (GB): 60.90 | Max Mem Allocated (GB): 50.50
2025-12-17 17:48:05,863 - root - INFO - Step: 225 | Loss: 7.30 | Tokens per second: 14871.67 | Training tokens per second (%): 46.07 | MFU (%): 19.38 | TFLOPs: 191.63 | Mem Allocated (GB): 33.28 | Mem Reserved (GB): 60.69 | Max Mem Allocated (GB): 50.50
2025-12-17 17:48:05,863 - root - INFO - Step: 225 | Loss: 7.53 | Tokens per second: 14871.69 | Training tokens per second (%): 17.36 | MFU (%): 19.38 | TFLOPs: 191.63 | Mem Allocated (GB): 33.28 | Mem Reserved (GB): 60.57 | Max Mem Allocated (GB): 50.50
2025-12-17 17:48:08,606 - root - INFO - Step: 230 | Loss: 6.73 | Tokens per second: 14937.97 | Training tokens per second (%): 33.52 | MFU (%): 19.46 | TFLOPs: 192.48 | Mem Allocated (GB): 33.28 | Mem Reserved (GB): 60.90 | Max Mem Allocated (GB): 50.50
2025-12-17 17:48:08,606 - root - INFO - Step: 230 | Loss: 6.66 | Tokens per second: 14937.97 | Training tokens per second (%): 53.33 | MFU (%): 19.46 | TFLOPs: 192.48 | Mem Allocated (GB): 33.28 | Mem Reserved (GB): 60.69 | Max Mem Allocated (GB): 50.50
2025-12-17 17:48:08,606 - root - INFO - Step: 230 | Loss: 6.30 | Tokens per second: 14938.01 | Training tokens per second (%): 45.48 | MFU (%): 19.46 | TFLOPs: 192.48 | Mem Allocated (GB): 33.28 | Mem Reserved (GB): 60.57 | Max Mem Allocated (GB): 50.50
2025-12-17 17:48:08,606 - root - INFO - Step: 230 | Loss: 7.07 | Tokens per second: 14937.95 | Training tokens per second (%): 54.63 | MFU (%): 19.46 | TFLOPs: 192.48 | Mem Allocated (GB): 33.28 | Mem Reserved (GB): 60.98 | Max Mem Allocated (GB): 50.50
2025-12-17 17:48:11,339 - root - INFO - Step: 235 | Loss: 6.97 | Tokens per second: 14988.36 | Training tokens per second (%): 47.39 | MFU (%): 19.53 | TFLOPs: 193.13 | Mem Allocated (GB): 33.28 | Mem Reserved (GB): 60.69 | Max Mem Allocated (GB): 50.50
2025-12-17 17:48:11,339 - root - INFO - Step: 235 | Loss: 7.10 | Tokens per second: 14988.37 | Training tokens per second (%): 38.65 | MFU (%): 19.53 | TFLOPs: 193.13 | Mem Allocated (GB): 33.28 | Mem Reserved (GB): 60.57 | Max Mem Allocated (GB): 50.50
2025-12-17 17:48:11,339 - root - INFO - Step: 235 | Loss: 7.27 | Tokens per second: 14988.33 | Training tokens per second (%): 28.22 | MFU (%): 19.53 | TFLOPs: 193.13 | Mem Allocated (GB): 33.28 | Mem Reserved (GB): 60.90 | Max Mem Allocated (GB): 50.50
2025-12-17 17:48:11,339 - root - INFO - Step: 235 | Loss: 6.60 | Tokens per second: 14988.39 | Training tokens per second (%): 33.66 | MFU (%): 19.53 | TFLOPs: 193.13 | Mem Allocated (GB): 33.28 | Mem Reserved (GB): 60.98 | Max Mem Allocated (GB): 50.50
2025-12-17 17:48:14,097 - root - INFO - Step: 240 | Loss: 6.62 | Tokens per second: 14858.03 | Training tokens per second (%): 21.17 | MFU (%): 19.36 | TFLOPs: 191.45 | Mem Allocated (GB): 33.28 | Mem Reserved (GB): 60.69 | Max Mem Allocated (GB): 50.50
2025-12-17 17:48:14,097 - root - INFO - Step: 240 | Loss: 7.69 | Tokens per second: 14858.04 | Training tokens per second (%): 31.04 | MFU (%): 19.36 | TFLOPs: 191.45 | Mem Allocated (GB): 33.28 | Mem Reserved (GB): 60.57 | Max Mem Allocated (GB): 50.50
2025-12-17 17:48:14,097 - root - INFO - Step: 240 | Loss: 6.40 | Tokens per second: 14857.66 | Training tokens per second (%): 23.52 | MFU (%): 19.36 | TFLOPs: 191.45 | Mem Allocated (GB): 33.28 | Mem Reserved (GB): 60.98 | Max Mem Allocated (GB): 50.50
2025-12-17 17:48:14,097 - root - INFO - Step: 240 | Loss: 6.80 | Tokens per second: 14857.83 | Training tokens per second (%): 44.20 | MFU (%): 19.36 | TFLOPs: 191.45 | Mem Allocated (GB): 33.28 | Mem Reserved (GB): 60.90 | Max Mem Allocated (GB): 50.50
2025-12-17 17:48:16,860 - root - INFO - Step: 245 | Loss: 7.07 | Tokens per second: 14825.63 | Training tokens per second (%): 52.58 | MFU (%): 19.32 | TFLOPs: 191.03 | Mem Allocated (GB): 33.28 | Mem Reserved (GB): 60.69 | Max Mem Allocated (GB): 50.50
2025-12-17 17:48:16,860 - root - INFO - Step: 245 | Loss: 6.94 | Tokens per second: 14825.65 | Training tokens per second (%): 42.85 | MFU (%): 19.32 | TFLOPs: 191.03 | Mem Allocated (GB): 33.28 | Mem Reserved (GB): 60.57 | Max Mem Allocated (GB): 50.50
2025-12-17 17:48:16,860 - root - INFO - Step: 245 | Loss: 7.12 | Tokens per second: 14825.68 | Training tokens per second (%): 20.40 | MFU (%): 19.32 | TFLOPs: 191.03 | Mem Allocated (GB): 33.28 | Mem Reserved (GB): 60.90 | Max Mem Allocated (GB): 50.50
2025-12-17 17:48:16,860 - root - INFO - Step: 245 | Loss: 7.28 | Tokens per second: 14825.65 | Training tokens per second (%): 56.42 | MFU (%): 19.32 | TFLOPs: 191.03 | Mem Allocated (GB): 33.28 | Mem Reserved (GB): 60.98 | Max Mem Allocated (GB): 50.50
2025-12-17 17:48:19,814 - root - INFO - Step: 250 | Loss: 6.86 | Tokens per second: 13869.75 | Training tokens per second (%): 42.08 | MFU (%): 18.07 | TFLOPs: 178.72 | Mem Allocated (GB): 33.28 | Mem Reserved (GB): 60.90 | Max Mem Allocated (GB): 50.50
2025-12-17 17:48:19,814 - root - INFO - Step: 250 | Loss: 6.88 | Tokens per second: 13869.72 | Training tokens per second (%): 41.03 | MFU (%): 18.07 | TFLOPs: 178.72 | Mem Allocated (GB): 33.28 | Mem Reserved (GB): 60.57 | Max Mem Allocated (GB): 50.50
2025-12-17 17:48:19,814 - root - INFO - Step: 250 | Loss: 7.46 | Tokens per second: 13869.76 | Training tokens per second (%): 23.88 | MFU (%): 18.07 | TFLOPs: 178.72 | Mem Allocated (GB): 33.28 | Mem Reserved (GB): 60.98 | Max Mem Allocated (GB): 50.50
2025-12-17 17:48:19,814 - root - INFO - Step: 250 | Loss: 7.18 | Tokens per second: 13869.61 | Training tokens per second (%): 26.95 | MFU (%): 18.07 | TFLOPs: 178.71 | Mem Allocated (GB): 33.28 | Mem Reserved (GB): 60.69 | Max Mem Allocated (GB): 50.50
2025-12-17 17:48:19,814 - root - INFO - Training completed
2025-12-17 17:48:19,814 - root - INFO - Training completed
2025-12-17 17:48:19,814 - root - INFO - Training completed
2025-12-17 17:48:19,814 - root - INFO - Training completed
[2025-12-17 17:48:22,237] [INFO] [launch.py:367:main] Process 37638 exits successfully.
[2025-12-17 17:48:22,237] [INFO] [launch.py:367:main] Process 37628 exits successfully.
[2025-12-17 17:48:22,237] [INFO] [launch.py:367:main] Process 37631 exits successfully.
[2025-12-17 17:48:22,237] [INFO] [launch.py:367:main] Process 37634 exits successfully.
Exception ignored in atexit callback: <function dump_compile_times at 0x4001a6e32020>
Traceback (most recent call last):
  File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/utils.py", line 438, in dump_compile_times
  File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/utils.py", line 424, in compile_times
  File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/utils.py", line 205, in tabulate
  File "<frozen importlib._bootstrap>", line 1360, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1331, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 935, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 991, in exec_module
  File "<frozen importlib._bootstrap_external>", line 1128, in get_code
  File "<frozen importlib._bootstrap_external>", line 1186, in get_data
OSError: [Errno 107] Transport endpoint is not connected: '/usr/local/lib/python3.12/dist-packages/tabulate/__init__.py'
END TIME: Wed Dec 17 17:48:26 CET 2025
